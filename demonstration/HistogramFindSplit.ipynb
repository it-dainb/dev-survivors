{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f645153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import pathlib, tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from graphviz import Digraph\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import stats\n",
    "\n",
    "from survivors import metrics as metr\n",
    "from survivors import constants as cnt\n",
    "from survivors import criteria as crit\n",
    "from numba import njit, jit\n",
    "from lifelines import KaplanMeierFitter, NelsonAalenFitter\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e20aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit('f8(f8[:], f8[:], i8[:], i8[:], i8, f8[:])', cache=True)\n",
    "def lr_statistic(dur_1, dur_2, cens_1, cens_2, wei_1, wei_2):\n",
    "    times = np.unique(np.hstack((dur_1, dur_2)))\n",
    "    dur_1 = np.searchsorted(times, dur_1) + 1\n",
    "    dur_2 = np.searchsorted(times, dur_2) + 1\n",
    "    times_range = np.array([1, times.shape[0]], dtype=np.int32)\n",
    "\n",
    "    bins = times_range[1] - times_range[0] + 1\n",
    "    n_1_j = np.histogram(dur_1, bins=bins, range=times_range)[0]\n",
    "    n_2_j = np.histogram(dur_2, bins=bins, range=times_range)[0]\n",
    "    O_1_j = np.histogram(dur_1 * cens_1, bins=bins, range=times_range)[0]\n",
    "    O_2_j = np.histogram(dur_2 * cens_2, bins=bins, range=times_range)[0]\n",
    "\n",
    "    N_1_j = np.cumsum(n_1_j[::-1])[::-1]\n",
    "    N_2_j = np.cumsum(n_2_j[::-1])[::-1]\n",
    "    ind = np.where(N_1_j * N_2_j != 0)\n",
    "    N_1_j = N_1_j[ind]\n",
    "    N_2_j = N_2_j[ind]\n",
    "    O_1_j = O_1_j[ind]\n",
    "    O_2_j = O_2_j[ind]\n",
    "\n",
    "    N_j = N_1_j + N_2_j\n",
    "    O_j = O_1_j + O_2_j\n",
    "    E_1_j = N_1_j * O_j / N_j\n",
    "    res = np.zeros((N_j.shape[0], 3), dtype=np.float32)\n",
    "    res[:, 1] = O_1_j - E_1_j\n",
    "    res[:, 2] = E_1_j * (N_j - O_j) * N_2_j / (N_j * (N_j - 1))\n",
    "    res[:, 0] = 1.0\n",
    "    # if np.any(N_j <= 1):\n",
    "    #     return 0.0\n",
    "    if weightings == 2:\n",
    "        res[:, 0] = N_j\n",
    "    elif weightings == 3:\n",
    "        res[:, 0] = np.sqrt(N_j)\n",
    "    elif weightings == 4:\n",
    "        res[:, 0] = np.cumprod((1.0 - O_j / (N_j + 1)))\n",
    "    elif weightings == 5:\n",
    "        res[:, 0] = np.cumprod((1.0 - O_j / (N_j + 1)))\n",
    "    logrank = np.power((res[:, 0] * res[:, 1]).sum(), 2) / ((res[:, 0] * res[:, 0] * res[:, 2]).sum())\n",
    "    return logrank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "00446994",
   "metadata": {},
   "outputs": [],
   "source": [
    "dur_A = np.random.uniform(0, 10000, 10000)\n",
    "cens_A = np.random.choice(2, 10000)\n",
    "dur_B = np.random.uniform(0, 10000, 10000)\n",
    "cens_B = np.random.choice(2, 10000)\n",
    "weight_A = np.random.uniform(0, 1, 10000)\n",
    "weight_B = np.random.uniform(0, 1, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "431786e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61787635, 0.87655056, 0.79484043, ..., 0.29210781, 0.38067468,\n",
       "       0.29061483])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f778d119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e62ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_criter_split(arr_nan, left, right, criterion):\n",
    "    none_to = 0\n",
    "    max_stat_val = 1.0\n",
    "    if arr_nan.shape[1] > 0:\n",
    "        left_and_nan = np.hstack([left, arr_nan])\n",
    "        right_and_nan = np.hstack([right, arr_nan])\n",
    "        a = criterion(left_and_nan[1], right[1], left_and_nan[0], right[0])\n",
    "        b = criterion(left[1], right_and_nan[1], left[0], right_and_nan[0])\n",
    "        # Nans move to a leaf with maximal statistical value\n",
    "        none_to = int(a < b)\n",
    "        max_stat_val = max(a, b)\n",
    "    else:\n",
    "        max_stat_val = criterion(left[1], right[1], left[0], right[0])\n",
    "    return (max_stat_val, none_to)\n",
    "\n",
    "\n",
    "def get_attrs(max_stat_val, values, none_to, l_sh, r_sh, nan_sh):\n",
    "    attrs = dict()\n",
    "    attrs[\"stat_val\"] = max_stat_val\n",
    "    attrs[\"values\"] = values\n",
    "    if none_to:\n",
    "        attrs[\"pos_nan\"] = [0, 1]\n",
    "        attrs[\"min_split\"] = min(l_sh, r_sh+nan_sh)\n",
    "    else:\n",
    "        attrs[\"pos_nan\"] = [1, 0]\n",
    "        attrs[\"min_split\"] = min(l_sh+nan_sh, r_sh)\n",
    "    return attrs\n",
    "\n",
    "\n",
    "def get_cont_attrs(uniq_set, arr_notnan, arr_nan, min_samples_leaf, criterion, \n",
    "                   signif_val, thres_cont_bin_max):\n",
    "    if uniq_set.shape[0] > thres_cont_bin_max:\n",
    "        uniq_set = np.quantile(arr_notnan[0], [i/float(thres_cont_bin_max) for i in range(1, thres_cont_bin_max)])\n",
    "    else:  # Set intermediate points\n",
    "        uniq_set = (uniq_set[:-1] + uniq_set[1:])*0.5\n",
    "    uniq_set = list(set(np.round(uniq_set, 3)))\n",
    "    attr_dicts = []\n",
    "    for value in uniq_set:\n",
    "        # Filter by attr value\n",
    "        ind = arr_notnan[0] >= value\n",
    "        left = arr_notnan[1:, np.where(ind)[0]].astype(np.int32)\n",
    "        right = arr_notnan[1:, np.where(~ind)[0]].astype(np.int32)\n",
    "        if min(left.shape[1], right.shape[1]) <= min_samples_leaf:\n",
    "            continue\n",
    "        max_stat_val, none_to = optimal_criter_split(arr_nan, left, right, criterion)\n",
    "        if max_stat_val >= signif_val:\n",
    "            attr_loc = get_attrs(max_stat_val, value, none_to,\n",
    "                                 left.shape[1], right.shape[1], arr_nan.shape[1])\n",
    "            attr_dicts.append(attr_loc)\n",
    "    return attr_dicts\n",
    "\n",
    "\n",
    "def get_categ_attrs(uniq_set, arr_notnan, arr_nan, min_samples_leaf, criterion, signif_val):\n",
    "    attr_dicts = []\n",
    "    pairs_uniq = power_set_nonover(uniq_set)\n",
    "    for l, r in pairs_uniq:\n",
    "        left = arr_notnan[1:, np.isin(arr_notnan[0], l)].astype(np.int32)\n",
    "        right = arr_notnan[1:, np.isin(arr_notnan[0], r)].astype(np.int32)\n",
    "        if min(left.shape[1], right.shape[1]) <= min_samples_leaf:\n",
    "            continue\n",
    "        max_stat_val, none_to = optimal_criter_split(arr_nan, left, right, criterion)\n",
    "        if max_stat_val >= signif_val:\n",
    "            attr_loc = get_attrs(max_stat_val, [list(l), list(r)], none_to,\n",
    "                                 left.shape[1], right.shape[1], arr_nan.shape[1])\n",
    "            attr_dicts.append(attr_loc)\n",
    "    return attr_dicts\n",
    "\n",
    "\n",
    "def hist_best_attr_split(arr, criterion=\"logrank\", type_attr=\"cont\", thres_cont_bin_max=100,\n",
    "                         signif=1.0, signif_stat=0.0, min_samples_leaf=10, bonf=True, verbose=0, **kwargs):\n",
    "    criterion = scrit.CRITERIA_DICT.get(criterion, None)\n",
    "    best_attr = {\"stat_val\": signif_stat, \"p_value\": signif,\n",
    "                 \"sign_split\": 0, \"values\": [], \"pos_nan\": [1, 0]}\n",
    "    if arr.shape[1] < 2*min_samples_leaf:\n",
    "        return best_attr\n",
    "\n",
    "    ind = np.isnan(arr[0])\n",
    "    arr_nan = arr[1:, np.where(ind)[0]].astype(np.int32)\n",
    "    arr_notnan = arr[:, np.where(~ind)[0]]\n",
    "    \n",
    "    if type_attr == \"woe\":\n",
    "        arr_notnan[0], descr_np = transform_woe(arr_notnan[0], arr_notnan[1])\n",
    "        \n",
    "    \n",
    "    uniq_set = np.unique(arr_notnan[0])\n",
    "    \n",
    "    if type_attr == \"categ\" and uniq_set.shape[0] > 0:\n",
    "        attr_dicts = get_categ_attrs(uniq_set, arr_notnan, arr_nan,\n",
    "                                     min_samples_leaf, criterion, signif_stat)\n",
    "    else:\n",
    "        attr_dicts = get_cont_attrs(uniq_set, arr_notnan, arr_nan,\n",
    "                                    min_samples_leaf, criterion, signif_stat, thres_cont_bin_max)\n",
    "    \n",
    "    if len(attr_dicts) == 0:\n",
    "        return best_attr\n",
    "    best_attr = max(attr_dicts, key=lambda x: x[\"stat_val\"])\n",
    "    best_attr[\"p_value\"] = stats.chi2.sf(best_attr[\"stat_val\"], df=1)\n",
    "    best_attr[\"sign_split\"] = len(attr_dicts)\n",
    "    if best_attr[\"sign_split\"] > 0:\n",
    "        if type_attr == \"cont\":\n",
    "            best_attr[\"values\"] = [f\" >= {best_attr['values']}\", f\" < {best_attr['values']}\"]\n",
    "        elif type_attr == \"categ\":\n",
    "            best_attr[\"values\"] = [f\" in {e}\" for e in best_attr[\"values\"]]\n",
    "        elif type_attr == \"woe\":\n",
    "            ind = descr_np[1] >= best_attr[\"values\"]\n",
    "            l, r = list(descr_np[0, np.where(ind)[0]]), list(descr_np[0, np.where(~ind)[0]])\n",
    "            best_attr[\"values\"] = [f\" in {e}\" for e in [l, r]]\n",
    "        if bonf:\n",
    "            best_attr[\"p_value\"] *= best_attr[\"sign_split\"]\n",
    "        if verbose > 0:\n",
    "            print(best_attr[\"p_value\"], len(uniq_set))\n",
    "    return best_attr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b47ffa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dur = np.random.uniform(0, 10000, 10000)\n",
    "cens = np.random.choice(2, 10000)\n",
    "vals = np.random.uniform(0, 10000, 10000)\n",
    "\n",
    "times = np.unique(dur)\n",
    "if times.shape[0] > 100:\n",
    "    bins = np.quantile(times, [i/float(100) for i in range(1, 100)])\n",
    "else:\n",
    "    bins = (times[:-1] + times[1:])*0.5\n",
    "        \n",
    "times_range = np.array([1, times.shape[0]], dtype=np.int32)\n",
    "\n",
    "hist_vals = bincount(inds)\n",
    "np.histogram(dur, bins=bins, range=times_range)[0]\n",
    "\n",
    "inds = np.digitize(dur_A, bins)\n",
    "bincount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8ac8f570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc113cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
