{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f645153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import pathlib, tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from graphviz import Digraph\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import stats\n",
    "\n",
    "from survivors import metrics as metr\n",
    "from survivors import constants as cnt\n",
    "from survivors import criteria as crit\n",
    "from numba import njit, jit\n",
    "from lifelines import KaplanMeierFitter, NelsonAalenFitter\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65e20aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit('f8(f8[:], f8[:], i8[:], i8[:], i8)', cache=True)\n",
    "def lr_statistic(dur_1, dur_2, cens_1, cens_2, weightings):\n",
    "    times = np.unique(np.hstack((dur_1, dur_2)))\n",
    "    dur_1 = np.searchsorted(times, dur_1) + 1\n",
    "    dur_2 = np.searchsorted(times, dur_2) + 1\n",
    "    times_range = np.array([1, times.shape[0]], dtype=np.int32)\n",
    "\n",
    "    bins = times_range[1] - times_range[0] + 1\n",
    "    n_1_j = np.histogram(dur_1, bins=bins, range=times_range)[0]\n",
    "    n_2_j = np.histogram(dur_2, bins=bins, range=times_range)[0]\n",
    "    O_1_j = np.histogram(dur_1 * cens_1, bins=bins, range=times_range)[0]\n",
    "    O_2_j = np.histogram(dur_2 * cens_2, bins=bins, range=times_range)[0]\n",
    "\n",
    "    N_1_j = np.cumsum(n_1_j[::-1])[::-1]\n",
    "    N_2_j = np.cumsum(n_2_j[::-1])[::-1]\n",
    "    ind = np.where(N_1_j * N_2_j != 0)\n",
    "    N_1_j = N_1_j[ind]\n",
    "    N_2_j = N_2_j[ind]\n",
    "    O_1_j = O_1_j[ind]\n",
    "    O_2_j = O_2_j[ind]\n",
    "\n",
    "    N_j = N_1_j + N_2_j\n",
    "    O_j = O_1_j + O_2_j\n",
    "    E_1_j = N_1_j * O_j / N_j\n",
    "    res = np.zeros((N_j.shape[0], 3), dtype=np.float32)\n",
    "    res[:, 1] = O_1_j - E_1_j\n",
    "    res[:, 2] = E_1_j * (N_j - O_j) * N_2_j / (N_j * (N_j - 1))\n",
    "    res[:, 0] = 1.0\n",
    "    # if np.any(N_j <= 1):\n",
    "    #     return 0.0\n",
    "    if weightings == 2:\n",
    "        res[:, 0] = N_j\n",
    "    elif weightings == 3:\n",
    "        res[:, 0] = np.sqrt(N_j)\n",
    "    elif weightings == 4:\n",
    "        res[:, 0] = np.cumprod((1.0 - O_j / (N_j + 1)))\n",
    "    logrank = np.power((res[:, 0] * res[:, 1]).sum(), 2) / ((res[:, 0] * res[:, 0] * res[:, 2]).sum())\n",
    "    return logrank\n",
    "\n",
    "\n",
    "def weight_lr_fast(dur_A, dur_B, cens_A=None, cens_B=None, weightings=\"\"):\n",
    "    try:\n",
    "        if cens_A is None:\n",
    "            cens_A = np.ones(dur_A.shape[0])\n",
    "        if cens_B is None:\n",
    "            cens_B = np.ones(dur_B.shape[0])\n",
    "        d = {\"logrank\": 1, \"wilcoxon\": 2, \"tarone-ware\": 3, \"peto\": 4}\n",
    "        weightings = d.get(weightings, 1)\n",
    "        logrank = lr_statistic(dur_A.astype(\"float64\"),\n",
    "                               dur_B.astype(\"float64\"),\n",
    "                               cens_A.astype(\"int64\"),\n",
    "                               cens_B.astype(\"int64\"),\n",
    "                               np.int64(weightings))\n",
    "        return logrank\n",
    "    except Exception as err:\n",
    "        return 0.0\n",
    "    \n",
    "def logrank(durations_A, durations_B, event_observed_A=None, event_observed_B=None):\n",
    "    return weight_lr_fast(durations_A, durations_B, event_observed_A, event_observed_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24e62ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_criter_split(arr_nan, left, right, criterion):\n",
    "    none_to = 0\n",
    "    max_stat_val = 1.0\n",
    "    if arr_nan.shape[1] > 0:\n",
    "        left_and_nan = np.hstack([left, arr_nan])\n",
    "        right_and_nan = np.hstack([right, arr_nan])\n",
    "        a = criterion(left_and_nan[1], right[1], left_and_nan[0], right[0])\n",
    "        b = criterion(left[1], right_and_nan[1], left[0], right_and_nan[0])\n",
    "        # Nans move to a leaf with maximal statistical value\n",
    "        none_to = int(a < b)\n",
    "        max_stat_val = max(a, b)\n",
    "    else:\n",
    "        max_stat_val = criterion(left[1], right[1], left[0], right[0])\n",
    "    return (max_stat_val, none_to)\n",
    "\n",
    "\n",
    "def get_attrs(max_stat_val, values, none_to, l_sh, r_sh, nan_sh):\n",
    "    attrs = dict()\n",
    "    attrs[\"stat_val\"] = max_stat_val\n",
    "    attrs[\"values\"] = values\n",
    "    if none_to:\n",
    "        attrs[\"pos_nan\"] = [0, 1]\n",
    "        attrs[\"min_split\"] = min(l_sh, r_sh+nan_sh)\n",
    "    else:\n",
    "        attrs[\"pos_nan\"] = [1, 0]\n",
    "        attrs[\"min_split\"] = min(l_sh+nan_sh, r_sh)\n",
    "    return attrs\n",
    "\n",
    "\n",
    "def get_cont_attrs(uniq_set, arr_notnan, arr_nan, min_samples_leaf, criterion, \n",
    "                   signif_val, thres_cont_bin_max):\n",
    "    if uniq_set.shape[0] > thres_cont_bin_max:\n",
    "        uniq_set = np.quantile(arr_notnan[0], [i/float(thres_cont_bin_max) for i in range(1, thres_cont_bin_max)])\n",
    "    else:  # Set intermediate points\n",
    "        uniq_set = (uniq_set[:-1] + uniq_set[1:])*0.5\n",
    "    uniq_set = list(set(np.round(uniq_set, 3)))\n",
    "    attr_dicts = []\n",
    "    for value in uniq_set:\n",
    "        # Filter by attr value\n",
    "        ind = arr_notnan[0] >= value\n",
    "        left = arr_notnan[1:, np.where(ind)[0]].astype(np.int32)\n",
    "        right = arr_notnan[1:, np.where(~ind)[0]].astype(np.int32)\n",
    "        if min(left.shape[1], right.shape[1]) <= min_samples_leaf:\n",
    "            continue\n",
    "        max_stat_val, none_to = optimal_criter_split(arr_nan, left, right, criterion)\n",
    "        if max_stat_val >= signif_val:\n",
    "            attr_loc = get_attrs(max_stat_val, value, none_to,\n",
    "                                 left.shape[1], right.shape[1], arr_nan.shape[1])\n",
    "            attr_dicts.append(attr_loc)\n",
    "    return attr_dicts\n",
    "\n",
    "\n",
    "def get_categ_attrs(uniq_set, arr_notnan, arr_nan, min_samples_leaf, criterion, signif_val):\n",
    "    attr_dicts = []\n",
    "    pairs_uniq = power_set_nonover(uniq_set)\n",
    "    for l, r in pairs_uniq:\n",
    "        left = arr_notnan[1:, np.isin(arr_notnan[0], l)].astype(np.int32)\n",
    "        right = arr_notnan[1:, np.isin(arr_notnan[0], r)].astype(np.int32)\n",
    "        if min(left.shape[1], right.shape[1]) <= min_samples_leaf:\n",
    "            continue\n",
    "        max_stat_val, none_to = optimal_criter_split(arr_nan, left, right, criterion)\n",
    "        if max_stat_val >= signif_val:\n",
    "            attr_loc = get_attrs(max_stat_val, [list(l), list(r)], none_to,\n",
    "                                 left.shape[1], right.shape[1], arr_nan.shape[1])\n",
    "            attr_dicts.append(attr_loc)\n",
    "    return attr_dicts\n",
    "\n",
    "\n",
    "def best_attr_split(arr, criterion=\"logrank\", type_attr=\"cont\", thres_cont_bin_max=100,\n",
    "                         signif=1.0, signif_stat=0.0, min_samples_leaf=10, bonf=True, verbose=0, **kwargs):\n",
    "#     criterion = crit.CRITERIA_DICT.get(criterion, None)\n",
    "    best_attr = {\"stat_val\": signif_stat, \"p_value\": signif,\n",
    "                 \"sign_split\": 0, \"values\": [], \"pos_nan\": [1, 0]}\n",
    "    if arr.shape[1] < 2*min_samples_leaf:\n",
    "        return best_attr\n",
    "\n",
    "    ind = np.isnan(arr[0])\n",
    "    arr_nan = arr[1:, np.where(ind)[0]].astype(np.int32)\n",
    "    arr_notnan = arr[:, np.where(~ind)[0]]\n",
    "    \n",
    "    if type_attr == \"woe\":\n",
    "        arr_notnan[0], descr_np = transform_woe(arr_notnan[0], arr_notnan[1])\n",
    "        \n",
    "    \n",
    "    uniq_set = np.unique(arr_notnan[0])\n",
    "    \n",
    "    if type_attr == \"categ\" and uniq_set.shape[0] > 0:\n",
    "        attr_dicts = get_categ_attrs(uniq_set, arr_notnan, arr_nan,\n",
    "                                     min_samples_leaf, logrank, signif_stat)\n",
    "    else:\n",
    "        attr_dicts = get_cont_attrs(uniq_set, arr_notnan, arr_nan,\n",
    "                                    min_samples_leaf, logrank, signif_stat, thres_cont_bin_max)\n",
    "    \n",
    "    if len(attr_dicts) == 0:\n",
    "        return best_attr\n",
    "    best_attr = max(attr_dicts, key=lambda x: x[\"stat_val\"])\n",
    "    best_attr[\"p_value\"] = stats.chi2.sf(best_attr[\"stat_val\"], df=1)\n",
    "    best_attr[\"sign_split\"] = len(attr_dicts)\n",
    "    if best_attr[\"sign_split\"] > 0:\n",
    "        if type_attr == \"cont\":\n",
    "            best_attr[\"values\"] = [f\" >= {best_attr['values']}\", f\" < {best_attr['values']}\"]\n",
    "        elif type_attr == \"categ\":\n",
    "            best_attr[\"values\"] = [f\" in {e}\" for e in best_attr[\"values\"]]\n",
    "        elif type_attr == \"woe\":\n",
    "            ind = descr_np[1] >= best_attr[\"values\"]\n",
    "            l, r = list(descr_np[0, np.where(ind)[0]]), list(descr_np[0, np.where(~ind)[0]])\n",
    "            best_attr[\"values\"] = [f\" in {e}\" for e in [l, r]]\n",
    "        if bonf:\n",
    "            best_attr[\"p_value\"] *= best_attr[\"sign_split\"]\n",
    "        if verbose > 0:\n",
    "            print(best_attr[\"p_value\"], len(uniq_set))\n",
    "    return best_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63eaf77",
   "metadata": {},
   "source": [
    "### NEW SPLITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2eb899e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit('f8(i8[:], i8[:], i8[:], i8[:], i8)', cache=True)\n",
    "def lr_hist_statistic(time_hist_1, time_hist_2, cens_hist_1, cens_hist_2, weightings):\n",
    "    N_1_j = np.cumsum(time_hist_1[::-1])[::-1]\n",
    "    N_2_j = np.cumsum(time_hist_2[::-1])[::-1]\n",
    "    ind = np.where(N_1_j * N_2_j != 0)\n",
    "    N_1_j = N_1_j[ind]\n",
    "    N_2_j = N_2_j[ind]\n",
    "    O_1_j = cens_hist_1[ind]\n",
    "    O_2_j = cens_hist_2[ind]\n",
    "\n",
    "    N_j = N_1_j + N_2_j\n",
    "    O_j = O_1_j + O_2_j\n",
    "    E_1_j = N_1_j * O_j / N_j\n",
    "    res = np.zeros((N_j.shape[0], 3), dtype=np.float32)\n",
    "    res[:, 1] = O_1_j - E_1_j\n",
    "    res[:, 2] = E_1_j * (N_j - O_j) * N_2_j / (N_j * (N_j - 1))\n",
    "    res[:, 0] = 1.0\n",
    "    \n",
    "    if weightings == 2:\n",
    "        res[:, 0] = N_j\n",
    "    elif weightings == 3:\n",
    "        res[:, 0] = np.sqrt(N_j)\n",
    "    elif weightings == 4:\n",
    "        res[:, 0] = np.cumprod((1.0 - O_j / (N_j + 1)))\n",
    "    stat_val = np.power((res[:, 0] * res[:, 1]).sum(), 2) / ((res[:, 0] * res[:, 0] * res[:, 2]).sum())\n",
    "    return stat_val\n",
    "\n",
    "def weight_hist_stat(time_hist_1, time_hist_2, cens_hist_1=None, cens_hist_2=None, weightings=\"\"):\n",
    "    try:\n",
    "        if cens_hist_1 is None:\n",
    "            cens_hist_1 = time_hist_1\n",
    "        if cens_hist_2 is None:\n",
    "            cens_hist_2 = time_hist_2\n",
    "        d = {\"logrank\": 1, \"wilcoxon\": 2, \"tarone-ware\": 3, \"peto\": 4}\n",
    "        weightings = d.get(weightings, 1)\n",
    "        logrank = lr_hist_statistic(time_hist_1.astype(\"int64\"),\n",
    "                                    time_hist_2.astype(\"int64\"),\n",
    "                                    cens_hist_1.astype(\"int64\"),\n",
    "                                    cens_hist_2.astype(\"int64\"),\n",
    "                                    np.int64(weightings))\n",
    "        return logrank\n",
    "    except Exception as err:\n",
    "        return 0.0\n",
    "    \n",
    "def logrank_hist(time_hist_1, time_hist_2, cens_hist_1=None, cens_hist_2=None):\n",
    "    return weight_hist_stat(time_hist_1, time_hist_2, cens_hist_1, cens_hist_2)\n",
    "\n",
    "def optimal_criter_split_hist(left_time_hist, left_cens_hist, \n",
    "                         right_time_hist, right_cens_hist,\n",
    "                         na_time_hist, na_cens_hist, criterion):\n",
    "    none_to = 0\n",
    "    max_stat_val = 1.0\n",
    "    if na_time_hist.shape[0] > 0:\n",
    "        a = criterion(left_time_hist + na_time_hist, right_time_hist, \n",
    "                      left_cens_hist + na_cens_hist, right_cens_hist)\n",
    "        b = criterion(left_time_hist, right_time_hist + na_time_hist, \n",
    "                      left_cens_hist, right_cens_hist + na_cens_hist)\n",
    "        # Nans move to a leaf with maximal statistical value\n",
    "        none_to = int(a < b)\n",
    "        max_stat_val = max(a, b)\n",
    "    else:\n",
    "        max_stat_val = criterion(left_time_hist, right_time_hist, \n",
    "                                 left_cens_hist, right_cens_hist)\n",
    "    return (max_stat_val, none_to)\n",
    "\n",
    "def split_time_to_bins(time):\n",
    "    return np.searchsorted(np.unique(time), time)\n",
    "\n",
    "\n",
    "def get_attrs(max_stat_val, values, none_to, l_sh, r_sh, nan_sh):\n",
    "    attrs = dict()\n",
    "    attrs[\"stat_val\"] = max_stat_val\n",
    "    attrs[\"values\"] = values\n",
    "    if none_to:\n",
    "        attrs[\"pos_nan\"] = [0, 1]\n",
    "        attrs[\"min_split\"] = min(l_sh, r_sh+nan_sh)\n",
    "    else:\n",
    "        attrs[\"pos_nan\"] = [1, 0]\n",
    "        attrs[\"min_split\"] = min(l_sh+nan_sh, r_sh)\n",
    "    return attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d169b09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_best_attr_split(arr, criterion=\"logrank\", type_attr=\"cont\", thres_cont_bin_max=100,\n",
    "                         signif=1.0, signif_stat=0.0, min_samples_leaf=10, bonf=True, verbose=0, **kwargs):\n",
    "    criterion = crit.CRITERIA_DICT.get(criterion, None)\n",
    "    best_attr = {\"stat_val\": signif_stat, \"p_value\": signif,\n",
    "                 \"sign_split\": 0, \"values\": [], \"pos_nan\": [1, 0]}\n",
    "    if arr.shape[1] < 2*min_samples_leaf:\n",
    "        return best_attr\n",
    "    vals = arr[0]\n",
    "    cens = arr[1]\n",
    "    dur = arr[2]\n",
    "    \n",
    "    dur = split_time_to_bins(dur)\n",
    "    ind = np.isnan(vals)\n",
    "\n",
    "    # split nan and not-nan\n",
    "    dur_notna = dur[~ind]\n",
    "    cens_notna = cens[~ind]\n",
    "    vals_notna = vals[~ind]\n",
    "\n",
    "    # find splitting values\n",
    "    uniq_set = np.unique(vals_notna)\n",
    "    if uniq_set.shape[0] > thres_cont_bin_max:\n",
    "        uniq_set = np.quantile(vals_notna, [i/float(thres_cont_bin_max) for i in range(1, thres_cont_bin_max)])\n",
    "    else:\n",
    "        uniq_set = (uniq_set[:-1] + uniq_set[1:])*0.5\n",
    "    uniq_set = np.unique(np.round(uniq_set, 3))\n",
    "    index_vals_bin = np.digitize(vals_notna, uniq_set)\n",
    "\n",
    "    # find global hist by times\n",
    "    na_time_hist = np.bincount(dur[ind])\n",
    "    na_cens_hist = np.bincount(dur[ind], weights=cens[ind])\n",
    "\n",
    "    right_time_hist = np.bincount(dur_notna)\n",
    "    right_cens_hist = np.bincount(dur_notna, weights=cens_notna)\n",
    "    left_time_hist = np.zeros_like(dur_notna, dtype=np.int32)\n",
    "    left_cens_hist = left_time_hist.copy()\n",
    "    \n",
    "    num_nan = ind.sum()\n",
    "    num_r = (~ind).sum()\n",
    "    num_l = 0\n",
    "\n",
    "    # for each split values get branches\n",
    "    attr_dicts = []\n",
    "    for u in np.unique(index_vals_bin)[:-1]:\n",
    "        curr_mask = index_vals_bin == u\n",
    "        curr_time_hist = np.bincount(dur_notna, weights=curr_mask).astype(\"int32\")\n",
    "        curr_cens_hist = np.bincount(dur_notna, weights=cens_notna*curr_mask).astype(\"int32\")\n",
    "        left_time_hist += curr_time_hist\n",
    "        left_cens_hist += curr_cens_hist\n",
    "        right_time_hist -= curr_time_hist\n",
    "        right_cens_hist -= curr_cens_hist\n",
    "        num_l += curr_mask.sum()\n",
    "        num_r -= curr_mask.sum()\n",
    "        \n",
    "        if min(num_l, num_r) <= min_samples_leaf:\n",
    "            continue\n",
    "        max_stat_val, none_to = optimal_criter_split_hist(left_time_hist, left_cens_hist, \n",
    "                                                     right_time_hist, right_cens_hist,\n",
    "                                                     na_time_hist, na_cens_hist, logrank_hist)\n",
    "        \n",
    "        if max_stat_val >= signif_stat:\n",
    "            attr_loc = get_attrs(max_stat_val, uniq_set[u], none_to, num_l, num_r, num_nan)\n",
    "            attr_dicts.append(attr_loc)\n",
    "#         print(uniq_set[u], max_stat_val)\n",
    "    \n",
    "    if len(attr_dicts) == 0:\n",
    "        return best_attr\n",
    "    best_attr = max(attr_dicts, key=lambda x: x[\"stat_val\"])\n",
    "    best_attr[\"p_value\"] = stats.chi2.sf(best_attr[\"stat_val\"], df=1)\n",
    "    best_attr[\"sign_split\"] = len(attr_dicts)\n",
    "    if best_attr[\"sign_split\"] > 0:\n",
    "        if type_attr == \"cont\":\n",
    "            best_attr[\"values\"] = [f\" >= {best_attr['values']}\", f\" < {best_attr['values']}\"]\n",
    "        elif type_attr == \"categ\":\n",
    "            best_attr[\"values\"] = [f\" in {e}\" for e in best_attr[\"values\"]]\n",
    "        elif type_attr == \"woe\":\n",
    "            ind = descr_np[1] >= best_attr[\"values\"]\n",
    "            l, r = list(descr_np[0, np.where(ind)[0]]), list(descr_np[0, np.where(~ind)[0]])\n",
    "            best_attr[\"values\"] = [f\" in {e}\" for e in [l, r]]\n",
    "        if bonf:\n",
    "            best_attr[\"p_value\"] *= best_attr[\"sign_split\"]\n",
    "        if verbose > 0:\n",
    "            print(best_attr[\"p_value\"], len(uniq_set))\n",
    "    return best_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658cc5a9",
   "metadata": {},
   "source": [
    "### Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b47ffa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dur = np.random.uniform(0, 10000, 10000)\n",
    "cens = np.random.choice(2, 10000)\n",
    "vals = np.random.uniform(100, 8000, 10000)\n",
    "\n",
    "arr = np.vstack([vals, cens, dur])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ac8f570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141 ms ± 489 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit best_attr_split(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bc113cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.4 ms ± 357 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hist_best_attr_split(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cfa380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b61585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
