{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3736631b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalene extension successfully loaded. Note: Scalene currently only\n",
      "supports CPU+GPU profiling inside Jupyter notebooks. For full Scalene\n",
      "profiling, use the command line version.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import pathlib, tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import stats\n",
    "\n",
    "from survivors import metrics as metr\n",
    "from survivors import constants as cnt\n",
    "from survivors import criteria as crit\n",
    "from numba import njit, jit, int32, float64\n",
    "from lifelines import KaplanMeierFitter, NelsonAalenFitter\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "from survivors.ensemble import BootstrapCRAID\n",
    "import survivors.datasets as ds\n",
    "\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "%load_ext line_profiler\n",
    "%load_ext scalene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d892b672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit\n",
    "\n",
    "from scipy import stats\n",
    "from survivors.tree.stratified_model import KaplanMeier, FullProbKM, NelsonAalen, KaplanMeierZeroAfter\n",
    "from survivors.metrics import ibs_WW, auprc\n",
    "from survivors.constants import get_y\n",
    "\n",
    "\"\"\" Auxiliary functions \"\"\"\n",
    "\n",
    "\n",
    "@njit('f4(f4[:], f4[:], f4[:], f4[:], u4, f4[:])', cache=True)\n",
    "def lr_hist_statistic(time_hist_1, time_hist_2, cens_hist_1, cens_hist_2,\n",
    "                      weightings, obs_weights):\n",
    "    N_1_j = np.cumsum(time_hist_1[::-1])[::-1]\n",
    "    N_2_j = np.cumsum(time_hist_2[::-1])[::-1]\n",
    "    \n",
    "#     plt.plot(N_1_j)\n",
    "#     plt.plot(N_2_j)\n",
    "#     plt.show()\n",
    "    \n",
    "    # N_1_j = np.cumsum(time_hist_1)\n",
    "    # N_2_j = np.cumsum(time_hist_2)\n",
    "    ind = np.where((cens_hist_1 + cens_hist_2 != 0) & (N_1_j * N_2_j != 0))[0]\n",
    "    # print(N_1_j[0], N_2_j[0], np.sum(cens_hist_1), np.sum(cens_hist_2))\n",
    "    # print(N_1_j, N_2_j, cens_hist_1, cens_hist_2)\n",
    "    # print(ind)\n",
    "    if ind.shape[0] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    N_1_j = N_1_j[ind]  # + 1\n",
    "    N_2_j = N_2_j[ind]  # + 1\n",
    "    O_1_j = cens_hist_1[ind]\n",
    "    O_2_j = cens_hist_2[ind]\n",
    "\n",
    "    N_j = N_1_j + N_2_j\n",
    "    O_j = O_1_j + O_2_j\n",
    "    E_1_j = N_1_j * O_j / N_j\n",
    "    \n",
    "    res = np.zeros((N_j.shape[0], 3), dtype=np.float32)\n",
    "    res[:, 1] = O_1_j - E_1_j  # np.abs(O_1_j - E_1_j)\n",
    "    res[:, 2] = E_1_j * (N_j - O_j) * N_2_j / (N_j * (N_j - 1))\n",
    "\n",
    "    # res[:, 2] = E_1_j * (N_j - O_j) * N_2_j / (N_j * (N_j - 1))  # TODO\n",
    "    # res[:, 2] = E_1_j * (N_j - O_j + 1) * (N_2_j + 1) / ((N_j + 1) * (N_j))\n",
    "    res[:, 0] = 1.0\n",
    "    if weightings == 2:\n",
    "        res[:, 0] = N_j\n",
    "    elif weightings == 3:\n",
    "        res[:, 0] = np.sqrt(N_j)\n",
    "    elif weightings == 4:\n",
    "        res[:, 0] = np.cumprod((1.0 - O_j / (N_j + 1)))\n",
    "    elif weightings == 5:\n",
    "        res[:, 0] = obs_weights[ind]\n",
    "    elif weightings == 6:\n",
    "        res[:, 0] = O_j/N_j\n",
    "    elif weightings == 7:\n",
    "        res[:, 0] = np.cumprod((1.0 - O_j / (N_j + 1)))\n",
    "    elif weightings == 8:\n",
    "        res[:, 0] = N_j/(N_1_j*N_2_j)\n",
    "        #res[:, 0] = np.cumprod((1.0 - O_1_j / (N_1_j + 1))) - np.cumprod((1.0 - O_2_j / (N_2_j + 1)))\n",
    "    var = (res[:, 0] * res[:, 0] * res[:, 2]).sum()\n",
    "    num = (res[:, 0] * res[:, 1]).sum()\n",
    "    \n",
    "    if var == 0:\n",
    "        return 0\n",
    "    stat_val = np.power(num, 2) / var  # ((res[:, 0] * res[:, 0] * res[:, 2]).sum())\n",
    "    \n",
    "#     print(\"ALL:\", N_j[0], \"(\", N_1_j[0], N_2_j[0], \")\",\n",
    "#           \"OBS:\", np.sum(O_j), \"(\", O_1_j, O_2_j, \")\",  # ind,\n",
    "#           \"WEI:\", res[:, 0],\n",
    "#           # \"OBS:\", np.sum(O_j), \"(\", np.sum(cens_hist_1), np.sum(cens_hist_2), \")\",\n",
    "#           \"STAT:\", stat_val, \"(\", num, var, \")\")\n",
    "\n",
    "    if weightings == 7:\n",
    "        res[:, 0] = 1 - res[:, 0]\n",
    "        stat_val2 = np.power((res[:, 0] * res[:, 1]).sum(), 2) / ((res[:, 0] * res[:, 0] * res[:, 2]).sum())\n",
    "        stat_val = max(stat_val, stat_val2)\n",
    "    return stat_val  # It must be square of value (without sqrt)\n",
    "\n",
    "\n",
    "def RMST_hist_statistic(time_hist_1, time_hist_2, cens_hist_1, cens_hist_2,\n",
    "                      weightings, obs_weights, bins):\n",
    "    N_1_j = np.cumsum(time_hist_1[::-1])[::-1]\n",
    "    N_2_j = np.cumsum(time_hist_2[::-1])[::-1]\n",
    "    \n",
    "    sf_1 = np.cumprod((1.0 - cens_hist_1 / (N_1_j + 1)))\n",
    "    sf_2 = np.cumprod((1.0 - cens_hist_2 / (N_2_j + 1)))\n",
    "    \n",
    "    sf_diff = sf_1 - sf_2\n",
    "    stat_val = np.abs(np.sum(np.trapz(np.tril(np.ones((time_hist_1.shape[0], bins.shape[0])), k=0) * sf_diff, bins)))\n",
    "    return stat_val\n",
    "\n",
    "\n",
    "def weight_hist_stat(time_hist_1, time_hist_2, cens_hist_1=None, cens_hist_2=None, weights_hist=None, weightings=\"\"):\n",
    "    try:\n",
    "        if cens_hist_1 is None:\n",
    "            cens_hist_1 = time_hist_1\n",
    "        if cens_hist_2 is None:\n",
    "            cens_hist_2 = time_hist_2\n",
    "        if weights_hist is None:\n",
    "            weights_hist = np.ones_like(time_hist_1)\n",
    "        d = {\"logrank\": 1, \"wilcoxon\": 2, \"tarone-ware\": 3, \"peto\": 4, \"weights\": 5}\n",
    "        d.update({\"diff\": 6, \"maxcombo\": 7, \"frac\": 8})\n",
    "        weightings = d.get(weightings, 1)\n",
    "        logrank = lr_hist_statistic(time_hist_1.astype(\"float32\"),\n",
    "                                    time_hist_2.astype(\"float32\"),\n",
    "                                    cens_hist_1.astype(\"float32\"),\n",
    "                                    cens_hist_2.astype(\"float32\"),\n",
    "                                    np.uint32(weightings),\n",
    "                                    weights_hist.astype(\"float32\"))\n",
    "        return logrank\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        # print(time_hist_1, time_hist_2, cens_hist_1, cens_hist_2, weights_hist)\n",
    "        return 0.0\n",
    "\n",
    "# def inverse_bincount_sf(time_hist, cens_hist):\n",
    "#     evs = np.repeat(np.arange(1, time_hist.size + 1), cens_hist)\n",
    "#     cns = np.repeat(np.arange(1, time_hist.size + 1), time_hist - cens_hist)\n",
    "    \n",
    "#     return get_y(time=np.hstack([evs, cns]),\n",
    "#                cens=np.hstack([np.ones_like(evs), np.zeros_like(cns)]))\n",
    "\n",
    "# def weight_hist_stat(time_hist_1, time_hist_2, cens_hist_1=None, cens_hist_2=None, weights_hist=None, weightings=\"\"):\n",
    "#     y_1 = inverse_bincount_sf(time_hist_1.astype(int), cens_hist_1.astype(int))\n",
    "#     y_2 = inverse_bincount_sf(time_hist_2.astype(int), cens_hist_2.astype(int))\n",
    "#     y_gl = np.hstack([y_1, y_2])\n",
    "    \n",
    "#     N_1_j = np.cumsum(time_hist_1[::-1])[::-1]\n",
    "#     N_2_j = np.cumsum(time_hist_2[::-1])[::-1]\n",
    "    \n",
    "#     N_j = N_1_j + N_2_j\n",
    "#     O_j = cens_hist_1 + cens_hist_2\n",
    "#     sf_gl = np.cumprod((1.0 - O_j / (N_j + 1)))\n",
    "#     sf_1 = np.cumprod((1.0 - cens_hist_1 / (N_1_j + 1)))\n",
    "#     sf_2 = np.cumprod((1.0 - cens_hist_2 / (N_2_j + 1)))\n",
    "    \n",
    "#     sf_gl = np.repeat(sf_gl[np.newaxis, :], y_gl.shape[0], axis=0)\n",
    "    \n",
    "#     sf_1 = np.repeat(sf_1[np.newaxis, :], y_1.shape[0], axis=0)\n",
    "#     sf_2 = np.repeat(sf_2[np.newaxis, :], y_2.shape[0], axis=0)\n",
    "# #     print(sf_gl, y_gl)\n",
    "\n",
    "# #     ibs_gl = metr.auprc(y_gl, y_gl, sf_gl, np.arange(time_hist_1.shape[0]))\n",
    "#     ibs_1 = metr.ibs_WW(y_1, y_1, sf_1, np.arange(time_hist_1.shape[0]))\n",
    "#     ibs_2 = metr.ibs_WW(y_2, y_2, sf_2, np.arange(time_hist_1.shape[0]))\n",
    "# #     return ibs_gl - ibs_1 + ibs_2 # (ibs_1*y_1.shape[0] + ibs_2*y_2.shape[0])/y_gl.shape[0]\n",
    "#     return - ibs_1*y_1.shape[0] + ibs_2*y_2.shape[0]\n",
    "    \n",
    "    \n",
    "\n",
    "def optimal_criter_split_hist(left_time_hist, left_cens_hist,\n",
    "                              right_time_hist, right_cens_hist,\n",
    "                              na_time_hist, na_cens_hist, weights_hist, criterion, dis_coef):\n",
    "    none_to = 0\n",
    "    max_stat_val = 1.0\n",
    "    \n",
    "    if int(dis_coef) > 1:\n",
    "        dis_coef = int(dis_coef)\n",
    "        left_time_hist = left_time_hist + (dis_coef - 1) * left_cens_hist\n",
    "        right_time_hist = right_time_hist + (dis_coef - 1) * right_cens_hist\n",
    "        na_time_hist = na_time_hist + (dis_coef - 1) * na_cens_hist\n",
    "\n",
    "        left_cens_hist = left_cens_hist * dis_coef\n",
    "        right_cens_hist = right_cens_hist * dis_coef\n",
    "        na_cens_hist = na_cens_hist * dis_coef\n",
    "    elif dis_coef != 0:\n",
    "        if int(1/dis_coef) > 1:\n",
    "            cens_dc = int(1/dis_coef)\n",
    "            left_time_hist = (cens_dc - 1)*left_time_hist + left_cens_hist\n",
    "            right_time_hist = (cens_dc - 1)*right_time_hist + right_cens_hist\n",
    "            na_time_hist = (cens_dc - 1)*na_time_hist + (dis_coef - 1) * na_cens_hist\n",
    "\n",
    "            left_cens_hist = left_cens_hist * cens_dc\n",
    "            right_cens_hist = right_cens_hist * cens_dc\n",
    "            na_cens_hist = na_cens_hist * cens_dc\n",
    "    \n",
    "    if na_time_hist.shape[0] > 0:\n",
    "        a = weight_hist_stat(left_time_hist + na_time_hist, right_time_hist,\n",
    "                             left_cens_hist + na_cens_hist, right_cens_hist,\n",
    "                             weights_hist, weightings=criterion)\n",
    "        b = weight_hist_stat(left_time_hist, right_time_hist + na_time_hist,\n",
    "                             left_cens_hist, right_cens_hist + na_cens_hist,\n",
    "                             weights_hist, weightings=criterion)\n",
    "        # Nans move to a leaf with maximal statistical value\n",
    "        none_to = int(a < b)\n",
    "        max_stat_val = max(a, b)\n",
    "    #         print(a, b)\n",
    "    else:\n",
    "        max_stat_val = weight_hist_stat(left_time_hist, right_time_hist,\n",
    "                                        left_cens_hist, right_cens_hist,\n",
    "                                        weights_hist, weightings=criterion)\n",
    "    return (max_stat_val, none_to)\n",
    "\n",
    "\n",
    "def split_time_to_bins(time):\n",
    "#     return np.searchsorted(np.quantile(time, np.arange(6)/5), time)\n",
    "    return np.searchsorted(np.unique(time), time)\n",
    "\n",
    "\n",
    "def get_attrs(max_stat_val, values, none_to, l_sh, r_sh, nan_sh):\n",
    "    attrs = dict()\n",
    "    attrs[\"stat_val\"] = max_stat_val\n",
    "    attrs[\"values\"] = values\n",
    "    if none_to:\n",
    "        attrs[\"pos_nan\"] = [0, 1]\n",
    "        attrs[\"min_split\"] = min(l_sh, r_sh + nan_sh)\n",
    "    else:\n",
    "        attrs[\"pos_nan\"] = [1, 0]\n",
    "        attrs[\"min_split\"] = min(l_sh + nan_sh, r_sh)\n",
    "    return attrs\n",
    "\n",
    "\n",
    "def transform_woe_np(x_feat, y):\n",
    "    N_T = y.shape[0]\n",
    "    N_D = y.sum()\n",
    "    N_D_ = N_T - N_D\n",
    "    x_uniq = np.unique(x_feat)\n",
    "    x_dig = np.digitize(x_feat, x_uniq) - 1\n",
    "\n",
    "    df_woe_iv = np.vstack([np.bincount(x_dig[y == 0], minlength=x_uniq.shape[0]),\n",
    "                           np.bincount(x_dig[y == 1], minlength=x_uniq.shape[0])])\n",
    "    all_0 = df_woe_iv[0].sum()\n",
    "    all_1 = df_woe_iv[1].sum()\n",
    "\n",
    "    p_bd = (df_woe_iv[1] + 1e-5) / (N_D + 1e-5)\n",
    "    p_bd_ = (df_woe_iv[0] + 1e-5) / (N_D_ + 1e-5)\n",
    "    p_b_d = (all_1 - df_woe_iv[1] + 1e-5) / (N_D + 1e-5)\n",
    "    p_b_d_ = (all_0 - df_woe_iv[0] + 1e-5) / (N_D_ + 1e-5)\n",
    "\n",
    "    woe_pl = np.log(p_bd / p_bd_)\n",
    "    woe_mn = np.log(p_b_d / p_b_d_)\n",
    "    descr_np = np.vstack([x_uniq, woe_pl - woe_mn])\n",
    "    features_woe = dict(zip(descr_np[0], descr_np[1]))\n",
    "    woe_x_feat = np.vectorize(features_woe.get)(x_feat)\n",
    "    # calculate information value\n",
    "    # iv = ((p_bd - p_bd_)*woe_pl).sum()\n",
    "    return (woe_x_feat, descr_np)\n",
    "\n",
    "\n",
    "def get_sa_hists(time, cens, minlength=1, weights=None):\n",
    "    if time.shape[0] > 0:\n",
    "        time_hist = np.bincount(time, minlength=minlength)\n",
    "        cens_hist = np.bincount(time, weights=cens, minlength=minlength)\n",
    "#         time_hist = np.bincount(time, weights=weights, minlength=minlength)\n",
    "#         cens_hist = np.bincount(time, weights=cens*weights, minlength=minlength)\n",
    "    else:\n",
    "        time_hist, cens_hist = np.array([]), np.array([])\n",
    "    return time_hist, cens_hist\n",
    "\n",
    "\n",
    "def select_best_split_info(attr_dicts, type_attr, bonf=True, descr_woe=None):\n",
    "    best_attr = max(attr_dicts, key=lambda x: x[\"stat_val\"])\n",
    "#     plt.scatter([d[\"values\"] for d in attr_dicts], [d[\"stat_val\"] for d in attr_dicts])\n",
    "#     plt.vlines([best_attr[\"values\"]], ymin=0, ymax=best_attr[\"stat_val\"], color=\"orange\")\n",
    "#     plt.show()\n",
    "    \n",
    "    best_attr[\"p_value\"] = stats.chi2.sf(best_attr[\"stat_val\"], df=1)\n",
    "    best_attr[\"sign_split\"] = len(attr_dicts)\n",
    "    if best_attr[\"sign_split\"] > 0:\n",
    "        best_attr[\"src_val\"] = best_attr['values']\n",
    "        if type_attr == \"cont\":\n",
    "            best_attr[\"values\"] = [f\" <= {best_attr['values']}\", f\" > {best_attr['values']}\"]\n",
    "        # elif type_attr == \"categ\":\n",
    "        #     best_attr[\"values\"] = [f\" in {e}\" for e in best_attr[\"values\"]]\n",
    "        elif type_attr == \"woe\" or type_attr == \"categ\":\n",
    "            ind = descr_woe[1] <= best_attr[\"values\"]\n",
    "            l, r = list(descr_woe[0, ind]), list(descr_woe[0, ~ind])\n",
    "            best_attr[\"values\"] = [f\" in {e}\" for e in [l, r]]\n",
    "        if bonf:\n",
    "            best_attr[\"p_value\"] *= best_attr[\"sign_split\"]\n",
    "    return best_attr\n",
    "\n",
    "\n",
    "def hist_best_attr_split(arr, criterion=\"logrank\", type_attr=\"cont\", weights=None, thres_cont_bin_max=100,\n",
    "                         signif=1.0, signif_stat=0.0, min_samples_leaf=10, bonf=True, verbose=0, balance=False, **kwargs):\n",
    "    best_attr = {\"stat_val\": signif_stat, \"p_value\": signif,\n",
    "                 \"sign_split\": 0, \"values\": [], \"pos_nan\": [1, 0]}\n",
    "    if arr.shape[1] < 2 * min_samples_leaf:\n",
    "        return best_attr\n",
    "    vals = arr[0].astype(\"float\")\n",
    "    cens = arr[1].astype(\"uint\")\n",
    "    dur = arr[2].astype(\"float\")\n",
    "    \n",
    "    if np.sum(cens) == 0:\n",
    "#         cens = np.ones_like(dur)\n",
    "        return best_attr\n",
    "    if weights is None:\n",
    "        weights = np.ones_like(dur)\n",
    "        '''\n",
    "        kmf = KaplanMeierZeroAfter()\n",
    "        kmf.fit(dur, cens)\n",
    "        dd = np.unique(dur)\n",
    "        sf = kmf.survival_function_at_times(dd)\n",
    "        sf = np.repeat(sf[np.newaxis, :], dur.shape[0], axis=0)\n",
    "\n",
    "        y = get_y(cens=cens, time=dur)\n",
    "        weights = ibs_WW(y, y, sf, dd, axis=0)\n",
    "        '''\n",
    "        \n",
    "    weights_hist = None\n",
    "    dur = split_time_to_bins(dur)\n",
    "    max_bin = dur.max()\n",
    "\n",
    "    ind = np.isnan(vals)\n",
    "\n",
    "    # split nan and not-nan\n",
    "    dur_notna = dur[~ind]\n",
    "    cens_notna = cens[~ind]\n",
    "    vals_notna = vals[~ind]\n",
    "    weights_notna = weights[~ind]\n",
    "\n",
    "    dis_coef = 1\n",
    "    if balance:\n",
    "        dis_coef = (cens.shape[0] - np.sum(cens)) / np.sum(cens)\n",
    "        # dis_coef = max(1, (cens.shape[0] - np.sum(cens)) // np.sum(cens))\n",
    "\n",
    "    if dur_notna.shape[0] < min_samples_leaf:\n",
    "        return best_attr\n",
    "\n",
    "    descr_woe = None\n",
    "    if type_attr == \"woe\" or type_attr == \"categ\":\n",
    "        vals_notna, descr_woe = transform_woe_np(vals_notna, cens_notna)\n",
    "\n",
    "    # find splitting values\n",
    "    uniq_set = np.unique(vals_notna)\n",
    "    if uniq_set.shape[0] > thres_cont_bin_max:\n",
    "        uniq_set = np.quantile(vals_notna, [i / float(thres_cont_bin_max) for i in range(1, thres_cont_bin_max)])\n",
    "    else:\n",
    "        uniq_set = (uniq_set[:-1] + uniq_set[1:]) * 0.5\n",
    "    uniq_set = np.unique(np.round(uniq_set, 3))\n",
    "\n",
    "    index_vals_bin = np.digitize(vals_notna, uniq_set, right=True)\n",
    "\n",
    "    # find global hist by times\n",
    "    na_time_hist, na_cens_hist = get_sa_hists(dur[ind], cens[ind],\n",
    "                                              minlength=max_bin + 1, weights=weights[ind])\n",
    "\n",
    "    r_time_hist, r_cens_hist = get_sa_hists(dur_notna, cens_notna,\n",
    "                                            minlength=max_bin + 1, weights=weights_notna)\n",
    "    l_time_hist = np.zeros_like(r_time_hist, dtype=np.float32)\n",
    "    l_cens_hist = l_time_hist.copy()\n",
    "\n",
    "    num_nan = ind.sum()\n",
    "    num_r = dur_notna.shape[0]\n",
    "    num_l = 0\n",
    "\n",
    "    if criterion == \"confident\" or criterion == \"confident_weights\":\n",
    "        kmf = KaplanMeier()\n",
    "        if criterion == \"confident_weights\":\n",
    "            kmf.fit(dur, cens, weights=weights)\n",
    "        else:\n",
    "            kmf.fit(dur, cens)\n",
    "        ci = kmf.get_confidence_interval_()\n",
    "        weights_hist = 1 / (ci[1:, 1] - ci[1:, 0] + 1)  # (ci[1:, 1] + ci[1:, 0] + 1e-5)\n",
    "        criterion = \"weights\"\n",
    "    elif criterion == \"fullprob\":\n",
    "        kmf = FullProbKM()\n",
    "        kmf.fit(dur, cens)\n",
    "        weights_hist = kmf.survival_function_at_times(np.unique(dur))\n",
    "        criterion = \"weights\"\n",
    "    elif criterion == \"ibswei\":\n",
    "        kmf = KaplanMeierZeroAfter()\n",
    "        dur_ = arr[2].copy()\n",
    "        kmf.fit(dur_, cens)\n",
    "\n",
    "        dd = np.unique(dur_)\n",
    "        sf = kmf.survival_function_at_times(dd)\n",
    "        sf = np.repeat(sf[np.newaxis, :], dd.shape[0], axis=0)\n",
    "\n",
    "        y_ = get_y(cens=np.ones_like(dd), time=dd)\n",
    "        y_[\"cens\"] = True\n",
    "        ibs_ev = ibs_WW(y_, y_, sf, dd, axis=0)\n",
    "        y_[\"cens\"] = False\n",
    "        ibs_cn = ibs_WW(y_, y_, sf, dd, axis=0)\n",
    "\n",
    "        ratio = np.sum(cens)/cens.shape[0]\n",
    "        weights_hist = ibs_ev*ratio + ibs_cn*(1-ratio)\n",
    "        criterion = \"weights\"\n",
    "    elif criterion == \"T-ET\":\n",
    "        kmf = KaplanMeierZeroAfter()\n",
    "        dur_ = arr[2].copy()\n",
    "        kmf.fit(dur_, cens)\n",
    "\n",
    "        dd = np.unique(dur_)\n",
    "        ET = np.trapz(kmf.survival_function_at_times(dd), dd)\n",
    "        weights_hist = (dd - ET)  # **2\n",
    "        criterion = \"weights\"\n",
    "    elif criterion == \"kde\":\n",
    "        na = NelsonAalen()\n",
    "        na.fit(dur, cens, np.ones(len(dur)))\n",
    "        weights_hist = na.get_smoothed_hazard_at_times(np.unique(dur))\n",
    "        criterion = \"weights\"\n",
    "    elif criterion == \"weights\":\n",
    "        weights_hist = np.bincount(dur, weights=weights,  # /sum(weights),\n",
    "                                   minlength=max_bin + 1)\n",
    "        #weights_hist /= np.bincount(dur, minlength=max_bin + 1)  # np.sqrt()\n",
    "        weights_hist = np.cumsum(weights_hist[::-1])[::-1]  # np.sqrt()\n",
    "\n",
    "        weights_hist = weights_hist / weights_hist.sum()\n",
    "\n",
    "    # for each split values get branches\n",
    "    attr_dicts = []\n",
    "    \n",
    "#     min_times = []\n",
    "#     stat_vals = []\n",
    "    for u in np.unique(index_vals_bin)[:-1]:\n",
    "        curr_mask = index_vals_bin == u\n",
    "        curr_n = curr_mask.sum()\n",
    "        curr_time_hist, curr_cens_hist = get_sa_hists(dur_notna[curr_mask], cens_notna[curr_mask],\n",
    "                                                      minlength=max_bin + 1, weights=weights_notna[curr_mask])\n",
    "        l_time_hist += curr_time_hist\n",
    "        l_cens_hist += curr_cens_hist\n",
    "        r_time_hist -= curr_time_hist\n",
    "        r_cens_hist -= curr_cens_hist\n",
    "        num_l += curr_n\n",
    "        num_r -= curr_n\n",
    "\n",
    "        if min(num_l, num_r) <= min_samples_leaf:\n",
    "            continue\n",
    "        \n",
    "        max_stat_val, none_to = optimal_criter_split_hist(\n",
    "            l_time_hist, l_cens_hist, r_time_hist, r_cens_hist,\n",
    "            na_time_hist, na_cens_hist, weights_hist, criterion, dis_coef)\n",
    "#         t1 = np.max(np.nonzero(l_time_hist))\n",
    "#         t2 = np.max(np.nonzero(r_time_hist))\n",
    "#         min_times.append(min(t1, t2))\n",
    "#         stat_vals.append(max_stat_val)\n",
    "        \n",
    "        if max_stat_val > signif_stat:\n",
    "            attr_loc = get_attrs(max_stat_val, uniq_set[u], none_to, num_l, num_r, num_nan)\n",
    "            attr_dicts.append(attr_loc)\n",
    "            \n",
    "#     plt.scatter(min_times, stat_vals)\n",
    "#     plt.show()\n",
    "    if len(attr_dicts) == 0:\n",
    "        return best_attr\n",
    "    best_attr = select_best_split_info(attr_dicts, type_attr, bonf, descr_woe=descr_woe)\n",
    "    \n",
    "#     plt.plot(weights_hist)\n",
    "#     plt.show()\n",
    "    \n",
    "    '''\n",
    "    fig, axes = plt.subplots(ncols=2)\n",
    "    axes[0].plot(np.cumsum((l_time_hist + r_time_hist)[::-1])[::-1])\n",
    "    axes[1].plot(l_cens_hist + r_cens_hist)\n",
    "    axes[1].plot(l_time_hist + r_time_hist - (l_cens_hist + r_cens_hist), color=\"orange\")\n",
    "    plt.show()\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    fig, axes = plt.subplots(ncols=2)\n",
    "    cols = np.where(vals_notna <= best_attr[\"src_val\"], \"blue\", \"orange\")\n",
    "    axes[1].scatter(vals_notna, weights_notna, c=cols)\n",
    "    axes[0].scatter(dur_notna, weights_notna, c=cols)\n",
    "    plt.show()\n",
    "    '''\n",
    "    \n",
    "    if verbose > 0:\n",
    "        print(best_attr[\"p_value\"], len(uniq_set))\n",
    "    return best_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "231779ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from survivors.tree.node import Node, Rule\n",
    "from survivors.tree import CRAID\n",
    "\n",
    "class Node1(Node):\n",
    "    def find_best_split(self):\n",
    "        numb_feats = self.info[\"max_features\"]\n",
    "        numb_feats = np.clip(numb_feats, 1, len(self.features))\n",
    "        n_jobs = min(numb_feats, self.info[\"n_jobs\"])\n",
    "\n",
    "        selected_feats = list(np.random.choice(self.features, size=numb_feats, replace=False))\n",
    "        # args = self.get_comb(selected_feats)\n",
    "        args = self.get_comb_fast(selected_feats)\n",
    "\n",
    "        ml = np.vectorize(lambda x: hist_best_attr_split(**x))(args)\n",
    "\n",
    "        # with Parallel(n_jobs=n_jobs, verbose=self.verbose, batch_size=10) as parallel:  # prefer=\"threads\"\n",
    "        #     ml = parallel(delayed(hist_best_attr_split)(**a) for a in args)  # hist_best_attr_split\n",
    "        attrs = {f: ml[ind] for ind, f in enumerate(selected_feats)}\n",
    "\n",
    "# #         attr = min(attrs, key=lambda x: attrs[x][\"p_value\"])\n",
    "        attr = max(attrs, key=lambda x: attrs[x][\"stat_val\"])\n",
    "        \n",
    "#         print(\"STAT_VAL:\", [v[\"stat_val\"] for k, v in attrs.items()])\n",
    "#         print(\"SIGN_SPLIT:\", [v[\"sign_split\"] for k, v in attrs.items()])\n",
    "#         print(\"P_VALUES:\", [v[\"p_value\"] for k, v in attrs.items()])\n",
    "        \n",
    "#         attrs_gr = dict(filter(lambda x: x[1][\"sign_split\"] > 0, attrs.items()))\n",
    "#         if len(attrs_gr) == 0:\n",
    "#             attr = min(attrs, key=lambda x: attrs[x][\"p_value\"])\n",
    "#         else:\n",
    "#             attr = min(attrs_gr, key=lambda x: attrs_gr[x][\"p_value\"])\n",
    "#             if self.info[\"bonf\"]:\n",
    "#                 attrs[attr][\"p_value\"] = attrs[attr][\"p_value\"] / attrs[attr][\"sign_split\"]\n",
    "            \n",
    "#         if attrs[attr][\"sign_split\"] > 0 and self.info[\"bonf\"]:\n",
    "#             attrs[attr][\"p_value\"] = attrs[attr][\"p_value\"] / attrs[attr][\"sign_split\"]\n",
    "        return (attr, attrs[attr])\n",
    "    \n",
    "    def split(self):\n",
    "#         print(\"TIME:\", self.df[cnt.TIME_NAME].to_numpy())\n",
    "#         print(\"TIME:\", self.df[cnt.CENS_NAME].to_numpy())\n",
    "#         bins = cnt.get_bins(time=self.df[cnt.TIME_NAME].to_numpy(),\n",
    "#                             cens=self.df[cnt.CENS_NAME].to_numpy())\n",
    "#         y__ = cnt.get_y(cens=self.df[cnt.CENS_NAME].to_numpy(), \n",
    "#                         time=self.df[cnt.TIME_NAME].to_numpy())\n",
    "#         sf = self.predict(self.df, target=\"surv\", bins=bins)\n",
    "        \n",
    "#         print(f\"NODE {self.numb}:\", metr.ibs_remain(y__, y__, sf, bins))\n",
    "        \n",
    "        node_edges = np.array([], dtype=int)\n",
    "        self.rule_edges = np.array([], dtype=Rule)\n",
    "        \n",
    "        attr, best_split = self.find_best_split()\n",
    "        \n",
    "        # The best split is not significant\n",
    "        if best_split[\"sign_split\"] == 0:\n",
    "            if self.verbose > 0:\n",
    "                print(f'Конец ветви, незначащее p-value: {best_split[\"p_value\"]}')\n",
    "            return node_edges\n",
    "        if self.verbose > 0:\n",
    "            print('='*6, best_split[\"p_value\"], attr)\n",
    "\n",
    "        branch_ind = self.ind_for_nodes(self.df[attr], best_split, attr in self.categ)\n",
    "\n",
    "        for n_b in np.unique(branch_ind):\n",
    "            rule = Rule(feature=attr,\n",
    "                        condition=best_split[\"values\"][n_b],\n",
    "                        has_nan=best_split[\"pos_nan\"][n_b])\n",
    "            d_node = self.df[branch_ind == n_b].copy()\n",
    "            N = Node1(df=d_node, full_rule=self.full_rule + [rule],\n",
    "                     features=self.features, categ=self.categ,\n",
    "                     depth=self.depth + 1, verbose=self.verbose, **self.info)\n",
    "            node_edges = np.append(node_edges, N)\n",
    "            self.rule_edges = np.append(self.rule_edges, rule)\n",
    "\n",
    "        if self.rule_edges.shape[0] == 1:\n",
    "            print(branch_ind, self.df[attr], best_split, attr in self.categ)\n",
    "            raise ValueError('ERROR: Only one branch created!')\n",
    "\n",
    "        return node_edges\n",
    "\n",
    "class CRAID1(CRAID):\n",
    "    def fit(self, X, y):\n",
    "        if len(self.features) == 0:\n",
    "            self.features = X.columns\n",
    "        self.bins = cnt.get_bins(time=y[cnt.TIME_NAME])  # , cens = y[cnt.CENS_NAME])\n",
    "        X = X.reset_index(drop=True)\n",
    "        X_tr = X.copy()\n",
    "        X_tr[cnt.CENS_NAME] = y[cnt.CENS_NAME].astype(np.int32)\n",
    "        X_tr[cnt.TIME_NAME] = y[cnt.TIME_NAME].astype(np.float32)\n",
    "\n",
    "        if not (\"min_samples_leaf\" in self.info):\n",
    "            self.info[\"min_samples_leaf\"] = 0.01\n",
    "        if isinstance(self.info[\"min_samples_leaf\"], float):\n",
    "            self.info[\"min_samples_leaf\"] = max(int(self.info[\"min_samples_leaf\"] * X_tr.shape[0]), 1)\n",
    "\n",
    "        cnt.set_seed(self.random_state)\n",
    "\n",
    "        if self.balance in [\"balance\", \"balance+correct\"]:\n",
    "            freq = X_tr[cnt.CENS_NAME].value_counts()\n",
    "            self.correct_proba = freq[1] / (freq[1] + freq[0])  # or freq[1] / (freq[0])\n",
    "\n",
    "            X_tr = get_oversample(X_tr, target=cnt.CENS_NAME)\n",
    "        elif self.balance in [\"balance+weights\"]:\n",
    "            freq = X_tr[cnt.CENS_NAME].value_counts()\n",
    "\n",
    "            X_tr[\"weights_obs\"] = np.where(X_tr[cnt.CENS_NAME], freq[0] / freq[1], 1)\n",
    "            self.info[\"weights_feature\"] = \"weights_obs\"\n",
    "        elif self.balance in [\"only_log_rank\"]:\n",
    "            self.info[\"balance\"] = True\n",
    "\n",
    "        if self.cut:\n",
    "            X_val = X_tr.sample(n=int(0.2 * X_tr.shape[0]), random_state=self.random_state)\n",
    "            X_tr = X_tr.loc[X_tr.index.difference(X_val.index), :]\n",
    "\n",
    "        self.nodes[0] = Node1(X_tr, features=self.features, categ=self.categ, **self.info)\n",
    "        stack_nodes = np.array([0], dtype=int)\n",
    "        while stack_nodes.shape[0] > 0:\n",
    "            node = self.nodes[stack_nodes[0]]\n",
    "            stack_nodes = stack_nodes[1:]\n",
    "            if node.depth >= self.depth:\n",
    "                continue\n",
    "            sub_nodes = node.split()\n",
    "            if sub_nodes.shape[0] > 0:\n",
    "                sub_numbers = np.array([len(self.nodes) + i for i in range(sub_nodes.shape[0])])\n",
    "                for i in range(sub_nodes.shape[0]):\n",
    "                    sub_nodes[i].numb = sub_numbers[i]\n",
    "                self.nodes.update(dict(zip(sub_numbers, sub_nodes)))\n",
    "                node.set_edges(sub_numbers)\n",
    "                stack_nodes = np.append(stack_nodes, sub_numbers)\n",
    "\n",
    "        if self.cut:\n",
    "            self.cut_tree(X_val, cnt.CENS_NAME, mode_f=roc_auc_score, choose_f=max)\n",
    "\n",
    "        # self.fit_cox_hazard(X, y)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "62e3ee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BootstrapCRAID1(BootstrapCRAID):\n",
    "    def fit(self, X, y):\n",
    "        self.features = X.columns\n",
    "        X = X.reset_index(drop=True)\n",
    "        X[cnt.CENS_NAME] = y[cnt.CENS_NAME].astype(np.int32)\n",
    "        X[cnt.TIME_NAME] = y[cnt.TIME_NAME].astype(np.float32)\n",
    "\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        self.update_params()\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            x_sub = self.X_train.sample(n=self.size_sample, replace=self.bootstrap, random_state=i)\n",
    "            x_oob = self.X_train.loc[self.X_train.index.difference(x_sub.index), :]\n",
    "\n",
    "            x_sub = x_sub.reset_index(drop=True)\n",
    "            X_sub_tr, y_sub_cr = cnt.pd_to_xy(x_sub)\n",
    "\n",
    "            model = CRAID1(features=self.features, random_state=i, **self.tree_kwargs)\n",
    "            model.fit(X_sub_tr, y_sub_cr)\n",
    "\n",
    "            self.add_model(model, x_oob)\n",
    "        print(f\"fitted: {len(self.models)} models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9d1f743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from survivors.ensemble import BoostingCRAID\n",
    "\n",
    "class IBSCleverBoostingCRAID1(BoostingCRAID):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.name = \"IBSCleverBoostingCRAID\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.features = X.columns\n",
    "        X = X.reset_index(drop=True)\n",
    "        X[cnt.CENS_NAME] = y[cnt.CENS_NAME].astype(np.int32)\n",
    "        X[cnt.TIME_NAME] = y[cnt.TIME_NAME].astype(np.float32)\n",
    "\n",
    "        self.X_train = X\n",
    "        self.X_train[\"ind_start\"] = self.X_train.index\n",
    "        self.y_train = y\n",
    "\n",
    "        self.weights = np.ones(self.X_train.shape[0], dtype=float)\n",
    "        self.bettas = []\n",
    "        self.l_ibs = []\n",
    "        self.l_weights = []\n",
    "        self.update_params()\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            x_sub = self.X_train.sample(n=self.size_sample, \n",
    "                                        weights=self.weights,\n",
    "                                        replace=self.bootstrap, random_state=i)\n",
    "            \n",
    "            x_oob = self.X_train.loc[self.X_train.index.difference(x_sub.index), :]\n",
    "            print(f\"UNIQUE ({i}):{np.unique(x_sub.index).shape[0]}, DIST:\", np.bincount(x_sub[\"cens\"]))\n",
    "            x_sub = x_sub.reset_index(drop=True)\n",
    "            X_sub_tr, y_sub_tr = cnt.pd_to_xy(x_sub)\n",
    "            if self.weighted_tree:\n",
    "                X_sub_tr[\"weights_obs\"] = self.weights[x_sub['ind_start']]\n",
    "            \n",
    "#             plt.scatter(y_sub_tr[\"time\"], self.weights[x_sub['ind_start']])\n",
    "#             plt.show()\n",
    "            \n",
    "            model = CRAID1(features=self.features, random_state=i, **self.tree_kwargs)\n",
    "            model.fit(X_sub_tr, y_sub_tr)\n",
    "\n",
    "            wei_i, betta_i = self.count_model_weights(model, X_sub_tr, y_sub_tr)\n",
    "            self.add_model(model, x_oob, wei_i, betta_i)\n",
    "            self.update_weight(x_sub['ind_start'], wei_i)\n",
    "\n",
    "    def predict(self, x_test, aggreg=True, **kwargs):\n",
    "        res = []\n",
    "        weights = []\n",
    "        for i in range(len(self.models)):\n",
    "            res.append(self.models[i].predict(x_test, **kwargs))\n",
    "\n",
    "        res = np.array(res)\n",
    "        weights = None # np.vstack(weights).T\n",
    "        if aggreg:\n",
    "            res = self.get_aggreg(res, weights)\n",
    "        return res\n",
    "\n",
    "    def predict_at_times(self, x_test, bins, aggreg=True, mode=\"surv\"):\n",
    "        res = []\n",
    "        weights = []\n",
    "        for i in range(len(self.models)):\n",
    "            res.append(self.models[i].predict_at_times(x_test, bins=bins,\n",
    "                                                       mode=mode))\n",
    "\n",
    "        res = np.array(res)\n",
    "        weights = None # np.vstack(weights).T\n",
    "        if aggreg:\n",
    "            res = self.get_aggreg(res, weights)\n",
    "            if mode == \"surv\":\n",
    "                res[:, -1] = 0\n",
    "                res[:, 0] = 1\n",
    "        return res\n",
    "\n",
    "    def count_model_weights(self, model, X_sub, y_sub):\n",
    "        if self.all_weight:\n",
    "            X_sub = self.X_train\n",
    "            y_sub = self.y_train\n",
    "        pred_sf = model.predict_at_times(X_sub, bins=self.bins, mode=\"surv\")\n",
    "        \n",
    "        # PRED WEI!!!\n",
    "#         ibs_sf = metr.ibs_WW(self.y_train, y_sub, pred_sf, self.bins, axis=0)\n",
    "        \n",
    "#         if len(self.bettas) > 0:\n",
    "#             pred_ens = self.predict_at_times(X_sub, bins=self.bins, mode=\"surv\")\n",
    "#             ibs_ens = metr.ibs_WW(self.y_train, y_sub, pred_ens, self.bins)\n",
    "#             betta = ibs_ens / np.mean(ibs_sf)\n",
    "#         else:\n",
    "#             betta = 1\n",
    "#         wei = ibs_sf\n",
    "        \n",
    "        ibs_sf = metr.auprc(self.y_train, y_sub, pred_sf, self.bins, axis=0)\n",
    "        betta = np.mean(ibs_sf)\n",
    "        wei = 1 - ibs_sf\n",
    "#         if len(self.bettas) > 0:\n",
    "#             pred_ens = self.predict_at_times(X_sub, bins=self.bins, mode=\"surv\")\n",
    "            \n",
    "#             ibs_ens = metr.ibs_WW(self.y_train, y_sub, pred_ens, self.bins)\n",
    "            \n",
    "#             betta = ibs_ens / np.mean(ibs_sf)\n",
    "#             # betta = ibs_ens / np.mean(ibs_sf)\n",
    "#             print(betta, np.mean(ibs_sf), np.std(ibs_sf))\n",
    "#             wei = (ibs_ens + (betta**2) * ibs_sf)/(1+ betta)**2\n",
    "#         else:\n",
    "#             betta = 1\n",
    "#             wei = ibs_sf\n",
    "        return wei, abs(betta)\n",
    "\n",
    "    def update_weight(self, index, wei_i):\n",
    "        # PRED WEI!!!\n",
    "#         if len(self.models) > 1:\n",
    "#             self.weights = self.weights + (self.bettas[-1]**2) * wei_i\n",
    "#             self.weights /= (1 + self.bettas[-1])**2\n",
    "#             self.bettas = list(np.array(self.bettas)/np.sum(self.bettas))\n",
    "#         else:\n",
    "#             self.weights = wei_i\n",
    "        self.weights += wei_i\n",
    "\n",
    "    def get_aggreg(self, x, wei=None):\n",
    "        if self.aggreg_func == 'median':\n",
    "            return np.median(x, axis=0)\n",
    "        elif self.aggreg_func == \"wei\":\n",
    "            if wei is None:\n",
    "                wei = np.array(self.bettas)\n",
    "            wei = wei / np.sum(wei)\n",
    "            return np.sum((x.T * wei).T, axis=0)\n",
    "        elif self.aggreg_func == \"argmean\":\n",
    "            wei = np.where(np.argsort(np.argsort(wei, axis=1), axis=1) > len(self.bettas)//2, 1, 0)\n",
    "            wei = wei / np.sum(wei, axis=1).reshape(-1, 1)\n",
    "            return np.sum((x.T * wei).T, axis=0)\n",
    "        elif self.aggreg_func == \"argwei\":\n",
    "            wei = np.where(np.argsort(np.argsort(wei, axis=1), axis=1) > len(self.bettas)//2, 1/np.array(self.bettas), 0)\n",
    "            wei = wei / np.sum(wei, axis=1).reshape(-1, 1)\n",
    "            return np.sum((x.T * wei).T, axis=0)\n",
    "        return np.mean(x, axis=0)\n",
    "\n",
    "    def plot_curve(self, X_tmp, y_tmp, bins, label=\"\", metric=\"ibs\"):\n",
    "        res = []\n",
    "        metr_vals = []\n",
    "        for i in range(len(self.models)):\n",
    "            res.append(self.models[i].predict_at_times(X_tmp, bins=bins, mode=\"surv\"))\n",
    "\n",
    "            res_all = np.array(res)\n",
    "            res_all = self.get_aggreg(res_all, np.array(self.bettas)[:i+1])\n",
    "            res_all[:, -1] = 0\n",
    "            res_all[:, 0] = 1\n",
    "            if metric == \"ibs\":\n",
    "                metr_vals.append(metr.ibs_WW(self.y_train, y_tmp, res_all, bins))\n",
    "            else:\n",
    "                metr_vals.append(metr.auprc(self.y_train, y_tmp, res_all, bins))\n",
    "        plt.plot(range(len(self.models)), metr_vals, label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9d7c8cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from survivors.ensemble import BoostingCRAID\n",
    "\n",
    "class IBSCRAID1(CRAID1):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.ibs_leaf = None\n",
    "\n",
    "    def set_ibs_by_leaf(self, X, y):\n",
    "        numbs = self.predict(X, target=\"numb\").astype(\"int\")\n",
    "        \n",
    "        sf = self.predict_at_times(X, bins=self.bins, mode=\"surv\")\n",
    "        ibs_ex = metr.auprc(y, y, sf, self.bins, axis=0)\n",
    "        \n",
    "        counts = np.bincount(numbs)\n",
    "        self.ibs_leaf = np.bincount(numbs, weights=ibs_ex)\n",
    "        self.ibs_leaf[counts > 0] /= counts[counts > 0]\n",
    "\n",
    "    def get_ibs_by_leaf(self, X):\n",
    "        numbs = self.predict(X, target=\"numb\").astype(\"int\")\n",
    "        return self.ibs_leaf[numbs]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        super().fit(X, y)\n",
    "        # self.set_ibs_by_leaf(X, y)\n",
    "\n",
    "class IBSCleverBoostingCRAID1(BoostingCRAID):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.name = \"IBSCleverBoostingCRAID\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.features = list(X.columns)\n",
    "        X = X.reset_index(drop=True)\n",
    "        X[cnt.CENS_NAME] = y[cnt.CENS_NAME].astype(np.int32)\n",
    "        X[cnt.TIME_NAME] = y[cnt.TIME_NAME].astype(np.float32)\n",
    "\n",
    "        self.X_train = X.copy()\n",
    "        self.X_train[\"ind_start\"] = self.X_train.index\n",
    "        self.y_train = y\n",
    "\n",
    "        self.weights = np.ones(self.X_train.shape[0], dtype=float)\n",
    "        self.bettas = []\n",
    "        self.l_ibs = []\n",
    "        self.l_weights = []\n",
    "        self.update_params()\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            x_sub = self.X_train.sample(n=self.size_sample, \n",
    "                                        # weights=self.weights,\n",
    "                                        replace=self.bootstrap, random_state=i)\n",
    "            \n",
    "            x_oob = self.X_train.loc[self.X_train.index.difference(x_sub.index), :]\n",
    "            print(f\"UNIQUE ({i}):{np.unique(x_sub.index).shape[0]}, DIST:\", np.bincount(x_sub[\"cens\"]))\n",
    "            x_sub = x_sub.reset_index(drop=True)\n",
    "            X_sub_tr, y_sub_tr = cnt.pd_to_xy(x_sub)\n",
    "            if self.weighted_tree:\n",
    "                X_sub_tr[\"weights_obs\"] = self.weights[x_sub['ind_start']].copy()\n",
    "#                 if len(self.bettas) > 0:\n",
    "#                     X_sub_tr[\"weights_obs\"] /= np.sum(self.bettas)\n",
    "            \n",
    "#             print(np.sum(self.bettas))\n",
    "#             if len(self.bettas) > 0:\n",
    "#                 plt.scatter(self.X_train[\"time\"], self.weights / np.sum(self.bettas), \n",
    "#                             c=np.where(self.X_train[\"cens\"], \"orange\", \"blue\"))\n",
    "#                 plt.show()\n",
    "#             else:\n",
    "            plt.scatter(self.X_train[\"time\"], self.weights, \n",
    "                        c=np.where(self.X_train[\"cens\"], \"orange\", \"blue\"))\n",
    "            plt.show()\n",
    "                \n",
    "            \n",
    "            model = IBSCRAID1(features=self.features.copy(), random_state=i, **self.tree_kwargs)\n",
    "            model.fit(X_sub_tr, y_sub_tr)\n",
    "\n",
    "            wei_i, betta_i = self.count_model_weights(model, X_sub_tr, y_sub_tr)\n",
    "            self.add_model(model, x_oob, wei_i, betta_i)\n",
    "            self.update_weight(x_sub['ind_start'], wei_i)\n",
    "            #self.X_train[f\"ibs_feature_{i}\"] = model.get_ibs_by_leaf(self.X_train)\n",
    "            #self.features.append(f\"ibs_feature_{i}\")\n",
    "\n",
    "    def predict(self, x_test_, aggreg=True, **kwargs):\n",
    "        res = []\n",
    "        weights = []\n",
    "        x_test = x_test_.copy()\n",
    "        for i in range(len(self.models)):\n",
    "            res.append(self.models[i].predict(x_test, **kwargs))\n",
    "            # x_test[f\"ibs_feature_{i}\"] = self.models[i].get_ibs_by_leaf(x_test)\n",
    "\n",
    "        res = np.array(res)\n",
    "        weights = None # np.vstack(weights).T\n",
    "        if aggreg:\n",
    "            res = self.get_aggreg(res, weights)\n",
    "        return res\n",
    "\n",
    "    def predict_at_times(self, x_test_, bins, aggreg=True, mode=\"surv\"):\n",
    "        res = []\n",
    "        weights = []\n",
    "        x_test = x_test_.copy()\n",
    "        for i in range(len(self.models)):\n",
    "            res.append(self.models[i].predict_at_times(x_test, bins=bins,\n",
    "                                                       mode=mode))\n",
    "            # x_test[f\"ibs_feature_{i}\"] = self.models[i].get_ibs_by_leaf(x_test)\n",
    "\n",
    "        res = np.array(res)\n",
    "        weights = None # np.vstack(weights).T\n",
    "        if aggreg:\n",
    "            res = self.get_aggreg(res, weights)\n",
    "            if mode == \"surv\":\n",
    "                res[:, -1] = 0\n",
    "                res[:, 0] = 1\n",
    "        return res\n",
    "\n",
    "    def count_model_weights(self, model, X_sub, y_sub):\n",
    "        if self.all_weight:\n",
    "            X_sub = self.X_train\n",
    "            y_sub = self.y_train\n",
    "        pred_sf = model.predict_at_times(X_sub, bins=self.bins, mode=\"surv\")\n",
    "        \n",
    "        # PRED WEI!!!\n",
    "        y_sub_ = y_sub.copy()\n",
    "#         y_sub_[\"cens\"] = 1\n",
    "        ibs_sf = metr.ibs_remain(self.y_train, y_sub_, pred_sf, self.bins, axis=0)\n",
    "        \n",
    "#         if len(self.bettas) > 0:\n",
    "#             pred_ens = self.predict_at_times(X_sub, bins=self.bins, mode=\"surv\")\n",
    "#             ibs_ens = metr.ibs_WW(self.y_train, y_sub, pred_ens, self.bins)\n",
    "#             betta = ibs_ens / np.mean(ibs_sf)\n",
    "#         else:\n",
    "#             betta = 1\n",
    "#         wei = ibs_sf\n",
    "\n",
    "#         if len(self.bettas) > 0:\n",
    "#             pred_ens = self.predict_at_times(X_sub, bins=self.bins, mode=\"surv\")\n",
    "            \n",
    "#             ibs_ens = metr.ibs_WW(self.y_train, y_sub, pred_ens, self.bins)\n",
    "            \n",
    "#             betta = ibs_ens / np.mean(ibs_sf)\n",
    "#             # betta = ibs_ens / np.mean(ibs_sf)\n",
    "#             print(betta, np.mean(ibs_sf), np.std(ibs_sf))\n",
    "#             wei = (ibs_ens + (betta**2) * ibs_sf)/(1+ betta)**2\n",
    "#         else:\n",
    "        wei = ibs_sf\n",
    "        # wei = self.weights * ibs_sf / np.sum(self.weights)\n",
    "        betta = 1/np.mean(ibs_sf)\n",
    "        return wei, abs(betta)\n",
    "\n",
    "    def update_weight(self, index, wei_i):\n",
    "        # PRED WEI!!!\n",
    "#         if len(self.models) > 1:\n",
    "#             self.weights = self.weights + (self.bettas[-1]**2) * wei_i\n",
    "#             self.weights /= (1 + self.bettas[-1])**2\n",
    "#             self.bettas = list(np.array(self.bettas)/np.sum(self.bettas))\n",
    "#         else:\n",
    "#             self.weights = wei_i\n",
    "\n",
    "        if len(self.models) > 1:\n",
    "            self.weights += (self.bettas[-1]) * wei_i\n",
    "        else:\n",
    "            self.weights = (self.bettas[-1]) * wei_i\n",
    "        # self.weights = 1 + (wei_i > (len(self.models) / np.sum(self.bettas)))\n",
    "\n",
    "    def get_aggreg(self, x, wei=None):\n",
    "        if self.aggreg_func == 'median':\n",
    "            return np.median(x, axis=0)\n",
    "        elif self.aggreg_func == \"wei\":\n",
    "            if wei is None:\n",
    "                wei = np.array(self.bettas)\n",
    "            wei = wei / np.sum(wei)\n",
    "            return np.sum((x.T * wei).T, axis=0)\n",
    "        elif self.aggreg_func == \"argmean\":\n",
    "            wei = np.where(np.argsort(np.argsort(wei, axis=1), axis=1) > len(self.bettas)//2, 1, 0)\n",
    "            wei = wei / np.sum(wei, axis=1).reshape(-1, 1)\n",
    "            return np.sum((x.T * wei).T, axis=0)\n",
    "        elif self.aggreg_func == \"argwei\":\n",
    "            wei = np.where(np.argsort(np.argsort(wei, axis=1), axis=1) > len(self.bettas)//2, 1/np.array(self.bettas), 0)\n",
    "            wei = wei / np.sum(wei, axis=1).reshape(-1, 1)\n",
    "            return np.sum((x.T * wei).T, axis=0)\n",
    "        return np.mean(x, axis=0)\n",
    "\n",
    "    def plot_curve(self, X_tmp, y_tmp, bins, label=\"\", metric=\"ibs\"):\n",
    "        res = []\n",
    "        metr_vals = []\n",
    "        for i in range(len(self.models)):\n",
    "            res.append(self.models[i].predict_at_times(X_tmp, bins=bins, mode=\"surv\"))\n",
    "\n",
    "            res_all = np.array(res)\n",
    "            res_all = self.get_aggreg(res_all, np.array(self.bettas)[:i+1])\n",
    "            res_all[:, -1] = 0\n",
    "            res_all[:, 0] = 1\n",
    "            if metric == \"ibs\":\n",
    "                metr_vals.append(metr.ibs_WW(self.y_train, y_tmp, res_all, bins))\n",
    "            else:\n",
    "                metr_vals.append(metr.auprc(self.y_train, y_tmp, res_all, bins))\n",
    "        plt.plot(range(len(self.models)), metr_vals, label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3048e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from survivors.ensemble import BoostingCRAID\n",
    "\n",
    "class IBSCleverBoostingCRAID1(BoostingCRAID):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.name = \"IBSCleverBoostingCRAID\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.features = list(X.columns)\n",
    "        X = X.reset_index(drop=True).copy()\n",
    "        X[cnt.CENS_NAME] = y[cnt.CENS_NAME].astype(np.int32)\n",
    "        X[cnt.TIME_NAME] = y[cnt.TIME_NAME].astype(np.float32)\n",
    "\n",
    "        self.X_train = X.copy()\n",
    "        self.X_train[\"ind_start\"] = self.X_train.index\n",
    "        self.y_train = y.copy()\n",
    "\n",
    "        self.weights = y[cnt.TIME_NAME].copy()\n",
    "        self.bettas = []\n",
    "        self.l_ibs = []\n",
    "        self.l_weights = []\n",
    "        self.update_params()\n",
    "        self.lr = 1\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            x_sub = self.X_train.sample(n=self.size_sample, \n",
    "                                        # weights=self.weights,\n",
    "                                        replace=self.bootstrap, random_state=i)\n",
    "            \n",
    "            x_oob = self.X_train.loc[self.X_train.index.difference(x_sub.index), :]\n",
    "            print(f\"UNIQUE ({i}):{np.unique(x_sub.index).shape[0]}, DIST:\", np.bincount(x_sub[\"cens\"]))\n",
    "            x_sub = x_sub.reset_index(drop=True)\n",
    "            X_sub_tr, y_sub_tr = cnt.pd_to_xy(x_sub.copy())\n",
    "            y_sub_tr[cnt.TIME_NAME] = self.weights[x_sub['ind_start'].values].copy()\n",
    "            print(\"TIME CONC:\", concordance_index(x_sub[cnt.TIME_NAME], y_sub_tr[cnt.TIME_NAME]))\n",
    "            \n",
    "            if self.weighted_tree:\n",
    "                X_sub_tr[\"weights_obs\"] = self.weights[x_sub['ind_start']].copy()\n",
    "                \n",
    "            plt.scatter(self.y_train[\"time\"], self.weights, c=np.where(self.y_train[\"cens\"], \"orange\", \"blue\"))\n",
    "            plt.show()\n",
    "            \n",
    "            model = IBSCRAID1(features=self.features.copy(), random_state=i, **self.tree_kwargs)\n",
    "            model.fit(X_sub_tr, y_sub_tr)\n",
    "\n",
    "            wei_i, betta_i = self.count_model_weights(model, X_sub_tr.copy(), y_sub_tr.copy())\n",
    "            self.add_model(model, x_oob, wei_i, betta_i)\n",
    "            self.update_weight(x_sub['ind_start'].values, wei_i)\n",
    "\n",
    "    def predict(self, x_test, aggreg=True, **kwargs):\n",
    "        res = []\n",
    "        weights = []\n",
    "        for i in range(len(self.models)):\n",
    "            res.append(self.models[i].predict(x_test, **kwargs))\n",
    "\n",
    "        res = np.array(res)\n",
    "        weights = None\n",
    "        if aggreg:\n",
    "            res = self.get_aggreg(res, weights)\n",
    "        return res\n",
    "\n",
    "    def predict_at_times(self, x_test, bins, aggreg=True, mode=\"surv\"):\n",
    "        res = []\n",
    "        weights = []\n",
    "        for i in range(len(self.models)):\n",
    "            res.append(self.models[i].predict_at_times(x_test, bins=bins,\n",
    "                                                       mode=mode))\n",
    "\n",
    "        res = np.array(res)\n",
    "        weights = None\n",
    "        if aggreg:\n",
    "            res = self.get_aggreg(res, weights)\n",
    "            if mode == \"surv\":\n",
    "                res[:, -1] = 0\n",
    "                res[:, 0] = 1\n",
    "        return res\n",
    "            \n",
    "#     def count_model_weights(self, model, X_sub, y_sub):\n",
    "#         if self.all_weight:\n",
    "#             X_sub = self.X_train.copy()\n",
    "#             y_sub = self.y_train.copy()\n",
    "#         pred_sf = model.predict_at_times(X_sub, bins=self.bins, mode=\"surv\")\n",
    "            \n",
    "#         y_sub[\"time\"] = self.weights[X_sub['ind_start'].values]\n",
    "#         ind = np.digitize(y_sub[\"time\"], self.bins) - 1\n",
    "#         sf_by_time = np.take_along_axis(pred_sf, ind[:, np.newaxis], axis=1).reshape(-1)\n",
    "        \n",
    "#         wei = 20 * ((1 - sf_by_time)**2 - y_sub[\"cens\"] * sf_by_time**2)\n",
    "#         betta = self.lr\n",
    "#         return wei, abs(betta)\n",
    "    \n",
    "    def count_model_weights(self, model, X_sub, y_sub):\n",
    "        if self.all_weight:\n",
    "            X_sub = self.X_train.copy()\n",
    "            y_sub = self.y_train.copy()\n",
    "        y_sub_ = y_sub.copy()\n",
    "#         pred_time = model.predict(X_sub, target=\"time\")\n",
    "        pred_sf = model.predict_at_times(X_sub, bins=self.bins, mode=\"surv\")\n",
    "            \n",
    "        y_sub[\"time\"] = self.weights[X_sub['ind_start'].values]\n",
    "        pred_time = self.bins[(pred_sf <= 0.5).argmax(axis=1)]\n",
    "        ibs_sf = metr.ibs_remain(self.y_train, y_sub, pred_sf, self.bins, axis=0)\n",
    "        \n",
    "        wei = (y_sub[\"time\"] - pred_time) * ibs_sf**2\n",
    "        betta = self.lr/np.mean(ibs_sf)\n",
    "        return wei, abs(betta)\n",
    "\n",
    "    def update_weight(self, index, wei_i):\n",
    "#         print(index, wei_i)\n",
    "        self.lr = 0.3\n",
    "        if self.all_weight:\n",
    "            self.weights -= wei_i * self.lr\n",
    "        else:\n",
    "            self.weights[index] -= wei_i * self.lr\n",
    "            \n",
    "    def get_aggreg(self, x, wei=None):\n",
    "        if self.aggreg_func == 'median':\n",
    "            return np.median(x, axis=0)\n",
    "        elif self.aggreg_func == \"wei\":\n",
    "            if wei is None:\n",
    "                wei = np.array(self.bettas)\n",
    "            wei = wei / np.sum(wei)\n",
    "            return np.sum((x.T * wei).T, axis=0)\n",
    "        elif self.aggreg_func == \"argmean\":\n",
    "            wei = np.where(np.argsort(np.argsort(wei, axis=1), axis=1) > len(self.bettas)//2, 1, 0)\n",
    "            wei = wei / np.sum(wei, axis=1).reshape(-1, 1)\n",
    "            return np.sum((x.T * wei).T, axis=0)\n",
    "        elif self.aggreg_func == \"argwei\":\n",
    "            wei = np.where(np.argsort(np.argsort(wei, axis=1), axis=1) > len(self.bettas)//2, 1/np.array(self.bettas), 0)\n",
    "            wei = wei / np.sum(wei, axis=1).reshape(-1, 1)\n",
    "            return np.sum((x.T * wei).T, axis=0)\n",
    "        return np.mean(x, axis=0)\n",
    "\n",
    "    \n",
    "    def plot_curve(self, X_tmp, y_tmp, bins, label=\"\", metric=\"ibs\", axes=None):\n",
    "        print(label)\n",
    "        res = []\n",
    "        metr_vals = []\n",
    "        q = [0.05, 0.25, 0.5, 0.75, 0.95]\n",
    "        quantiles = []\n",
    "        \n",
    "        for i in range(len(self.models)):\n",
    "            if metric == \"IAUC_WW_TI\":\n",
    "                res.append(self.models[i].predict_at_times(X_tmp, bins=bins, mode=\"hazard\"))\n",
    "                res_all = np.array(res)\n",
    "                res_all = self.get_aggreg(res_all, np.array(self.bettas)[:i+1])\n",
    "                \n",
    "                metr_vals.append(metr.iauc_WW_TI(self.y_train, y_tmp, res_all, bins))\n",
    "            elif metric == \"CI_CENS\":\n",
    "                res.append(self.models[i].predict(X_tmp, target=\"time\"))\n",
    "                res_all = np.array(res)\n",
    "                res_all = self.get_aggreg(res_all, np.array(self.bettas)[:i+1])\n",
    "\n",
    "                metr_vals.append(concordance_index(y_tmp[cnt.TIME_NAME], res_all, y_tmp[cnt.CENS_NAME]))\n",
    "            else:\n",
    "                res.append(self.models[i].predict_at_times(X_tmp, bins=bins, mode=\"surv\"))\n",
    "                res_all = np.array(res)\n",
    "                res_all = self.get_aggreg(res_all, np.array(self.bettas)[:i+1])\n",
    "                res_all[:, -1] = 0\n",
    "                res_all[:, 0] = 1\n",
    "\n",
    "                if metric == \"AUPRC\":\n",
    "                    metr_by_obs = metr.auprc(self.y_train, y_tmp, res_all, bins, axis=0)\n",
    "                    metr_vals.append(metr.auprc(self.y_train, y_tmp, res_all, bins))\n",
    "                elif metric == \"ibs_WW\":\n",
    "                    metr_by_obs = metr.ibs_WW(self.y_train, y_tmp, res_all, bins, axis=0)\n",
    "                    metr_vals.append(metr.ibs_WW(self.y_train, y_tmp, res_all, bins))\n",
    "                elif metric == \"ibs_REMAIN\":\n",
    "                    metr_by_obs = metr.ibs_remain(self.y_train, y_tmp, res_all, bins, axis=0)\n",
    "                    metr_vals.append(metr.ibs_remain(self.y_train, y_tmp, res_all, bins))\n",
    "                quantiles.append(np.quantile(metr_by_obs, q))\n",
    "        \n",
    "        if axes is None:\n",
    "            fig, axes = plt.subplots(ncols=2)\n",
    "            axes[0].plot(range(len(self.models)), metr_vals, label=label)\n",
    "            axes[1].plot(np.hstack([quantiles]), label=q)\n",
    "            axes[1].legend()\n",
    "        else:\n",
    "            plt.plot(range(len(self.models)), metr_vals, label=label)\n",
    "            plt.xlabel(\"Size of ensemble\")\n",
    "            plt.ylabel(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d720409c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['mean_' + c] = df_agg[c].apply(np.nanmean)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:108: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['min_' + c] = df_agg[c].apply(np.nanmin)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\pandas\\core\\apply.py:1076: RuntimeWarning: All-NaN axis encountered\n",
      "  mapped = lib.map_infer(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['max_' + c] = df_agg[c].apply(np.nanmax)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\datasets\\other.py:111: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_agg['time'] = df_agg.loc[:, ['Admission time', 'Discharge time']].apply(lambda x: (x['Discharge time'] - x['Admission time']).days, axis=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from survivors.experiments.grid import generate_sample, prepare_sample, count_metric\n",
    "\n",
    "\n",
    "# def get_bins(time, cens=None, mode='a', num_bins=100):\n",
    "#     if not(cens is None):\n",
    "#         time = time[np.where(cens)]\n",
    "#     time = time.astype(np.int32)\n",
    "#     bins = np.array([])\n",
    "#     if mode == 'q':\n",
    "#         bins = np.quantile(time, np.arange(num_bins) / num_bins)\n",
    "#     elif mode == 'a':\n",
    "# #         bins = np.arange(time.min(), time.max()+1)  # all bins\n",
    "#         bins = np.unique(np.quantile(time, np.arange(2.5, 97.5) / 100)) \n",
    "# #         t_max = np.quantile(time, 0.95)  # NEW\n",
    "# #         bins = np.arange(time.min(), t_max)  # NEW\n",
    "#     return bins\n",
    "\n",
    "# def prepare_sample(X, y, train_index, test_index):\n",
    "#     X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "#     bins = get_bins(time=y_train[cnt.TIME_NAME], cens=y_train[cnt.CENS_NAME], mode='q')\n",
    "# #     y_train[cnt.TIME_NAME] = np.clip(y_train[cnt.TIME_NAME], bins.min() - 1, bins.max() + 1)\n",
    "\n",
    "#     y_test[cnt.TIME_NAME] = np.clip(y_test[cnt.TIME_NAME], bins.min(), bins.max())\n",
    "#     return X_train, y_train, X_test, y_test, bins\n",
    "\n",
    "# X, y, features, categ, sch_nan = ds.load_gbsg_dataset()\n",
    "X, y, features, categ, sch_nan = ds.load_wuhan_dataset()\n",
    "# y[\"time\"] += 1\n",
    "\n",
    "# y[\"cens\"] = ~y[\"cens\"]\n",
    "features = list(set(features) - {\"max_2019_nCoV_nucleic_acid_detection\", \n",
    "                                 \"mean_2019_nCoV_nucleic_acid_detection\", \n",
    "                                 \"min_2019_nCoV_nucleic_acid_detection\"})\n",
    "X = X[features]\n",
    "\n",
    "X_TR, X_HO = train_test_split(X, stratify=y[cnt.CENS_NAME],\n",
    "                              test_size=0.33, random_state=42)\n",
    "X_tr, y_tr, X_HO, y_HO, bins_HO = prepare_sample(X, y, X_TR.index, X_HO.index)\n",
    "\n",
    "df = X_HO.copy()\n",
    "df[\"time\"] = y_HO[\"time\"]\n",
    "df[\"cens\"] = y_HO[\"cens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b63d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmf = KaplanMeierZeroAfter()\n",
    "kmf.fit(y[X[\"sex\"] == 0][\"time\"], 1 - y[X[\"sex\"] == 0][\"cens\"])\n",
    "plt.step(bins_HO,  -np.log(kmf.survival_function_at_times(bins_HO)))\n",
    "\n",
    "kmf = KaplanMeierZeroAfter()\n",
    "kmf.fit(y[X[\"sex\"] == 1][\"time\"], 1 - y[X[\"sex\"] == 1][\"cens\"])\n",
    "plt.step(bins_HO,  -np.log(kmf.survival_function_at_times(bins_HO)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da80a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "\n",
    "# ff = list(set(df.columns) - set([\"mean_basophil_count___\", \"min_basophil_count___\", \"max_basophil_count___\", \n",
    "#                                  \"mean_basophil___\", \"min_basophil___\", \"max_basophil___\",\n",
    "#                                  \"max_HCV_antibody_quantification\", \"min_HCV_antibody_quantification\", \"mean_HCV_antibody_quantification\",\n",
    "#                                  \"max_HIV_antibody_quantification\", \"min_HIV_antibody_quantification\", \"mean_HIV_antibody_quantification\"]))\n",
    "df_ = df.fillna(0).replace(np.nan, 0)\n",
    "cph = CoxPHFitter(penalizer=0.01).fit(df_, 'time', 'cens')\n",
    "# axes = cph.check_assumptions(df_, p_value_threshold=0.1)\n",
    "cph.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c04292",
   "metadata": {},
   "outputs": [],
   "source": [
    "-207.42\n",
    "\n",
    "Concordance\t1.00\n",
    "Partial AIC\t856.84\n",
    "log-likelihood ratio test\t67.89 on 221 df\n",
    "-log2(p) of ll-ratio test\t-0.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8343125b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf08ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from survivors.tree import CRAID\n",
    "from survivors.tree.node import Node\n",
    "\n",
    "tree = Node(df, features=features, criterion=\"logrank\", depth=10, min_samples_leaf=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ec0810",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c43de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmf = KaplanMeierZeroAfter()\n",
    "kmf.fit(y[\"time\"], y[\"cens\"])\n",
    "\n",
    "dd = np.unique(y[\"time\"])\n",
    "sf = kmf.survival_function_at_times(dd)\n",
    "sf = np.repeat(sf[np.newaxis, :], y.shape[0], axis=0)\n",
    "\n",
    "ibs_full = ibs_WW(y, y, sf, dd, axis=0)\n",
    "\n",
    "y_ = y.copy()\n",
    "y_[\"cens\"] = True\n",
    "ibs_ev = ibs_WW(y, y_, sf, dd, axis=0)\n",
    "y_[\"cens\"] = False\n",
    "ibs_cn = ibs_WW(y, y_, sf, dd, axis=0)\n",
    "\n",
    "ratio = np.sum(y[\"cens\"])/y.shape[0]\n",
    "weights_hist = ibs_ev*ratio + ibs_cn*(1-ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d849a402",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.sort(y[\"time\"]), ibs_ev[np.argsort(y[\"time\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6516867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_w = np.cumsum(np.bincount(y[\"time\"].astype(int), weights=ibs_ev)[::-1])[::-1]\n",
    "o_w = np.bincount(y[\"time\"].astype(int), weights=ibs_ev)\n",
    "\n",
    "n_j = np.cumsum(np.bincount(y[\"time\"].astype(int))[::-1])[::-1]\n",
    "o_j = np.bincount(y[\"time\"].astype(int))\n",
    "\n",
    "# plt.plot(n_w)\n",
    "# plt.plot(n_j)\n",
    "\n",
    "# plt.plot(n_w/n_j)\n",
    "# plt.plot(o_w[o_w>0]/o_j[o_w>0])\n",
    "\n",
    "plt.plot(np.bincount(y[\"time\"].astype(int)))\n",
    "plt.plot(np.bincount(y[\"time\"].astype(int), weights=ibs_ev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a2ca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_for_nodes(X_attr, best_split_, is_categ):\n",
    "    rule_id = best_split_[\"pos_nan\"].index(0)\n",
    "    query = best_split_[\"values\"][rule_id]\n",
    "    if is_categ:\n",
    "        values = np.isin(X_attr, eval(query[query.find(\"[\"):]))\n",
    "    else:\n",
    "        values = eval(\"X_attr\" + query)\n",
    "    return np.where(values, rule_id, 1 - rule_id)\n",
    "\n",
    "def fit_predict_KM(X_):\n",
    "    y_ = get_y(time=X_[-1], cens=X_[-2])\n",
    "#     print(y_[\"time\"])\n",
    "    kmf = KaplanMeierZeroAfter()\n",
    "    kmf.fit(y_[\"time\"], y_[\"cens\"])\n",
    "    sf = kmf.survival_function_at_times(bins_HO)\n",
    "    sf = np.repeat(sf[np.newaxis, :], y_.shape[0], axis=0)\n",
    "    print(f\"IBS:{ibs_WW(y_, y_, sf, bins_HO):.5f}, AUPRC:{auprc(y_, y_, sf, bins_HO):.5f}, {X_.shape[1]}\")\n",
    "\n",
    "def get_ibs(X_, best_split_, f, is_categ):\n",
    "    fit_predict_KM(X_)\n",
    "    inds = ind_for_nodes(X_[f], best_split_, is_categ)\n",
    "    fit_predict_KM(X_[:, np.where(inds == 0)[0]])\n",
    "    fit_predict_KM(X_[:, np.where(inds == 1)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a204520",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = np.arange(1, 100)\n",
    "plt.plot((n1*(100-n1))/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e401b77f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_ = df.loc[:, features + [cnt.CENS_NAME, cnt.TIME_NAME]].to_numpy().T\n",
    "for i_w, w in enumerate([np.ones(X_.shape[1]), ibs_full, weights_hist]):\n",
    "    attr_by = {}\n",
    "    for i_f, f in enumerate(features):\n",
    "        d = tree.info.copy()\n",
    "        d[\"arr\"] = np.vstack((X_[i_f], X_[-2:]))\n",
    "        d[\"type_attr\"] = (\"woe\" if tree.woe else \"categ\") if f in tree.categ else \"cont\"\n",
    "        d[\"weights\"] = w\n",
    "        a = hist_best_attr_split(**d)\n",
    "        #print(f, w[:5], a)\n",
    "        #get_ibs(X_, a, i_f, d[\"type_attr\"] == \"categ\")\n",
    "        attr_by[f] = a\n",
    "\n",
    "    attr = max(attr_by, key=lambda x: attr_by[x][\"stat_val\"])\n",
    "    print(attr, \"BEST FEATURE\")\n",
    "    get_ibs(X_, attr_by[attr], features.index(attr), attr in tree.categ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8580750f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9c326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from survivors.tree.stratified_model import KaplanMeier, FullProbKM, NelsonAalen, KaplanMeierZeroAfter\n",
    "\n",
    "y_sub = y[:100].copy()\n",
    "\n",
    "kmf1 = KaplanMeierZeroAfter()\n",
    "kmf1.fit(y_sub[\"time\"], y_sub[\"cens\"])\n",
    "\n",
    "dd = np.unique(y_sub[\"time\"])\n",
    "sf1 = kmf1.survival_function_at_times(bins_HO)\n",
    "sf1 = np.repeat(sf1[np.newaxis, :], y_sub.shape[0], axis=0)\n",
    "\n",
    "ibs_full1 = ibs_WW(y_sub, y_sub, sf1, bins_HO, axis=0)\n",
    "print(ibs_full1, auprc(y_sub, y_sub, sf1, bins_HO))\n",
    "\n",
    "\n",
    "kmf2 = KaplanMeierZeroAfter()\n",
    "kmf2.fit(y_sub[\"time\"], y_sub[\"cens\"], weights=ibs_full1)\n",
    "\n",
    "dd = np.unique(y_sub[\"time\"])\n",
    "sf2 = kmf2.survival_function_at_times(bins_HO)\n",
    "sf2 = np.repeat(sf2[np.newaxis, :], y_sub.shape[0], axis=0)\n",
    "\n",
    "ibs_full2 = ibs_WW(y_sub, y_sub, sf2, bins_HO, axis=0)\n",
    "print(ibs_full2, auprc(y_sub, y_sub, sf2, bins_HO))\n",
    "\n",
    "\n",
    "kmf3 = KaplanMeierZeroAfter()\n",
    "kmf3.fit(y_sub[\"time\"], y_sub[\"cens\"], weights=ibs_full2)\n",
    "\n",
    "dd = np.unique(y_sub[\"time\"])\n",
    "sf3 = kmf3.survival_function_at_times(bins_HO)\n",
    "sf3 = np.repeat(sf3[np.newaxis, :], y_sub.shape[0], axis=0)\n",
    "\n",
    "ibs_full3 = ibs_WW(y_sub, y_sub, sf3, bins_HO, axis=0)\n",
    "print(ibs_full3, auprc(y_sub, y_sub, sf3, bins_HO))\n",
    "\n",
    "\n",
    "# na = NelsonAalen()  # KaplanMeierZeroAfter()\n",
    "# na.fit(y_sub[\"time\"], y_sub[\"cens\"]) #, weights=np.random.rand(y_sub[\"time\"].shape[0]))\n",
    "\n",
    "# sf_na = np.exp(-na.cumulative_hazard_at_times(bins_HO))\n",
    "# sf_na[bins_HO > na.timeline[-1]] = 0\n",
    "# sf_na[bins_HO < na.timeline[0]] = 1\n",
    "# sf_na = np.repeat(sf_na[np.newaxis, :], y_sub.shape[0], axis=0)\n",
    "\n",
    "# ibs_full = ibs_WW(y_sub, y_sub, sf_na, bins_HO, axis=0)\n",
    "# print(ibs_full.mean(), np.dot(wei, ibs_full), auprc(y_sub, y_sub, sf_na, bins_HO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293777e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(wei, y_sub)\n",
    "plt.step(bins_HO, sf1[0])\n",
    "plt.step(bins_HO, sf2[0])\n",
    "plt.step(bins_HO, sf3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda7ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0092f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf1[0][np.searchsorted(bins_HO, y_sub[\"time\"])-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7895a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fa285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(sf1 < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e6e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(sf1[0] < 0.5)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e36aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.take_along_axis(sf1, (np.searchsorted(bins_HO, y_sub[\"time\"])-1).reshape(-1, 1), axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5127294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_sub[\"time\"], sf1[0][np.searchsorted(bins_HO, y_sub[\"time\"])-1], s=np.where(y_sub[\"cens\"], 20, 3))\n",
    "plt.scatter(y_sub[\"time\"], sf2[0][np.searchsorted(bins_HO, y_sub[\"time\"])-1], s=np.where(y_sub[\"cens\"], 20, 3))\n",
    "plt.scatter(y_sub[\"time\"], sf3[0][np.searchsorted(bins_HO, y_sub[\"time\"])-1], s=np.where(y_sub[\"cens\"], 20, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1522dbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_sub[\"time\"], y_sub[\"time\"] - np.where(sf1[0] < 0.5)[0][0], s=np.where(y_sub[\"cens\"], 20, 3))\n",
    "plt.scatter(y_sub[\"time\"], y_sub[\"time\"] - np.where(sf2[0] < 0.5)[0][0], s=np.where(y_sub[\"cens\"], 20, 3))\n",
    "plt.scatter(y_sub[\"time\"], y_sub[\"time\"] - np.where(sf3[0] < 0.5)[0][0], s=np.where(y_sub[\"cens\"], 20, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38ac0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(wei, ibs_full1, s=np.where(y_sub[\"cens\"], 20, 3))\n",
    "# plt.scatter(wei, ibs_full2, s=np.where(y_sub[\"cens\"], 20, 3))\n",
    "\n",
    "# print(ibs_full1.mean(), ibs_full2.mean(), ibs_full3.mean())\n",
    "plt.scatter(y_sub[\"time\"], ibs_full1, s=np.where(y_sub[\"cens\"], 20, 3))\n",
    "plt.scatter(y_sub[\"time\"], ibs_full2, s=np.where(y_sub[\"cens\"], 20, 3))\n",
    "plt.scatter(y_sub[\"time\"], ibs_full3, s=np.where(y_sub[\"cens\"], 20, 3))\n",
    "\n",
    "# plt.scatter(ibs_full1, ibs_full1, s=np.where(y_sub[\"cens\"], 20, 3))\n",
    "# plt.scatter(ibs_full1, ibs_full2, s=np.where(y_sub[\"cens\"], 20, 3))\n",
    "# plt.scatter(ibs_full2, ibs_full3, s=np.where(y_sub[\"cens\"], 20, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6785994",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_HO[np.where(sf1[0] <= 0.5)[0]][0], bins_HO[np.where(sf2[0] <= 0.5)[0]][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c1f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(wei[y_sub[\"cens\"]], y_sub[y_sub[\"cens\"]][\"time\"])/np.sum(wei[y_sub[\"cens\"]]),\n",
    "np.dot(np.ones_like(wei[y_sub[\"cens\"]]), y_sub[y_sub[\"cens\"]][\"time\"])/np.sum(np.ones_like(wei[y_sub[\"cens\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05fafe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.rand(y_sub[\"time\"].shape[0]) # np.where(y_sub[\"cens\"], 1, 0.5) # np.ones(y_sub[\"time\"].shape[0])\n",
    "dur_ = np.searchsorted(bins_HO, y_sub[\"time\"])\n",
    "hist_dur = np.bincount(dur_, weights=weights)\n",
    "hist_cens = np.bincount(dur_, weights=y_sub[\"cens\"]*weights)\n",
    "cumul_hist_dur = np.cumsum(hist_dur[::-1])[::-1]\n",
    "\n",
    "hist_cens, cumul_hist_dur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f26f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1991e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.step(bins_HO, sf_na[0])\n",
    "plt.step(bins_HO, sf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7687371",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaplanMeierBal:\n",
    "    def __init__(self):\n",
    "        self.timeline = None\n",
    "        self.survival_function = None\n",
    "        self.confidence_interval_ = None\n",
    "        self.alpha = 0.05\n",
    "\n",
    "    def fit(self, durations, right_censor, weights=None):\n",
    "        if weights is None:\n",
    "            weights = np.ones(right_censor.shape)\n",
    "        self.timeline = np.unique(durations)\n",
    "#         dis_coef = 1\n",
    "        dur_ = np.searchsorted(self.timeline, durations)\n",
    "        hist_dur = np.bincount(dur_, weights=weights)\n",
    "        self.hist_cens = np.bincount(dur_, weights=right_censor*weights)\n",
    "        \n",
    "#         if np.sum(right_censor) > 0:\n",
    "#             dis_coef = (right_censor.shape[0] - np.sum(right_censor)) / np.sum(right_censor)\n",
    "        \n",
    "#         if int(dis_coef) > 1:\n",
    "#             dis_coef = int(dis_coef)\n",
    "#             hist_dur = hist_dur + (dis_coef - 1) * self.hist_cens\n",
    "#             self.hist_cens = self.hist_cens * dis_coef\n",
    "#         elif dis_coef != 0:\n",
    "#             if int(1/dis_coef) > 1:\n",
    "#                 cens_dc = int(1/dis_coef)\n",
    "#                 hist_dur = (cens_dc - 1) * hist_dur + self.hist_cens\n",
    "#                 self.hist_cens = self.hist_cens * cens_dc\n",
    "                \n",
    "        self.cumul_hist_dur = np.cumsum(hist_dur[::-1])[::-1]\n",
    "        self.survival_function = np.hstack([1.0, np.cumprod((1.0 - self.hist_cens / (self.cumul_hist_dur)))])\n",
    "        self.survival_function = (self.survival_function - np.min(self.survival_function))/(np.max(self.survival_function) - np.min(self.survival_function))\n",
    "\n",
    "    def count_confidence_interval(self):\n",
    "        ''' exponential Greenwood: https://www.math.wustl.edu/~sawyer/handouts/greenwood.pdf '''\n",
    "        z = ss.norm.ppf(1 - self.alpha / 2)\n",
    "        cumulative_sq_ = np.sqrt(np.hstack([0.0, np.cumsum(self.hist_cens / (self.cumul_hist_dur * (self.cumul_hist_dur - self.hist_cens)))]))\n",
    "        np.nan_to_num(cumulative_sq_, copy=False, nan=0)\n",
    "        v = np.log(self.survival_function)\n",
    "        np.nan_to_num(v, copy=False, nan=0)\n",
    "        self.confidence_interval_ = np.vstack([np.exp(v * np.exp(- z * cumulative_sq_ / v)),\n",
    "                                               np.exp(v * np.exp(+ z * cumulative_sq_ / v))]).T\n",
    "        np.nan_to_num(self.confidence_interval_, copy=False, nan=1)\n",
    "\n",
    "    def get_confidence_interval_(self):\n",
    "        if self.confidence_interval_ is None:\n",
    "            self.count_confidence_interval()\n",
    "        return self.confidence_interval_\n",
    "\n",
    "    def survival_function_at_times(self, times):\n",
    "        place_bin = np.digitize(times, self.timeline)\n",
    "        sf = self.survival_function[np.clip(place_bin, 0, None)]\n",
    "        sf[times > self.timeline[-1]] = 0\n",
    "        sf[times < self.timeline[0]] = 1\n",
    "        return sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0057a1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5b7c2729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09089013 0.83088622] [0.71666651 0.42457778]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kmf1 = KaplanMeierZeroAfter()\n",
    "kmf1.fit(y_tr[\"time\"], y_tr[\"cens\"])\n",
    "\n",
    "sf1 = kmf1.survival_function_at_times(bins_HO)\n",
    "sf1 = np.repeat(sf1[np.newaxis, :], y_HO.shape[0], axis=0)\n",
    "\n",
    "ibs1 = metr.ibs(y, y_HO, sf1, bins_HO, axis=0)\n",
    "auprc1 = metr.auprc(y, y_HO, sf1, bins_HO, axis=0)\n",
    "print(ibs1[[24, 42]], auprc1[[24, 42]])\n",
    "plt.step(bins_HO, sf1[0])\n",
    "plt.vlines(y_HO[[24]][\"time\"], ymin=0, ymax=1, color=\"red\", label=\"T=18\")\n",
    "plt.vlines(y_HO[[42]][\"time\"], ymin=0, ymax=1, color=\"green\", label=\"T=28\")\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.ylabel(\"Survival Probability\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# kmf1 = KaplanMeierZeroAfter()\n",
    "# kmf1.fit(y_tr[\"time\"][10:20], y_tr[\"cens\"][10:20])\n",
    "\n",
    "# sf1 = kmf1.survival_function_at_times(bins_HO)\n",
    "# sf1 = np.repeat(sf1[np.newaxis, :], y_HO.shape[0], axis=0)\n",
    "\n",
    "# ibs1 = metr.ibs(y, y_HO, sf1, bins_HO, axis=0)\n",
    "# auprc1 = metr.auprc(y, y_HO, sf1, bins_HO, axis=0)\n",
    "# print(ibs1[[24, 42]], auprc1[[24, 42]])\n",
    "# plt.step(bins_HO, sf1[0])\n",
    "# plt.vlines(y_HO[[24]][\"time\"], ymin=0, ymax=1, color=\"red\", label=\"T=18\")\n",
    "# plt.vlines(y_HO[[42]][\"time\"], ymin=0, ymax=1, color=\"green\", label=\"T=28\")\n",
    "# plt.ylim([-0.05, 1.05])\n",
    "# plt.ylabel(\"Survival Probability\")\n",
    "# plt.xlabel(\"Time\")\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "715cca47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAccAAAEJCAYAAADhHux2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAt40lEQVR4nO3de3idZZ3v//ez0rRJk7a0aXqMHXQzfBF/VqYW8JKDjFCvGZwf/vylFC0jUA/VS2TP2BmpQ2CwuFOtuDt4KtsRLTBY3WyqzkHAKRY2OGwGOw4HB/g6KLYkTU9psW2alCZr7T+etZJ1bNdKso75vK7ruci6n0PuOw/NN/c5iMViiIiIyIhIuTMgIiJSaRQcRURE0ig4ioiIpFFwFBERSaPgKCIikmZSuTNQIlOAc4EeYKjMeRERkcpQB8wHfg4cTz4xUYLjucAT5c6EiIhUpIuAnyUnTJTg2ANw6FAf0WjmvM6WlmZ6e4+WPFPFVGtlUnkqW62VB2qvTCpPpkgkYObMJojHiGQTJTgOAUSjsazBMXGu1tRamVSeylZr5YHaK5PKk1NGd5sG5IiIiKRRcBQREUkzUZpVRUQmpFgsxqFD+9m//1WGhqLlzs642bcvQjSaT3kCJk9uYObMVoIgyPv5Co4iIjXs6NHfEQQB8+a9gaEamsg2aVKEwcFTB8dYLMprrx3g6NHfMW3aafk/fwx5m1BatrUQcGL4c4x6epf1ljFHIiKn1t9/lFmz5hIEEaB2ao75CoII06bN5ODBvQUFR/U55iERGANIOk7Qsq2lzDkTETm5aHSIurqJXQ+qq5tENFpYtXli/8TylAiMqWlAUk1SRKRSFdLXVotGU37VHEVEpOrt3t3NF75w27g9T8FRRESq3p49PXR3d43b89SsmocY9ZDWtBobThcRqS1Teu6n6eV1RAa6iDa00XfGrRyfv2Jcnv13f3c3jz66jaGhKOef/w4GBwdpbZ3LBz/4pwB0dHyG97znct761sXcfvt69u7dSyQS4eMfv55zzz2fb3/7mxw4sJ+urlfZs6eHP/mT93HttR/hK1/5Mrt3d/Pf//sG/uIv1o45n6o55qF3WS8x6uMBcSQwarSqiNSaKT33M+2FG6gbeJWAGHUDrzLthRuY0nP/mJ/91FNP4v4i3/rWvWze/F3279/P9OkzeOSRnwBw7Fgfv/zl87zznRfyla98mfe+9wq+8537+OIXN3L77es5dqwPgJdf/k+++tVN/O3f3s19993DkSNH+LM/+0vM3jwugRFUc8ybAqGITARNL68jiPanpAXRfppeXjfm2uOOHU/zwgu/5CMf+RAAx48PMHfuPF5//ThdXa/y/PPPcsEFF1FfX8+OHU+zc+dO7rrrmwAMDg4ON5suWbKU+vp6Zs6cxfTp0+nrG/8F1RUcx1nLtukZza+9yw6XKzsiIgWJDGTvt8uVXohodIgVKz7IBz4QNqEeOXKEuro6mpqa+elP/5lf/vI5/vRPrwNgaCjKV796J9OnzwDgwIEDzJw5k8cff4zJkycPPzMIAmKx8V9QXc2q4ygRGNOPlm3Ty5ovEZF8RRvaCkovxJIl5/KTnzzIsWPHGBwc5K/+6i947LGf8p73/BHbt2+jq+tVFi8+B4C3v30pP/jB/wLglVd+wzXXXMXx4wM5n11XN4mhcVwCSDXHcZQIhulpIiLVou+MW5n2wg0pTauxSCN9Z9w65mdfeOHFvPzyr1i9+jqi0SHOP/+d/PEf/wlBEDBjxmm85S1vHZ6T+OlP38iXvtTJtdd+gFgsxi233MbUqU05n3366adz9OgRPv/5W7jlls+POa9BMaqjFeh04JXe3qNZ9/9qbZ3G/v1HxvxNZqc1qSbEgAMlblodrzJVCpWnstVaeaB2yrRnz07mzfu9vNciheKOVh0vhZQHRn4OySKRgJaWZoA3Ar9Nef7YsygiIrXk+PwVFRcMS60swdHMVgI3A/XAHe7+jbTz7wfWAXXAz4HV7v66mV0LfBHYG7/0x+7eUbqcn1yiTpo5H1JERKpJyQfkmNlCoBO4EDgHWG1mZyedbwK+Dixz97cADcB18dNLgTXufk78qJjACOGo1NS5kBqtKiJSjcpRc7wM2O7uBwHM7AFgOXAbgLv3mdnp7n7CzKYCc4BD8XvPBX7fzG4CngVucPdDGd+hjBQIRUSqXzmC4wKgJ+lzD3Be8gXxwPjHwH1AN/DPSdd+GXgSWE9Yw7w6328c73jNqrV1Wr6PqRq1ViaVp7LVWnmgNsq0b1+ESZPCRsLEf2tFIeWJRCIFvc9yBMcIqd1wAVl24HT3h4AWM1sP3AmsdPf3J86b2ZeAXxfyjYs9WrWS1FqZVJ7KVmvlgdopUzQaZXAwWvDozkpXaHmi0WjG+0warZqhHH9GdAHzkz7PA3YnPpjZLDN7T9L57wKLzWyGmX06KT0ABouaUxERmZDKERwfAS41s9Z4n2I78HDS+QC4z8wWxT9fCfwMOArcaGbnx9M/BfywRHkWEZEJpOTB0d27gQ7gUeAZYIu7P21mD5rZUnfvBVYD/2RmzwIGrHX3IWAFcKeZvQi8Hbix1PkXEal1W7dOYsmSJubObWbJkia2bp14U+LLUmJ33wJsSUu7POnrHwE/ynLfE8CSImdPRGTC2rp1EmvWNNDfH87Y7uoKWLOmARigvX1sPVmxWIw77/wajz/+GJMm1XHFFf8/73znhXz5y1/g8OHfMWVKA5/+9Gc488yz6Oz8HE1Nzbi/yIED+7nuuo/y3vdewY4dT7Np01eJRAKam6fxuc+t57TTTht7wdNMvD8HREQkp87OKcOBMaG/P6Czc8qYg+Ojj/6U559/lnvv/T6Dg4N88pMf5YEHvs9/+28bOPPMs3jlld9w001/yfe+9wMA9u3by6ZNd/Gb3/yaG274OO997xXcc8+3+cxn/oq3vvWt3HPPZn71q5c477x3jClf2Sg4iojIsO7u7Nsl5EovxDPP/BvvfvcyJk+ezOTJk9m06S4uv/zdrF9/2/A1/f39/O53rwFw3nnnEwQBb3rTf+Hw4d8B4eLlN930Gd71rku44IKLOffc8Q+MoOBYVtr7UUQqzcKFMbq6MgPhwoVjXwhz0qRJBEmP7u7uIhaLcffdI71s+/btHd7DcfLkKQDDO3UAXHXV1VxwwcU89dTP2LTpq1xyyX9w7bUfGXPe0tXWjNAqor0fRaQSdXQcp7ExNRA2Nsbo6Dg+5me/7W1LeOyx7QwODjIwMMCtt/4VjY1T+clPHgTg5z9/iuuvX33SZ3zsY9dy7FgfH/jA1axYsZJf/eqlMecrG9Ucy0R7P4pIJQr7FQfo7JxCd3fAwoVhYBxrfyPAu971h7z00gt8+MNXE43GuPLKD7JkyVJuv309W7bcy6RJ9dx22/qUmmK6j3/8ejo71zFpUh2NjVNZu/bmMecrG+3nSHlWwij23o+1srpHgspT2WqtPFA7ZRrNfo7VoNj7OapZVUREJI2CY5lk2+dRez+KiFQGBccy0d6PIlIqE6T7LKfRlF8DcspIgVBEii0SqWNoaJD6+rpyZ6VshoYGiUQKK79qjiIiNayxsZkjR14jFqudwTiFiMWiHDlyiMbG3Pv5ZqOao4hIDWtunsGhQ/vZs+dVhoZqJ0BGIhGi0XzKEzB5cgPNzTMKer6Co4hIDQuCgFmz5tTM1JSEYpdHzaoiIiJpFBxFRETSKDiKiIikUZ9jFWjZ1kLAieHPMerpXdZbxhyJiNS2sgRHM1sJ3AzUA3e4+zfSzr8fWAfUAT8HVrv762a2CLgPmAM4cLW7Hy1p5kssERhT12E9Qcu2FgVIEZEiKXmzqpktBDqBC4FzgNVmdnbS+Sbg68Ayd38L0ABcFz+9Cdjk7mcBO4BbSpfz8sgMjIkdPU5ku1xERMZBOfocLwO2u/tBd+8DHgCWJ07G0053971mNpWwlnjIzOqBi+PXA9wNXFnSnIuIyIRQjmbVBUBP0uce4LzkC9z9hJn9MWETajfwz8Bs4LC7Dybd11bIN45vTZJVa+u0Qh5VdgGnznO1lelUVJ7KVmvlgdork8qTv3IExwipm08EQMYyB+7+ENBiZuuBO4HPkLlpRUHLPVTSfo75aqE+o2k1XKS8nt6T5LmSyzQaKk9lq7XyQO2VSeXJlLSfY+a5MT15dLqA+Umf5wG7Ex/MbJaZvSfp/HeBxcA+YIaZJVaPnZ98X63qXdZLjPq03Ts0WlVEpJjKUXN8BPicmbUCfUA7sDrpfADcZ2ZL3X0XYb/iz+JNrU8AVwFbgGuAh0qb9fJQIBQRKa2S1xzdvRvoAB4FngG2uPvTZvZgPCD2EgbLfzKzZwED1sZv/yTh6NYXgIsIp4OIiIiMq7LMc3T3LYS1v+S0y5O+/hHwoyz37QQuKW7uRERkotPycSIiImkUHEVERNIoOIqIiKTRwuM1pmXb9OE5kbMJp370LjtcxhyJiFQf1RxrSCIwJoJj4uuWbdPLlykRkSqkmmMNSQ6MyWkiIlIY1RxFRETSKDiKiIikUXCsIYm1V0+VJiIiJ6fgWEN6lx1OCYaJrzVaVUSkMBqQU2MSgbC1dRoHamh7GhGRUlLNUUREJI2Co4iISBoFRxERkTQKjiIiImk0IGeCatnWQsCJ4c8x6uld1lvGHImIVA7VHCegRGBMLDcXHido2dZS5pyJiFSGstQczWwlcDNQD9zh7t9IO/8+YB3h7+1XgFXufsjMrgW+COyNX/pjd+8oXc5rQyIwpqYBSTVJEZGJrOTB0cwWAp3A24HjwJNm9qi7vxA/Px24EzjX3bvN7Dbgc8CfAUuBNe7+vVLnW0REJo5yNKteBmx394Pu3gc8ACxPOl8PXO/u3fHPzwGL4l+fC1xrZs+b2X1mNrNkuRYRkQmjHMFxAdCT9LkHaEt8cPded/8hgJk1Ap8FfpR07eeBxcCrwNdLkN+aE6M+xxqs9eXIjohIxSlHn2OE1LWwAyCafpGZzQB+CDzr7vcAuPv7k85/Cfh1Id+4paU557nW1mmFPKoq5CzTytdhy2SS+xgD6glWvk5rabI2KrX2jlSeyldrZVJ58leO4NgFXJT0eR6wO/kCM5sP/ATYDnw6njYD+LC7/038sgAYLOQb9/YeJRrN3KOitXUa+2tsHdJTlinbtI0K/hnU2jtSeSpfrZVJ5ckUiQQ5K03laFZ9BLjUzFrNbCrQDjycOGlmdcA/Ave7+5+7eyKaHQVuNLPz458/RVizFBERGVclrznGR6B2AI8Ck4G73P1pM3sQ+GvgDcASYJKZJQbq7HD3j5rZCuDOeF/kr4BrSp1/ERGpfWWZ5+juW4AtaWmXx7/cQY4arbs/QRg4pYSaXlxDY9dmYAioo79tFX1v3ljubImIFI2Wj5OTCgPjXUmLBgzR2HUXgAKkiNQsLR8nJ9XYtTnrajphTVJEpDYpOMopDBWYLiJS/RQc5RTqCkwXEal+Co5yUv1tq7KuptPftqoc2RERKQkNyJGTSgy60WhVEZlIFBzllPrevFHBUEQmFAVHGTezfrqISPS14c/RyGkcvHRX+TIkIjJK6nOUcZEIjAEMH5Hoa8z66aJT3CkiUnkUHGVcJAJjskSAFBGpNgqOIiIiaRQcRURE0ig4yriIRk7LOh8yGjmtDLkRERkbBUcZFwcv3TUcIBOHRquKSL7Wrp3C/PnNzJnTzPz5zaxdOyXrdVu3TmLJkiYiEViypImtW4sz6UJTOWTcKBCKyGisXTuFzZvrIT6sb2iI+GfYsOH48HVbt05izZoG+vvD67q6IqxZ0wAM0N4+OK55Us1RSq5l22nM3jZ9+GjZdlq5syQiZXTvvSOBcUQQTx/R2TllODAm9PcHdHZmr2WOhYKjlFTLttMIiKbMhwyIKkCKTGBDOTb5SU/v7k4PoCdPH4uTNqua2Rnu/vJ4f1MzWwncDNQDd7j7N9LOvw9YR/i78xVglbsfMrNFwH3AHMCBq9396HjnT4onERhT0wCipc+MiFSEurrsAbIubfOfhQtjdHVlBsKFC9OHA47dqWqOlwGY2eNm9r/M7BYze5+ZvXG039DMFgKdwIXAOcBqMzs76fx04E7gve7+NuA54HPx05uATe5+FrADuGW0+RARkdGZ0nM/s554C7O3zWDWE29hSs/9Y3reNdecgCzj3cP0ER0dx2lsTL2usTFGR8dxxttJg6O7/4/4fy8GPgH8DDgduGkM3/MyYLu7H3T3PuABYHnS+Xrgenfvjn9+DlhkZvXAxfHrAe4GrhxDPkREpEBTeu5n2gs3UDfwKgEx6gZeZdoLN4wpQG7YcJxVq05QVxeOda+ri7Fq1YmUwTgA7e2DbNw4QFtblCCAtrYoGzeO/2AcKGC0qrv3Ao/Gj7FYAPQkfe4Bzkv7Pj8EMLNG4LPA14DZwGF3H0y6r22MeZESixGBtKbV2HC6iFS6ppfXEUT7U9KCaD9NL6/j+PwVo37uhg3HM4JhNu3tg7S3D9LaOo39+/tG/f1OJe/gaGbzgUPuPjDG7xkhtf4ckKXDycxmEAbJZ939nnhzbHq9u6COqpaW5pznWlunFfKoqlCRZVo5BFvqSH51ARGClUO0nuLWiizPGKg8la/WyjQu5RnoyppcN9BV8p9XMb9fIfMcv0s4GOY7AGY2G1gLNAHfcfcdeT6nC7go6fM8YHfyBfFA/BNgO/DpePI+YIaZ1bn7EDA//b5T6e09SjSa2XEb/gVypJBHVbyKLtOy1zLTTpHXii7PKKg8la/WyjRe5ZnV0EbdwKsZ6UMNbRws4c9rPMoTiQQ5K02FtGW9DfhR0ufvEPb5vQHYbmZL83zOI8ClZtZqZlOBduDhxEkzqwP+Ebjf3f/c3WMA7n4CeAK4Kn7pNcBDBeRfqlBLfC4kW4L4nMjp5c6SyITWd8atxCKNKWmxSCN9Z9xaphwVRyHBMeLuBwHMrAW4HHi/u/+/wH8lnHpxSvGBNh2EfZfPAFvc/WkzezAeYK8AlgDLzeyZ+HFX/PZPEo5ufYGw9nlzAfmXKtOybfrwXEgYmRepAClSPsfnr+DI2V9jqOENxAgYangDR87+2pj6GytRIc2qvzazc9z9GeDdQJe7/3v83BbgC/k+yN23xO9JTrs8/uUOcgRtd98JXFJAnqWKJQfG5DQRKa/j81fUXDBMV0hw/BvgfjP7JvBh4Adp55vGLVciIiJllHezqrt/F1gPvI9wUM0Xk04vJXV6hoiIVJHxnthf7QralcPd7yacfJ/uIkYm54uMi8S44sw5kSIynhIT+xPzFxMT+4Gabz7NpZB5jg3AhwhXsPmf8cn6xKdWbChS/mQC6112eHjwTcBIYOxddjjj2pmPnUXdiZGZPUP1Czh0yUulyahIlSvWxP5qVkjN8e+AC4BXgQ4zuxq4F5hjZn8PfDi+HJzIuEkEwtbWaRzIMacpERiTa5h1J3Yz87GzFCBF8hDJMbE/V/pEUMhUjsuAP3D384E1hHMe7wQuBZqBvx733InkIT0wQljTTK5Jikhu0YbsK3HmSp8ICgmOde6+N/71A0Aj8EV3/xfgo4ST+UVEpMpMlIn9hRjVas/x5dv6klav6QFaxjNjIsUyfccVzI6vvDN723Sm77ii3FkSKauJMrG/EIX0OTab2T7gP+LHZDP7A+D5+E4ZdSe9W6RIhuoXZDStxuLp6abvuILJhx5LuXbyoceYvuMKDi/9h2JnVaTkpvTcT9PL62Cgi1kNbfSdcWvWoDcRJvYXopCa4yzCdU3/HpgG/Br4V+CImT0NNIx/9kRO7dAlLzFUv2B4NGsiMGYbjJMeGCHsn5x86LGi51Ok1JL3XmSc9l6cKAqpOU5z95T9HM1sMvBW4A8IFyYXKQuNShXJpCkao1dIcNxpZgcJFwt/Bvj3xOHu/zb+WRMpr6YX19DYtRkYAurob1tF35s3ljtbInmLDHTx3X/5IB33r2fXgUUsmr2LzhU3sfKC75c7axWvkOA4CzgnfvwBcCPwZmDQzH5JGCQ/Pt4ZFBlPr8+8JKNpNRZPTxYGxruSrhuisSvcHEYBUqrFfU9fzyfu+iLHXg+Xvt554HRW3/UtovUt/NGyMmeuwuUdHN39NeCx+AEMN6v+P4TB8pxxzZlIERxe+g/Dg3ISXp95ScZgnMauzVn7Jhu7Nis4Stnl26rRcf/64cCYcOz1JjruX88fdZQmr9WqoLVV07n768Av4odIVchvVOpQQelqgpVSKaRVo2tP9l3uw/SjRctjLRjVPEeR2pdrZlJm+sgvq6H4HpThL6umF9cUNYcyMZ2sVSPdwoXZl+nPlS4jFBxFsuhvW5Wx+0csnp6ukF9WImOXf6tGR8dxGhtT/09ubIzR0XG8CPmqLQqOIln0vXkj/W0fJUZdfO5kHf1tH83RVFpYE6xILvntqZh/q0Z7+yAbNw7Q1hYlCKCtLcrGjQO0tw+Oa75r0Zj6HEfLzFYCNxNuf3WHu38jx3X3Atvj+0hiZtcSbrKcWOP1x+6ubmUpir43b8yz37CO7IFwbItGqR9zYsl3T8X+tlVpfY65WzUgDJDt7YO0tk5j/35tnJSvkgdHM1sIdAJvB44DT5rZo+7+QtI1C4BvEu74sT3p9qXAGnf/XgmzLHJShf6yyoemktSOxPJtkYEuoidZvi3fCfuJ968/nIqrHDXHywhrgwcBzOwBYDlwW9I1VxMuU9ebdu+5wO+b2U3As8AN7n6o+FkWya0Yv6w0laQ25FsbhML2VMy/VUNGqxzBcQHQk/S5Bzgv+QJ3vx3AzC5Mu7cH+DLwJLAe+DphIM1LS0v2Yc0QbqZba2qtTBVdntZvAd8a/jg1fpz0lpOWJ3t/ZcBQxf4cKjVfY3HSMr3yXXi2A47tgqmL4G2d8Ma0X0f/8nnIUhuc/pvPw+KPpF47dREc25nxbYKpi8btZ1tr76iY5SlHcIxAykDAAIjmc6O7vz/xtZl9iXDx87z19h4lGs0cwhy2xWffZb5a1VqZJlp5ZlNHkCVAxqjjwCh/DvksfjBatfZ+4ORlSq8RcmwnsX/9GEeODKTUCGcf25XRAgAQO7Yr4z1OedMtqc8k3FPxyJtu4fg4/Gxr7R2NR3kikSBnpakco1W7gPlJn+cBp9yy3cxmmNmnk5ICQEOupCYVMpUkH8lbdSWOxFZdUriT9Q8miza0Zb0/W7r2VKws5QiOjwCXmlmrmU0F2oGH87jvKHCjmZ0f//wp4IdFyqNIWRU2leTUCt2qK78pBeVXrnzm2z/Yd8atxCKNKWmxSCN9Z9ya9f4/v+NDTFmxk8jVQ0xZsZM/v+ND45NhKVjJg6O7dwMdhFtfPQNscfenzexBM1t6kvuGgBXAnWb2IuFo1xtLkGWRsuh780YOLDvEgWWHObDsUMkGYCTvARiM4x6A4x3IipXPfORbIyykNrh27RQ2b65naCi+ztJQwObN9axdO6UYRZBTCGKxCbGM0OnAK+pzrF4qz9jM3jY9e98XcGDZ4ZS0WU+8Jb45bqqhhjdw8KL/yPr8U5Uno4+OeH/aGJoNR5PPQhTU58jYyzN/fnM8MKaqq4vR0zP2dVD1byhTUp/jG4Hfppwb05NFpCq8PvOSrH2Y6Vt1QWFTCvKVbx9dIYqRz3wVo39wKMeCSrnSpbjKskKOiJRWvlt1Qdg0mK1GlqspMR/FCGTFyGchjs9fMa6DZerqsgfCurEttCSjpJqjyARxeOk/xPsvwyPXNI5CB5Hko5BRm/kqRj7L6ZprTkCW+n2YLqWm4CgiKYrRZFiMQFYtUx+2bp3EkiVNzJ3bzJIlTWzdmr3BbsOG46xadYK6uvj45LoYq1adYMMG7aBRDmpWFZEM491kmHhWPmuMFvrcSguGybZuncSaNQ3094cDbbq6AtasaQCy74yxYcNxBcMKoeAoIiVR6YGsGDo7pwwHxoT+/oDOzinaNqrCqVlVRKRIuruzTaDJnS6VQ8FRRKRIFi7MPo88V7pUDgVHEZEi6eg4TmNjaiBsbIzR0aF+xUqn4CgiUiTt7YNs3DhAW1uUIIjR1hZl48bsg3GksmhAjohIEbW3DyoYViHVHEVERNIoOIqIiKRRcBQREUmj4CgiIpJGwVFERCSNgqOIiEiaskzlMLOVwM1APXCHu38jx3X3Atvd/e7450XAfcAcwIGr3X3sW2SLiIgkKXnN0cwWAp3AhcA5wGozOzvtmgVm9o/A8rTbNwGb3P0sYAdwS/FzLCLVLt9to0QSytGsehlhbfCgu/cBD5AZBK8G/h64P5FgZvXAxfHrAe4Grix6bkWkqiW2jerqihCLBXR1RVizpkEBUk6qHMFxAdCT9LkHSNkO3N1vd/e70u6bDRx298Fc94mIpDvZtlEiuZTjT6cIkLwSbwBER3Efed43rKWlOee51tZphTyqKtRamVSeylap5enuzpUeOWWeK7VMo6Xy5K8cwbELuCjp8zxgdx737QNmmFmduw8B8/O8b1hv71Gi0cytYlpbp7F//5FCHlXxaq1MKk9lK1d5tm6dRGfnFLq7AxYuDHe7SF/HdOHCJrq6MhvJFi6Msn9/X85n6x1VtvEoTyQS5Kw0laNZ9RHgUjNrNbOpQDvw8KlucvcTwBPAVfGka4CHipZLEalo+fYlatsoGY2SB0d37wY6gEeBZ4At7v60mT1oZktPcfsnCUe3vkBY+7y5qJkVkYqVb1+ito2S0QhisQmxI/XpwCtqVq1eKk9lK0d55s5tJhYLMtKDIMbevWOf/qx3VNnGuVn1jcBvU86N6ckiImWycGH2P+xzpYsUQsFRRCrK4sVNzJnTPHwsXtyU9Tr1JUoxKTiKSMVYvLiJPXsCwhle4bFnT5A1QKovUYpJS0SISMUYCYzJAvbsyX59e/uggqEUhWqOIiIiaRQcRaQktPi3VBMFRxEpunwn7M+bFyNzlchYPF2kdBQcRaTo8p2w/9xzfUkBMjzmzYvx3HO5l3kTKQa1a4hI0XV3Z07Wz5WuQCiVQDVHERm1M84I5yQGAcyZ08wZZ2Sfk6gJ+1JtFBxFZFTOOKOJw4eTp14EHD4cZA2QmrAv1UbBUURGJTUwJgTx9FSasC/VRn2OIlISmrAv1UTBUUQyLFrUxMDASA2woSHGrl0aKCMTh5pVRSTFSGAcOQYGAhYtSu1LnD49+5zEMF2kuik4ikiKkcCYLEipSQK8/HJfWoAMA+PLL6uGKdVPzaoiMmqJQBhuPDv2DYZFKkVZgqOZrQRuBuqBO9z9G2nnzwHuAqYDjwOfcPdBM7sW+CKwN37pj929o2QZF6lic+Y0kVojjLFvn2p5ItmUvFnVzBYCncCFwDnAajM7O+2y+4BPufuZhP+aPxZPXwqscfdz4ocCo0geRgJj6hGmp2poyN6XGKaLTAzl6HO8DNju7gfdvQ94AFieOGlmvwc0uvtT8aS7gSvjX58LXGtmz5vZfWY2s4T5Fqli2fsRM9Ng166+pAAZHhqtKhNNOZpVFwA9SZ97gPNOcb4t6esvA08C64GvA1cXLaciVWBkpZrQeAyKUSCUia4cwTFCaptNAETzOe/u708kmtmXgF8X8o1bWppznmttnVbIo6pCrZVJ5ck0cyYcPpyadvhwwJlnTuPQoXyeEIzbz7XW3g/UXplUnvyVIzh2ARclfZ4H7E47Pz/9vJnNAD7s7n8TTw+Agpbb6O09SjSa2W8SjrQ7UsijKl6tlUnlye6115rJ1jT62muxtNGjyX2OCWGT6f79Y68l1tr7gdork8qTKRIJclaaytHn+AhwqZm1mtlUoB14OHHS3XcCA2Z2QTzpQ8BDwFHgRjM7P57+KeCHpcu2SGnNnRvueJE45s7NvuNFPsJRqbGMQ6NVRbIreXB0926gA3gUeAbY4u5Pm9mDZrY0ftnVwN+Y2UtAM/BVdx8CVgB3mtmLwNuBG0udf5FSmDu3iVgsdWRpLBaMOUDu23c06VBgFMmlLPMc3X0LsCUt7fKkr58ldZBOIv0JYEnRMyhSZiOBMVlALK1XYPr0WLzPMbW5VEu4iYyNlo8TqWKpS7iFh5ZwExk7LR8nUkILFjQxOJio5TUzaVKM3bvHFsgUCEXGn2qOIiUyEhgTwTFgcDBgwYLMfsQgyL5KTZguIsWm4ChSIqmBMSFIqkmO2Lu3LylAhkcQxNi7V7VEkVJQs6rIOCjGot4KhCLlo5qjyBgVsqi3iFQHBUeRMctvUe9Jk7L3I4bpIlJJ1KwqksPixU3s2TMS4ObNi/Hcc6Nv6ty9uy8+KAfCwBkbl9GqIjL+VHMUyWIkMI4ce/YELF48tqbS3bvDVWpiMdi376gCo0iFUs1RJpzUuYZkrb2NBMZkAXv2ZHtibPh8apqaS0WqlWqOMqGkzjUMj1xzDfOlRb1Fao9qjjKh5J5rOLbnKhCK1BYFR6kJixY1MTAwEvQaGmJj2s1+3rxYvAk1tal03jw1lYpMBGpWlYq2YEG4p2EQwJw5zVmbP0cC48gxMBCwaNHom0qfe64vHghHjrGOVhWR6qGao1Ss7GuRhunJA2hGAmOygIGBzGdOmhRLmkqRkH2uoQKhyMSl4Cgll89oUShO/2DqXMOTf38RmbjUrCrjZu3aKcyf38ycOc3Mn9/M2rVTMq4pxmjRQiXmGiYOBUYRSafgKKe0desklixpYu7cZpYsaWLr1swGh7Vrp7B5cz1DQ2HAGxoK2Ly5PiNAFrIzRb4aGrIvyxami4gUrizB0cxWmtkLZvafZnZ9lvPnmNkOM/uVmd1lZpPi6YvM7HEze8nM/t7MmkuV53xqRQDLlzcyZ07z8LF8eWPOZ+Z77UUXTU257qKLpp7ymYkBLLmemW95tm6dxJo1DXR1RYjFArq6IqxZ05ARIO+9t55sQS9MH5181yLdtasvKUCGx1hHq4rIxFby4GhmC4FO4ELgHGC1mZ2ddtl9wKfc/UzC37gfi6dvAja5+1nADuCWUuQ531rR8uWNPP54HclNho8/Xpc1QOV77UUXTcU9knKdeyRrgEx9JjmfmW95ADo7p9Dfnxr0+vsDOjtTrx0ayrj1pOn52L27Ly1A5l6LdNeu1KZSBUYRGYty1BwvA7a7+0F37wMeAJYnTprZ7wGN7v5UPOlu4Eozqwcujl8/nF6KDOdbK0oNTCPXhemM6tqRwJh6XZg+umcWUsvr7s7e3JmeXpdZxKzphe5MobVIRaQcyjFadQHQk/S5BzjvFOfbgNnAYXcfTEvPW0tL7lbY1tZpOc/lrhUFJ71vRL7XFXLt6J9ZSHkWLYKdOzOvXbQo9drVq+HOOzOvW7069boTJ2Dy5PC/CfX1Aa+/HgAnL0/+5a0OKk/lq7UyqTz5K0dwjJBadQiAaB7n09NJu++UenuPEo1m1lBaW6exf/+RnPfV1TXHmyDT02Ps3380KaWZzBoZQPp1hVw7/s/Mvzzw2c+GfY7JTauNjTE++9kB9u8fmQ+xbh0MDEzh3nvrGRoKa4zXXHOCdeuOs39/6vfp7s7MYfo16U71jqqNylP5aq1MKk+mSCTIWWkqR7NqFzA/6fM8YHce5/cBM8ws0VA3P+2+ornmmhNkawoM00dcfPFQ1uvCdEZ1rVk063Vh+uiemW95ANrbB9m4cYC2tihBEKOtLcrGjQO0t2dONtyw4Tg9PWGfX0/PUTZsOJ5xjYhINShHcHwEuNTMWs1sKtAOPJw46e47gQEzuyCe9CHgIXc/ATwBXBVPvwZ4qBQZ3rDhOKtWnaCuLuwvq6uLsWrViYxf/g880J8UoMLj4ouHeOCB/oxn5nvtE08cSwqQ4WEW5Yknjp3imeR8Zr7lSWhvH+QXv+hj796j/OIXfVkDo4hILQlisdLPBTOzlcBNwGTgLnf/kpk9CPy1u+8ws7cB3wKmA78AVrn78fhgnXuAOcAu4IPufiiPb3k68Mpom1WrUa2VSeWpbLVWHqi9Mqk8mZKaVd8I/Db5XFmWj3P3LcCWtLTLk75+ltRBOon0ncAlxc6fiIhMbFohR0REJI2Co4iISBoFRxERkTQTZcuqOgg7X3M52blqVWtlUnkqW62VB2qvTCpPzvsz1vgqy2jVMriQcBqIiIhIuouAnyUnTJTgOAU4l3DJuTEshS0iIjWkjnBBmZ8DKRO9J0pwFBERyZsG5IiIiKRRcBQREUmj4CgiIpJGwVFERCSNgqOIiEgaBUcREZE0Co4iIiJpJsrycTnF95a8GagH7nD3b5Q5S2NiZo8S7nd5Ip70cXf/1zJmaVTMbDrwJPAn7v5bM7sM2Ag0Av/T3W8uawYLlKU8mwlXbuqLX7LO3X9YtgwWwMxuBVbEP/7Y3W+sgfeTrUzV/I5uA5YT7nz+bXffWM3vKEd5ivp+JvQiAGa2kHDJoLcTro7wJOEGyi+UNWOjZGYB0AX8nrsPljs/o2Vm5xNudn0WcCawF3DgXcCrwI8J/5B5qGyZLEB6eeLB8XngPe7eU97cFSb+C3Yd8IeEv6geBu4CNlC97ydbmb4O3EZ1vqN3AZ2Ee9/WAy8A/x/wj1ThO8pRnj8CHqCI72eiN6teBmx394Pu3kf4w15e5jyNhcX/+89m9qyZfaqsuRm9jwHXA7vjn88D/tPdX4kH/fuAK8uVuVFIKY+ZTQUWAd8xs+fMbJ2ZVcu/xR7gL9z9dXc/AbxI+AdMNb+fbGVaRJW+I3f/38Afxt/FHMIWwtOo0neUozz9FPn9VMXLLqIFhP8wEnqAtjLlZTzMBH4KvB+4FPiEmS0rb5YK5+4fdffkheKr+j1lKc88YDvwYeAdhIsef6QceSuUu/+Huz8FYGa/T9gUGaW630+2Mj1Mlb4jAHc/YWbrCGtZP6X6/w2ll6eeIr+fid7nGCFsRkkICP+hVyV3/z/A/0l8NrNvA5cD28qWqfFRa+/pN4R/wABgZl8DriFseq0KZvYWwqa5zwCDhLXHhKp8P8llcnenyt+Ru99qZhsIm1PPpMr/DaWV51J3L+r7meg1xy7CFdkT5jHSlFd1zOxCM7s0KSlgZGBONau19/RWM2tPSqqq92RmFxD+9f5Zd7+HGng/6WWq5ndkZmeZ2TkA7n4M+AFhf11VvqMc5bmq2O9notccHwE+Z2athCOe2oHV5c3SmJwG3GZm7yRsdrgW+ERZczQ+/hUwMzsDeAVYCXynvFkakwC4w8y2A0cJ/5+7p7xZyo+ZvQH4EXCVu2+PJ1f1+8lRpqp9R8CbgHVmdiFhbfF9wDeB26v0HWUrz/+myO9nQtcc3b0b6AAeBZ4Btrj702XN1Bi4+z8RNgv9O/BvwHfiTa1Vzd0HgOuArYR9Di8RDp6qSu7+HPAF4F8Iy/OMu3+vvLnK218CDcBGM3vGzJ4hfDfXUb3vJ1uZ3kmVviN3f5DU3wNPuvv3qdJ3lKM8t1Hk9zOhp3KIiIhkM6FrjiIiItkoOIqIiKRRcBQREUmj4CgiIpJGwVFERCTNRJ/nKFL1zOyrwMXxj2cTzmPrJ1zofJ27f6FceROpVprKIVJDzOy3wHJ331HuvIhUM9UcRWqUmX0OmO3un4oHzS3AuwkXqP8ScAHhdm0ngCvcfXd8G7evE+54UA98393XlyH7ImWlPkeRiaPB3d8B/DXwt8BX3P1thPv7XRe/5u8IV1Z6O+FWYZeZ2YpsDxOpZao5ikwcW+P//TWwx92fTfo8y8yaCDfDnWVmn4+fawbOAe4vZUZFyk3BUWTiOJ70dbYdDOoIF9x+Z3z3A8xsNjBQgryJVBQ1q4oIAO5+GHgKWANgZqcRLuz8vjJmS6QsFBxFJNlK4B1m9jzhVlTfc/fvljlPIiWnqRwiIiJpVHMUERFJo+AoIiKSRsFRREQkjYKjiIhIGgVHERGRNAqOIiIiaRQcRURE0ig4ioiIpPm/IXHXfhDxQ5cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sf1 = kmf1.survival_function_at_times(bins_HO)\n",
    "sf1 = np.repeat(sf1[np.newaxis, :], y_tr.shape[0], axis=0)\n",
    "\n",
    "ibs1 = metr.ibs_WW(y, y_tr, sf1, bins_HO, axis=0)\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.scatter(y_tr[\"time\"][y_tr[\"cens\"] == 1], ibs1[y_tr[\"cens\"] == 1], c=\"orange\", label=\"event\")\n",
    "plt.scatter(y_tr[\"time\"][y_tr[\"cens\"] == 0], ibs1[y_tr[\"cens\"] == 0], c=\"blue\", label=\"cens\")\n",
    "# plt.ylim([-0.05, 1.05])\n",
    "# plt.ylabel(r\"$AUPRC$\")\n",
    "plt.ylabel(r\"$IBS^i$\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "93144d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 18.)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_HO[38]  # 0, 8, 32, 38, \n",
    "y_HO[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "89f5ee65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kmf1 = KaplanMeierZeroAfter()\n",
    "kmf1.fit(np.array([1, 5, 10, 100, 105, 115]), np.array([1, 1, 1, 1, 1, 1]))\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "sf1 = kmf1.survival_function_at_times(np.arange(1, 120))\n",
    "plt.step(np.arange(1, 120), sf1)\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.ylabel(\"Survival Probability\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7799510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3e2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y[\"time\"], ibs_full1, s=np.where(y[\"cens\"], 20, 3))\n",
    "plt.scatter(y[\"time\"], ibs_full2, s=np.where(y[\"cens\"], 20, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e92d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_haz.shape, pred_surv.shape, pred_time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3838f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from survivors.experiments.grid import generate_sample, prepare_sample, count_metric\n",
    "\n",
    "tree_m = CRAID(features=features, criterion=\"logrank\", depth=10, min_samples_leaf=0.05)\n",
    "tree_m.fit(X_tr, y_tr)\n",
    "\n",
    "pred_time = tree_m.predict(X_HO, target=\"time\")\n",
    "pred_surv = tree_m.predict_at_times(X_HO, bins=bins_HO, mode=\"surv\")\n",
    "pred_haz = tree_m.predict_at_times(X_HO, bins=bins_HO, mode=\"hazard\")\n",
    "print(count_metric(y_tr, y_HO, pred_time,\n",
    "                   pred_surv, pred_haz, bins_HO, \n",
    "                   ['CI', \"IBS_WW\", \"IBS_REMAIN\", \"AUPRC\", \"EVENT_AUPRC\", \"BAL_AUPRC\"]))\n",
    "tree_m.visualize(mode=\"surv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff05190",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_m = CRAID1(features=features, max_features=10, criterion=\"ibswei\", depth=10, min_samples_leaf=0.05, n_jobs=1)\n",
    "tree_m.fit(X_tr, y_tr)\n",
    "\n",
    "pred_time1 = tree_m.predict(X_HO, target=\"time\")\n",
    "pred_surv1 = tree_m.predict_at_times(X_HO, bins=bins_HO, mode=\"surv\")\n",
    "pred_haz1 = tree_m.predict_at_times(X_HO, bins=bins_HO, mode=\"hazard\")\n",
    "print(count_metric(y_tr, y_HO, pred_time1,\n",
    "                   pred_surv1, pred_haz1, bins_HO, \n",
    "                   ['CI', \"IBS_WW\", \"IBS_REMAIN\", \"AUPRC\", \"EVENT_AUPRC\", \"BAL_AUPRC\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108e8e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b97c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = bins_HO\n",
    "time = y_HO[[3]][\"time\"]\n",
    "event = y_HO[[3]][\"cens\"]\n",
    "estimate = pred_surv[[3]]\n",
    "\n",
    "steps = np.linspace(1e-5, 1 - 1e-5, 100)\n",
    "before_time = np.dot(time[:, np.newaxis], steps[np.newaxis, :])\n",
    "after_time = np.dot(time[:, np.newaxis], 1 / steps[np.newaxis, :])\n",
    "before_ind = np.clip(np.searchsorted(times, before_time), 0, times.shape[0] - 1)\n",
    "after_ind = np.clip(np.searchsorted(times, after_time), 0, times.shape[0] - 1)\n",
    "\n",
    "est1 = np.take_along_axis(estimate, before_ind, axis=1)\n",
    "est2 = np.take_along_axis(estimate, after_ind, axis=1)\n",
    "\n",
    "np.trapz(est1[0], steps), np.trapz(est2[0], steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983d4a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trapz(estimate[0][before_ind[0]], steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cefeae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trapz(estimate[0][bins_HO < time[0]], bins_HO[bins_HO < time[0]]) / time[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b06ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e44c3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef9d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_bstr = {'aggreg_func': 'mean', 'all_weight': True, 'balance': None, 'categ': categ, \n",
    "        'criterion': 'peto', 'depth': 10, 'ens_metric_name': 'IBS_REMAIN', \n",
    "        'leaf_model': 'base_zero_after', 'max_features': 0.3, 'min_samples_leaf': 0.01, # 0.01 \n",
    "        'n_estimators': 100, 'n_jobs': 5, 'size_sample': 0.7}\n",
    "\n",
    "bstr = IBSCleverBoostingCRAID1(**param_bstr)\n",
    "bstr.fit(X_tr, y_tr)\n",
    "bstr.tolerance_find_best(param_bstr[\"ens_metric_name\"])\n",
    "pred_time = bstr.predict(X_HO, target=\"time\")\n",
    "pred_surv = bstr.predict_at_times(X_HO, bins=bins_HO, mode=\"surv\")\n",
    "pred_haz = bstr.predict_at_times(X_HO, bins=bins_HO, mode=\"hazard\")\n",
    "\n",
    "print(count_metric(y_tr, y_HO, pred_time,\n",
    "                   pred_surv, pred_haz, bins_HO, \n",
    "                   ['CI', \"IBS_REMAIN\", \"BAL_IBS_REMAIN\", \"IAUC_WW_TI\", \"AUPRC\", \"EVENT_AUPRC\", \"BAL_AUPRC\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55afe97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.63013484 0.1418403 0.34070651 0.76727171 0.69328389 0.51621181\n",
    " 0.67322494]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "292fecc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitted: 50 models.\n",
      "[0.138  0.1266 0.1108 0.1032 0.0944 0.0894 0.0832 0.0809 0.0773 0.0785\n",
      " 0.0789 0.0763 0.0768 0.0767 0.0767 0.0775 0.0775 0.0774 0.0777 0.0785\n",
      " 0.0775 0.0767 0.075  0.0747 0.0746 0.0746 0.0737 0.0728 0.0733 0.073\n",
      " 0.073  0.0733 0.0737 0.0726 0.0723 0.0715 0.0709 0.071  0.0705 0.0705\n",
      " 0.0705 0.0707 0.0703 0.0706 0.0702 0.0704 0.0705 0.0706 0.0704 0.0704]\n",
      "fitted: 45 models.\n",
      "[0.76703221 0.07544973 0.14209613 0.85055543 0.74599572 0.52798263\n",
      " 0.73278281]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\metrics.py:374: RuntimeWarning: invalid value encountered in divide\n",
      "  false_pos = cumsum_fp / n_controls\n"
     ]
    }
   ],
   "source": [
    "from survivors.ensemble import BootstrapCRAID\n",
    "param_bstr = {'balance': \"only_log_rank\", 'categ': categ, \n",
    "        'criterion': 'peto', 'depth': 10, 'ens_metric_name': 'IBS_REMAIN', \n",
    "        'leaf_model': 'base_zero_after', 'max_features': 0.3, 'min_samples_leaf': 0.001, # 0.01 \n",
    "        'n_estimators': 50, 'n_jobs': 5, 'size_sample': 0.7}\n",
    "\n",
    "bstr = BootstrapCRAID1(**param_bstr)\n",
    "bstr.fit(X_tr, y_tr)\n",
    "bstr.tolerance_find_best(param_bstr[\"ens_metric_name\"])\n",
    "pred_time = bstr.predict(X_HO, target=\"time\")\n",
    "pred_surv = bstr.predict_at_times(X_HO, bins=bins_HO, mode=\"surv\")\n",
    "pred_haz = bstr.predict_at_times(X_HO, bins=bins_HO, mode=\"hazard\")\n",
    "\n",
    "print(count_metric(y_tr, y_HO, pred_time,\n",
    "                   pred_surv, pred_haz, bins_HO, \n",
    "                   ['CI', \"IBS_REMAIN\", \"BAL_IBS_REMAIN\", \"IAUC_WW_TI\", \"AUPRC\", \"EVENT_AUPRC\", \"BAL_AUPRC\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07275af",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.76100069 0.07011951 0.12759118 0.87563497 0.74828594 0.51627036\n",
    " 0.73422439]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2e913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.77087046 0.0714181  0.13772382 0.87419811 0.7440283  0.51076982\n",
    " 0.72989142]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28a471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.75030843 0.07748653 0.1448151  0.82615882 0.74452061 0.5191674\n",
    " 0.73086284]\n",
    "[0.76703221 0.07544973 0.14209613 0.85055543 0.74599572 0.52798263\n",
    " 0.73278281]  # only_log_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a26f39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.75058259 0.87791219 0.07801019 0.18533862 0.88196739 0.75960723\n",
    " 0.55266705 0.7470654 ]\n",
    "\n",
    "[0.75277587 0.88149642 0.07252386 0.16060168 0.86366427 0.76527479\n",
    " 0.5589904  0.75277271]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2920ebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_HO[np.argsort(a)[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3544c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = metr.auprc(y_tr, y_HO, st, bins_HO, axis=0)\n",
    "plt.plot(np.sort(a))\n",
    "plt.hlines(y=[np.median(a)], xmin=0, xmax=a.shape[0], color=\"green\")\n",
    "plt.hlines(y=[np.mean(a)], xmin=0, xmax=a.shape[0], color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d86b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.63302716 0.13979877 0.33711561 0.77352287 0.69542915 0.51581538\n",
    " 0.67508228]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c919da14",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.628376   0.14211706 0.34293727 0.7677789  0.69501956 0.51565712\n",
    " 0.67470116]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f69851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.62888411 0.14833584 0.34296446 0.76911903 0.70039634 0.46559482\n",
    " 0.67379773]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a746db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = bstr.models[0].predict_at_times(X_HO, bins=bins_HO, mode=\"surv\")\n",
    "num = bstr.models[0].predict(X_HO, target=\"numb\").astype('int')\n",
    "ibss = metr.ibs_WW(y_tr, y_HO, st, bins_HO, axis=0)\n",
    "\n",
    "a = np.bincount(num, weights=ibss)/np.bincount(num)\n",
    "print(a)\n",
    "np.argsort(np.nan_to_num(a, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_numb = 100\n",
    "y_tr[bstr.models[0].predict(X_tr, target=\"numb\").astype('int') == leaf_numb], y_HO[num == leaf_numb], ibss[num == leaf_numb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1f73ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "bstr.models[0].visualize(mode=\"surv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b49c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bstr.models[0].nodes[leaf_numb].leaf_model.lists[\"time\"], bstr.models[0].nodes[leaf_numb].leaf_model.lists[\"cens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef5f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmf = KaplanMeierZeroAfter()\n",
    "kmf.fit(np.array([1703.0, 2175.0, 692.0, 2175.0]), np.array([1, 1, 1, 1]))\n",
    "kmf.survival_function\n",
    "\n",
    "kmf1 = KaplanMeierZeroAfter()\n",
    "kmf1.fit(np.array([1703.0, 2175.0, 692.0, 2175.0]), np.array([0, 0, 0, 0]))\n",
    "kmf.survival_function*0.5 + kmf1.survival_function*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ec8695",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.step(bins_HO, bstr.models[0].nodes[leaf_numb].leaf_model.predict_survival_at_times(X=None, bins=bins_HO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b85a112",
   "metadata": {},
   "outputs": [],
   "source": [
    "bstr.models[0].nodes[110].get_full_rule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f649108",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.63447332 0.14794288 0.34201422 0.76260881 0.70294555 0.46961011\n",
    " 0.67651302]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a323831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.63552863 0.1460382  0.33880928 0.7713849  0.70406205 0.46900253\n",
    " 0.67743421]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0b8192",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.62571819 0.14431613 0.34435273 0.76853784 0.69823154 0.47693504\n",
    " 0.6731628 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1e772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.63537229 0.14244827 0.33418399 0.76911905 0.69135619 0.46550928\n",
    " 0.66577197]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82318418",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.63005667 0.14378532 0.33457048 0.76142872 0.69421038 0.4725349\n",
    " 0.66909871]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5609e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_bstr = {'balance': None, 'categ': categ, \n",
    "    'criterion': 'ibswei', 'depth': 10, 'ens_metric_name': 'IBS_WW', \n",
    "    'leaf_model': 'base_zero_after', 'max_features': 0.3, 'min_samples_leaf': 0.001, # 0.01 \n",
    "    'n_estimators': 50, 'n_jobs': 5, 'size_sample': 0.7}\n",
    "\n",
    "bstr1 = BootstrapCRAID(**param_bstr)\n",
    "bstr1.fit(X_tr, y_tr)\n",
    "bstr1.tolerance_find_best(param_bstr[\"ens_metric_name\"])\n",
    "pred_time1 = bstr1.predict(X_HO, target=\"time\")\n",
    "pred_surv1 = bstr1.predict_at_times(X_HO, bins=bins_HO, mode=\"surv\")\n",
    "pred_haz1 = bstr1.predict_at_times(X_HO, bins=bins_HO, mode=\"hazard\")\n",
    "\n",
    "print(count_metric(y_tr, y_HO, pred_time1,\n",
    "                   pred_surv1, pred_haz1, bins_HO, \n",
    "                   ['CI', \"BAL_IBS_REMAIN\", \"IAUC_WW_TI\", \"AUPRC\", \"EVENT_AUPRC\", \"BAL_AUPRC\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21406b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.61708032 0.10647576 0.14558612 0.66939811 0.49017465 0.64909545]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ed50b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.62431112 0.10494258 0.14357879 0.66784064 0.4888831  0.64756811]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839535fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_HO[\"time\"], metr.ibs_WW(y_tr, y_HO, pred_surv, bins_HO, axis=0))\n",
    "plt.scatter(y_HO[\"time\"], metr.ibs_WW(y_tr, y_HO, pred_surv1, bins_HO, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeacf3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(metr.ibs_WW(y_tr, y_HO, pred_surv, bins_HO, axis=0),\n",
    "            metr.ibs_WW(y_tr, y_HO, pred_surv1, bins_HO, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd78bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from survivors.ensemble import ProbBoostingCRAID\n",
    "param_bstr = {'aggreg_func': 'wei', \"with_arc\": False, \n",
    "              'balance': None, 'categ': categ, \"all_weight\": True, \n",
    "                'criterion': 'peto', 'depth': 10, 'ens_metric_name': 'IBS_WW', \n",
    "                'leaf_model': 'base_zero_after', 'max_features': 0.3, 'min_samples_leaf': 0.001, # 0.01 \n",
    "                'n_estimators': 50, 'n_jobs': 5, 'size_sample': 0.7}\n",
    "prbst = ProbBoostingCRAID(**param_bstr)\n",
    "prbst.fit(X_tr, y_tr)\n",
    "prbst.tolerance_find_best(param_bstr[\"ens_metric_name\"])\n",
    "pr_pred_time = prbst.predict(X_HO, target=\"time\")\n",
    "pr_pred_surv = prbst.predict_at_times(X_HO, bins=bins_HO, mode=\"surv\")\n",
    "pr_pred_haz = prbst.predict_at_times(X_HO, bins=bins_HO, mode=\"hazard\")\n",
    "\n",
    "print(count_metric(y_tr, y_HO, pr_pred_time,\n",
    "                   pr_pred_surv, pr_pred_haz, bins_HO, \n",
    "                   ['CI', \"IBS_WW\", \"IBS_REMAIN\", \"AUPRC\", \"EVENT_AUPRC\", \"BAL_AUPRC\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda9fe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from survivors.ensemble import BoostingCRAID\n",
    "param_bstr = {'aggreg_func': 'wei', \"with_arc\": False, 'weighted_tree': False,\n",
    "              'balance': None, 'categ': categ, \"all_weight\": False, \"mode_wei\": 'square',\n",
    "                'criterion': 'peto', 'depth': 10, 'ens_metric_name': 'IBS_WW', \n",
    "                'leaf_model': 'base_fast', 'max_features': 0.3, 'min_samples_leaf': 0.001, # 0.01 \n",
    "                'n_estimators': 50, 'n_jobs': 5, 'size_sample': 0.7}\n",
    "prbst = BoostingCRAID(**param_bstr)\n",
    "prbst.fit(X_tr, y_tr)\n",
    "prbst.tolerance_find_best(param_bstr[\"ens_metric_name\"])\n",
    "pr_pred_time = prbst.predict(X_HO, target=\"time\")\n",
    "pr_pred_surv = prbst.predict_at_times(X_HO, bins=bins_HO, mode=\"surv\")\n",
    "pr_pred_haz = prbst.predict_at_times(X_HO, bins=bins_HO, mode=\"hazard\")\n",
    "\n",
    "print(count_metric(y_tr, y_HO, pr_pred_time,\n",
    "                   pr_pred_surv, pr_pred_haz, bins_HO, \n",
    "                   ['CI', \"IBS_WW\", \"IBS_REMAIN\", \"BAL_IBS_REMAIN\", \"IAUC_WW_TI\", \"AUPRC\", \"EVENT_AUPRC\", \"BAL_AUPRC\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dab46c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72919808 0.85640681 0.09240043 0.21129718 0.84030537 0.65518371\n",
      " 0.41999913 0.6409301 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\survive\\lib\\site-packages\\survivors-1.4.4-py3.10.egg\\survivors\\metrics.py:374: RuntimeWarning: invalid value encountered in divide\n",
      "  false_pos = cumsum_fp / n_controls\n"
     ]
    }
   ],
   "source": [
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "param_est = {'learning_rate': 0.05, 'loss': 'coxph', \n",
    "              'max_depth': 20, 'max_features': 'sqrt', \n",
    "              'min_samples_leaf': 20, 'n_estimators': 50, 'random_state': 123}\n",
    "\n",
    "est = GradientBoostingSurvivalAnalysis(**param_est)\n",
    "X_tr_sks = X_tr.fillna(0).replace(np.nan, 0)\n",
    "X_HO_sks = X_HO.fillna(0).replace(np.nan, 0)\n",
    "\n",
    "est = est.fit(X_tr_sks, y_tr)\n",
    "survs = est.predict_survival_function(X_HO_sks)\n",
    "hazards = est.predict_cumulative_hazard_function(X_HO_sks)\n",
    "pred_surv_bstr = np.array(list(map(lambda x: x(bins_HO), survs)))\n",
    "pred_haz_bstr = np.array(list(map(lambda x: x(bins_HO), hazards)))\n",
    "pred_time_bstr = -1*est.predict(X_HO_sks)\n",
    "\n",
    "print(count_metric(y_tr, y_HO, pred_time_bstr,\n",
    "                   pred_surv_bstr, pred_haz_bstr, bins_HO, \n",
    "                   ['CI', \"CI_CENS\", \"IBS_REMAIN\", \"BAL_IBS_REMAIN\", \"IAUC_WW_TI\", \"AUPRC\", \"EVENT_AUPRC\", \"BAL_AUPRC\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eeb209",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.61959187 0.15830578 0.3311214  0.75086706 0.64686629 0.41465875\n",
    " 0.61896538]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a9fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(zip(features, est.feature_importances_))\n",
    "d_ = dict(sorted(d.items(), key=lambda item: -item[1]))\n",
    "d_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d978eee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ibs_vals = metr.auprc(y_tr, y_HO, pred_surv_bstr, bins_HO, axis=0)\n",
    "plt.scatter(y_HO[\"time\"], ibs_vals, c=np.where(y_HO[\"cens\"], \"orange\", \"blue\"))\n",
    "plt.title(\"TRAIN\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"IBS_REMAIN\")\n",
    "print(np.mean(ibs_vals), np.std(ibs_vals))\n",
    "plt.show()\n",
    "\n",
    "ibs_vals = metr.auprc(y_tr, y_HO, pred_surv, bins_HO, axis=0)\n",
    "plt.scatter(y_HO[\"time\"], ibs_vals, c=np.where(y_HO[\"cens\"], \"gold\", \"darkblue\"))\n",
    "plt.title(\"TEST\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"IBS_REMAIN\")\n",
    "plt.show()\n",
    "print(np.mean(ibs_vals), np.std(ibs_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed0e082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eefdd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param = {'aggreg_func': 'mean', 'weighted_tree': False,\n",
    "         'balance': None, 'categ': categ, 'all_weight': True,  # \"only_log_rank\"\n",
    "         'criterion': 'peto', 'depth': 7, 'ens_metric_name': 'IBS_WW', \n",
    "         'leaf_model': 'base_zero_after', 'max_features': 0.3, 'min_samples_leaf': 0.05, \n",
    "         'n_estimators': 50, 'n_jobs': 1, 'size_sample': 0.5}\n",
    "\n",
    "# param = {'aggreg_func': 'wei', 'weighted_tree': False,\n",
    "#          'balance': None, 'categ': categ, 'all_weight': True,  # \"only_log_rank\"\n",
    "#          'criterion': 'peto', 'depth': 7, 'ens_metric_name': 'IBS_REMAIN', \n",
    "#          'leaf_model': 'base_fast', 'max_features': 0.3, 'min_samples_leaf': 0.05, \n",
    "#          'n_estimators': 50, 'n_jobs': 1, 'size_sample': 1.0}\n",
    "\n",
    "bst = IBSCleverBoostingCRAID1(**param)\n",
    "bst.fit(X_tr, y_tr)\n",
    "# bst.tolerance_find_best(param[\"ens_metric_name\"])\n",
    "\n",
    "pred_surv = bst.predict_at_times(X_HO, bins=bins_HO, mode=\"surv\")\n",
    "pred_time = bst.predict(X_HO, target=cnt.TIME_NAME)\n",
    "pred_haz = bst.predict_at_times(X_HO, bins=bins_HO, mode=\"hazard\")\n",
    "print(count_metric(y_tr, y_HO, pred_time, \n",
    "                   pred_surv, pred_haz, bins_HO, \n",
    "                   ['CI', \"IBS_REMAIN\", \"BAL_IBS_REMAIN\", \"IAUC_WW_TI\", \"AUPRC\", \"EVENT_AUPRC\", \"BAL_AUPRC\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.76004112 0.08142023 0.15725193 0.82522674 0.74669112 0.5263278\n",
    " 0.73333577]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f3beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.74530257 0.0898127  0.20803243 0.89115491 0.71697038\n",
    " 0.45596368 0.68388502]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32cd724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e020943",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.74804607 0.08801539 0.18134788 0.83107438 0.70331706 0.43851512 0.68726846]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b1c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.74489236 0.08457115 0.14168006 0.85179607 0.72299419 0.46876279 0.70758623]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb4376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.6281024  0.10568593 0.14523742 0.35297625 0.75068269 0.70383533\n",
    " 0.52024883 0.68303842]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a868c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.62337307 0.10386134 0.14200631 0.38739308 0.76488304 0.68675712\n",
    " 0.51934772 0.66779278]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5ff848",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.628376   0.10954972 0.14885949 0.42151545 0.75263177 0.72983042\n",
    " 0.54319429 0.70868805]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4857c657",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.62767246 0.10946605 0.14872402 0.41986858 0.75285723 0.72985129\n",
    " 0.54328022 0.70871629]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4f317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.63040844 0.10627041 0.14539301 0.37721831 0.75708808 0.71503153\n",
    " 0.53049403 0.69412689]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c940a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.62884503 0.12118926 0.16319798 0.44877019 0.74167396 0.74745494\n",
    " 0.53584763 0.7234838 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e9625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.67986326 0.10607445 0.13573885 0.50670191 0.77434917 0.74461208\n",
    " 0.49396821 0.71244981]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05a9cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.62528825 0.10841821 0.14737888 0.38667048 0.74371303 0.73301454\n",
    " 0.53644526 0.71074692]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dce4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.62747704 0.10943691 0.148753 0.407465 0.75551115 0.73796047 0.54631429 0.71625056]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 500 trees with (ibs + 0.01)**2\n",
    "\n",
    "[0.62728161 0.11491767 0.15558508 0.4369346  0.75162291 0.74540918\n",
    " 0.55013961 0.7232888 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a236d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 9\n",
    "print(y_HO[ind])\n",
    "plt.step(bins_HO, pred_surv_bstr[ind])\n",
    "plt.step(bins_HO, pred_surv[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fc48f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_HO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eb63b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.6270471  0.10445372 0.14307231 0.36536187 0.75919258 0.72271412\n",
    " 0.54059226 0.70208313]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ee7e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.61829197 0.10674626 0.14629515 0.37302563 0.75679989 0.72305381\n",
    " 0.54106211 0.70243756]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36e1989",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.77607951 0.05217564 0.07227308 0.13924877 0.86552461 0.70292791\n",
    " 0.42100231 0.68584151]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb507f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.76566141 0.05515452 0.07489702 0.1462262  0.86473855 0.70503936\n",
    " 0.41670694 0.68756467]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718c6827",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.7601782  0.04886587 0.06888839 0.12813492 0.90029203 0.67683953\n",
    " 0.3892159  0.65940779]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f22dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.80911583 0.04892541 0.06904    0.12325186 0.88567412 0.67987295\n",
    " 0.3925011  0.66245647]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9baff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"min_Amino_terminal_brain_natriuretic_peptide_precursor_NT_proBNP_\"] > 473][[\"min_Amino_terminal_brain_natriuretic_peptide_precursor_NT_proBNP_\", \"cens\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07de2d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(bst.models[0].nodes[0].leaf_model.lists[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c7d1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = bst.models[0].nodes[25].leaf_model.lists\n",
    "dt = np.array(d[\"time\"])\n",
    "dc = np.array(d[\"cens\"])\n",
    "\n",
    "km = KaplanMeierFitter()\n",
    "km.fit(dt, dc)\n",
    "km.plot()\n",
    "\n",
    "dt[np.where(dc == 0)] = 500\n",
    "\n",
    "km = KaplanMeierFitter()\n",
    "km.fit(dt, dc)\n",
    "km.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c5bf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(bst.models[0].nodes)):\n",
    "    print(i, bst.models[0].nodes[i].get_full_rule())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69391fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt[np.where(dc == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0483c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.models[0].visualize(mode=\"surv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd33f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.80911583 0.04892541 0.06904    0.12325186 0.88567412 0.67987295\n",
    " 0.3925011  0.66245647]\n",
    "\n",
    "[0.81034955 0.04914468 0.06954718 0.12530352 0.88259484 0.67759486\n",
    " 0.38931385 0.66012328]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03362b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.62278679 0.10448363 0.14378023 0.33201395 0.7578171  0.69019376\n",
    " 0.51303588 0.67012509]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19baae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'aggreg_func': 'wei', 'weighted_tree': True,\n",
    "         'balance': None, 'categ': categ, 'all_weight': True,  # \"only_log_rank\"\n",
    "         'criterion': 'weights', 'depth': 7, 'ens_metric_name': 'IBS_REMAIN', \n",
    "         'leaf_model': 'base_fast', 'max_features': 0.3, 'min_samples_leaf': 0.05, \n",
    "         'n_estimators': 100, 'n_jobs': 1, 'size_sample': 1.0}\n",
    "\n",
    "[0.81172036 0.04935577 0.06994577 0.12587805 0.88561388 0.67656763\n",
    " 0.38859076 0.65911449]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b6f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.79718986 0.04992077 0.07067745 0.12626976 0.87837713 0.6817066\n",
    " 0.39409409 0.66427554]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3855353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(np.ones_like(bst.bettas)) / np.cumsum(bst.bettas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7914581c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "axx = 1\n",
    "for mtr in [\"ibs_REMAIN\", \"CI_CENS\", \"IAUC_WW_TI\", \"AUPRC\"]:\n",
    "    bst.plot_curve(X_tr, y_tr, bins_HO, \"train\", mtr, axes=axx)\n",
    "    bst.plot_curve(X_HO, y_HO, bins_HO, \"test\", mtr, axes=axx)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "axx = None\n",
    "for mtr in [\"ibs_REMAIN\", \"AUPRC\"]:\n",
    "    bst.plot_curve(X_tr, y_tr, bins_HO, \"train\", mtr, axes=axx)\n",
    "    bst.plot_curve(X_HO, y_HO, bins_HO, \"test\", mtr, axes=axx)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969a84de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "axx = 1\n",
    "bst.plot_curve(X_tr, y_tr, bins_HO, \"train\", \"ibs_REMAIN\", axes=axx)\n",
    "bst.plot_curve(X_HO, y_HO, bins_HO, \"test\", \"ibs_REMAIN\", axes=axx)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "bst.plot_curve(X_tr, y_tr, bins_HO, \"train\", \"auprc\", axes=axx)\n",
    "bst.plot_curve(X_HO, y_HO, bins_HO, \"test\", \"auprc\", axes=axx)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c2e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "axx = 1\n",
    "bst.plot_curve(X_tr, y_tr, bins_HO, \"train\", \"ibs_REMAIN\", axes=axx)\n",
    "bst.plot_curve(X_HO, y_HO, bins_HO, \"test\", \"ibs_REMAIN\", axes=axx)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "bst.plot_curve(X_tr, y_tr, bins_HO, \"train\", \"auprc\", axes=axx)\n",
    "bst.plot_curve(X_HO, y_HO, bins_HO, \"test\", \"auprc\", axes=axx)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9833b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "axx = 1\n",
    "bst.plot_curve(X_tr, y_tr, bins_HO, \"train\", \"ibs_REMAIN\", axes=axx)\n",
    "bst.plot_curve(X_HO, y_HO, bins_HO, \"test\", \"ibs_REMAIN\", axes=axx)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "bst.plot_curve(X_tr, y_tr, bins_HO, \"train\", \"auprc\", axes=axx)\n",
    "bst.plot_curve(X_HO, y_HO, bins_HO, \"test\", \"auprc\", axes=axx)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac308a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "axx = None\n",
    "bst.plot_curve(X_tr, y_tr, bins_HO, \"train\", \"ibs_REMAIN\", axes=axx)\n",
    "bst.plot_curve(X_HO, y_HO, bins_HO, \"test\", \"ibs_REMAIN\", axes=axx)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "bst.plot_curve(X_tr, y_tr, bins_HO, \"train\", \"auprc\", axes=axx)\n",
    "bst.plot_curve(X_HO, y_HO, bins_HO, \"test\", \"auprc\", axes=axx)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308bb9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.plot_curve(X_tr, y_tr, bins_HO, \"train\", \"ibs\")\n",
    "bst.plot_curve(X_HO, y_HO, bins_HO, \"test\", \"ibs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "bst.plot_curve(X_tr, y_tr, bins_HO, \"train\", \"auprc\")\n",
    "bst.plot_curve(X_HO, y_HO, bins_HO, \"test\", \"auprc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89808c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.plot_curve(X_tr, y_tr, bins_HO, \"train\", \"ibs\")\n",
    "bst.plot_curve(X_HO, y_HO, bins_HO, \"test\", \"ibs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "bst.plot_curve(X_tr, y_tr, bins_HO, \"train\", \"auprc\")\n",
    "bst.plot_curve(X_HO, y_HO, bins_HO, \"test\", \"auprc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c92102",
   "metadata": {},
   "outputs": [],
   "source": [
    "## [0.79520219 0.07152668 0.12995207 0.86920129 0.67270816 0.38657166 0.65536655]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0139e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_HO[y_HO[\"time\"] > 25])\n",
    "X_HO[(y_HO[\"time\"] > 25) & (y_HO[\"cens\"] == 1)].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e13b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_HO[(y_HO[\"time\"] > 25) & (y_HO[\"cens\"] == 0)].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef260b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "model = sm.OLS(y[\"cens\"], X.fillna(0).replace(np.nan, 0))\n",
    "results = model.fit()\n",
    "results.params\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02027cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"mean_Amino_terminal_brain_natriuretic_peptide_precursor_NT_proBNP_\", \"cens\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6308799",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(df.reset_index(drop=True), x=\"mean_thrombocytocrit\", y=\"time\", palette=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0c690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = []\n",
    "l2 = []\n",
    "for b in bins_HO[1:]:\n",
    "    ind = np.where(y_HO[\"time\"] <= b)\n",
    "    l1.append(metr.ibs_remain(y_tr, y_HO[ind], pred_surv[ind], bins_HO))\n",
    "    l2.append(metr.ibs_remain(y_tr, y_HO[ind], pred_surv_bstr[ind], bins_HO))\n",
    "plt.plot(bins_HO[1:], l1)\n",
    "plt.plot(bins_HO[1:], l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfab61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.617393   0.14809655 0.34428443 0.75925409 0.68397882 0.50880313 0.6641347 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e04e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.62384209 0.14637976 0.34313499 0.77219706 0.67969879 0.50319173 0.65970385]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b31aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.62904045 0.14410236 0.3889508  0.75498701 0.68365474 0.5262897 0.66582823]\n",
    "\n",
    "[0.63443424 0.14384832 0.34170951 0.76274264 0.68895888 0.50904109 0.66857756]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c537d0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0.80020562 0.06946576 0.12550715 0.88966621 0.67995909 0.39606099\n",
    "#  0.66275315]\n",
    "\n",
    "# [0.80089102 0.06919537 0.12511141 0.89019215 0.68100682 0.39766488\n",
    "#  0.66383458]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d43e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.80089102 0.06919537 0.12511141 0.89019215 0.68100682 0.39766488 0.66383458]\n",
    "\n",
    "[0.80020562 0.12550715 0.88966621 0.67995909 0.39606099 0.66275315]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856267fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibs_vals = metr.ibs_remain(y_tr, y_HO, pred_surv, bins_HO, axis=0)\n",
    "plt.scatter(y_HO[\"time\"], ibs_vals, c=np.where(y_HO[\"cens\"], \"orange\", \"blue\"))\n",
    "plt.title(\"TEST\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"IBS_WW\")\n",
    "plt.show()\n",
    "print(np.mean(ibs_vals), np.std(ibs_vals))\n",
    "\n",
    "pred_sv_tr = bst.predict_at_times(X_tr, bins=bins_HO, mode=\"surv\")\n",
    "ibs_vals = metr.ibs_remain(y_tr, y_tr, pred_sv_tr, bins_HO, axis=0)\n",
    "plt.scatter(y_tr[\"time\"], ibs_vals, c=np.where(y_tr[\"cens\"], \"orange\", \"blue\"))\n",
    "plt.title(\"TRAIN\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"IBS_WW\")\n",
    "print(np.mean(ibs_vals), np.std(ibs_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09613d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibs_vals = metr.ibs_remain(y_tr, y_HO, pred_surv, bins_HO, axis=0)\n",
    "plt.scatter(y_HO[\"time\"], ibs_vals, c=np.where(y_HO[\"cens\"], \"orange\", \"blue\"))\n",
    "plt.show()\n",
    "print(np.mean(ibs_vals), np.std(ibs_vals))\n",
    "\n",
    "pred_sv_tr = bst.predict_at_times(X_tr, bins=bins_HO, mode=\"surv\")\n",
    "ibs_vals = metr.ibs_remain(y_tr, y_tr, pred_sv_tr, bins_HO, axis=0)\n",
    "plt.scatter(y_tr[\"time\"], ibs_vals, c=np.where(y_tr[\"cens\"], \"orange\", \"blue\"))\n",
    "print(np.mean(ibs_vals), np.std(ibs_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27b2cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibs_vals = metr.ibs_remain(y_tr, y_HO, pred_surv, bins_HO, axis=0)\n",
    "plt.scatter(y_HO[\"time\"], ibs_vals, c=np.where(y_HO[\"cens\"], \"orange\", \"blue\"))\n",
    "plt.show()\n",
    "print(np.mean(ibs_vals), np.std(ibs_vals))\n",
    "\n",
    "pred_sv_tr = bst.predict_at_times(X_tr, bins=bins_HO, mode=\"surv\")\n",
    "ibs_vals = metr.ibs_remain(y_tr, y_tr, pred_sv_tr, bins_HO, axis=0)\n",
    "plt.scatter(y_tr[\"time\"], ibs_vals, c=np.where(y_tr[\"cens\"], \"orange\", \"blue\"))\n",
    "print(np.mean(ibs_vals), np.std(ibs_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fed0d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibs_vals = metr.ibs_remain(y_tr, y_HO, pred_surv, bins_HO, axis=0)\n",
    "plt.scatter(y_HO[\"time\"], ibs_vals, c=np.where(y_HO[\"cens\"], \"orange\", \"blue\"))\n",
    "plt.show()\n",
    "print(np.mean(ibs_vals), np.std(ibs_vals))\n",
    "\n",
    "pred_sv_tr = bst.predict_at_times(X_tr, bins=bins_HO, mode=\"surv\")\n",
    "ibs_vals = metr.ibs_remain(y_tr, y_tr, pred_sv_tr, bins_HO, axis=0)\n",
    "plt.scatter(y_tr[\"time\"], ibs_vals, c=np.where(y_tr[\"cens\"], \"orange\", \"blue\"))\n",
    "print(np.mean(ibs_vals), np.std(ibs_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ee33c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibs_vals = metr.auprc(y_tr, y_HO, pred_surv, bins_HO, axis=0)\n",
    "plt.scatter(y_HO[\"time\"], ibs_vals, c=np.where(y_HO[\"cens\"], \"orange\", \"blue\"))\n",
    "print(np.mean(ibs_vals), np.std(ibs_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d45f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "GBSG 0.10520 0.09359"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28a3d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.62458472 0.14642696 0.33393545 0.77167726 0.68903982 0.50546998 0.6682448 ]\n",
    "\n",
    "[0.61219465 0.14676605 0.3704488  0.75069901 0.68089246 0.51223358 0.66178657]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5530b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.62454563 0.16316924 0.31681772 0.76381182 0.68671327 0.48745588 0.66414114] # wei base_fast\n",
    "[0.61864374 0.16558463 0.31666503 0.76349835 0.68477513 0.48610323 0.66226933] # mean base_fast\n",
    "\n",
    "[0.62532734 0.14605382 0.33177793 0.76701593 0.68692525 0.50427755 0.66623469] # mean base_zero_after\n",
    "[0.62161423 0.14433037 0.33712839 0.77097816 0.68708272 0.50941252 0.66695602] # wei base_zero_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812ea9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m_i, b in enumerate(bst.models):\n",
    "    l = []\n",
    "    for i, n in b.nodes.items():\n",
    "        if n.is_leaf:\n",
    "            l.append(n.leaf_model.shape[0])\n",
    "    \n",
    "    plt.scatter(x=np.ones_like(l)*m_i, y=l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87aefbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m_i, b in enumerate(bst.models):\n",
    "    l = []\n",
    "    for i, n in b.nodes.items():\n",
    "        if n.is_leaf:\n",
    "            l.append(n.leaf_model.shape[0])\n",
    "    \n",
    "    plt.scatter(x=np.ones_like(l)*m_i, y=l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96b8a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.78265936 0.19179243 0.85868291 0.65793649 0.36772733 0.64034806]\n",
    "[0.78457848 0.13934414 0.87004602 0.65747228 0.36876048 0.63997459]\n",
    "\n",
    "[0.80020562 0.12550715 0.88966621 0.67995909 0.39606099 0.66275315]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a80a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.62016807 0.31981274 0.7676767  0.67971621 0.48581002 0.65775028] # base_fast with partial ens ibs as wei\n",
    "[0.61031855 0.3421834  0.75881519 0.68310193 0.50573737 0.66300985] # base_zero_after with partial ens ibs as wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b6b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.6227477  0.31638412 0.77072591 0.68433562 0.48604648 0.66187318] # base_fast with mean ens ibs as wei\n",
    "[0.62149697 0.33383418 0.77246182 0.68627578 0.50415848 0.6656453 ] # base_zero_after with mean ens ibs as wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2752a9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.61872191 0.33394521 0.76483232 0.68241727 0.49935149 0.66167935]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5078a1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fdd905",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.hstack([np.linspace(10, 1, 50), np.linspace(1, 5, 50)])\n",
    "plt.plot(x)\n",
    "plt.plot(np.cumsum(x[::-1])[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098bc11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linprog\n",
    "\n",
    "obj = [0.3, 0.4, 0.77, 0.1, 0.2]\n",
    "\n",
    "lhs_ineq = []  # левая сторона желтого неравенства\n",
    "rhs_ineq = []\n",
    "\n",
    "lhs_eq = [np.ones(len(obj))]  # левая сторона зеленого равенства\n",
    "rhs_eq = [1]       # правая сторона зеленого равенства\n",
    "\n",
    "bnd = list(zip(np.zeros(len(obj)), \n",
    "               np.ones(len(obj))))\n",
    "\n",
    "opt = linprog(c=obj, # A_ub=lhs_ineq, b_ub=rhs_ineq,\n",
    "              A_eq=lhs_eq, b_eq=rhs_eq, bounds=bnd,\n",
    "              method=\"revised simplex\")\n",
    "\n",
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e38f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "\n",
    "def get_sum_sqrt_rand1(ibss_):\n",
    "    a = np.random.rand(n)\n",
    "    a = a / np.sum(a)\n",
    "    return np.sum(ibss_) * np.sum(a**2)\n",
    "\n",
    "def get_sum_sqrt_rand2(ibss_):\n",
    "    a = np.random.rand(n)\n",
    "    a = a / np.sum(a)\n",
    "    return np.dot(ibss_, a) * np.sum(a)\n",
    "\n",
    "def get_sum_sqrt_rand3(ibss_):\n",
    "    a = np.random.rand(n)\n",
    "    a = a / np.sum(a)\n",
    "    return np.dot(ibss_, a**2) * a.shape[0]\n",
    "\n",
    "np.random.seed(100)\n",
    "ibss_ = np.random.rand(n)\n",
    "plt.hist([get_sum_sqrt_rand3(ibss_) for i in range(1000)])\n",
    "plt.hist([get_sum_sqrt_rand2(ibss_) for i in range(1000)])\n",
    "plt.hist([get_sum_sqrt_rand1(ibss_) for i in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d61a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_h = []\n",
    "x_h = np.linspace(0.01, 3.0, 100)\n",
    "for i in x_h:\n",
    "    tm = np.clip(y[4][\"time\"]*i, bins_HO[0], bins_HO[-1])\n",
    "    sfs_h.append(np.where(tm > bins_HO, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49cedba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_v = []\n",
    "x_v = np.linspace(0.0, 0.5, 100)\n",
    "for i in x_v:\n",
    "    sfs_v.append(np.where(y[4][\"time\"] > bins_HO, 1 - i, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24654821",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_one = np.repeat(y[4], 100)\n",
    "\n",
    "plt.scatter(x_h, metr.auprc(y_one, y_one, np.vstack(sfs_h), bins_HO, axis=0))\n",
    "plt.scatter(x_h, metr.ibs_WW(y_one, y_one, np.vstack(sfs_h), bins_HO, axis=0))\n",
    "plt.show()\n",
    "plt.scatter(x_v, metr.auprc(y_one, y_one, np.vstack(sfs_v), bins_HO, axis=0))\n",
    "plt.scatter(x_v, metr.ibs_WW(y_one, y_one, np.vstack(sfs_v), bins_HO, axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5d21b9",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c866b790",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = np.ones((y_tr.shape[0], bins_HO.shape[0]))*0.5\n",
    "sf[:, 0] = 1\n",
    "sf[:, 0] = 0\n",
    "\n",
    "# sf[:, np.where(bins_HO > 500)] = 0.75\n",
    "# sf[:, np.where(bins_HO > 1000)] = 0.5\n",
    "# sf[:, np.where(bins_HO > 1500)] = 0\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(6, 8))\n",
    "ibs_vals = metr.ibs_WW(y_tr, y_tr, sf, bins_HO, axis=0)\n",
    "axes[0].scatter(y_tr[\"time\"], ibs_vals, c=np.where(y_tr[\"cens\"], \"orange\", \"blue\"))\n",
    "\n",
    "auprc_vals = metr.auprc(y_tr, y_tr, sf, bins_HO, axis=0)\n",
    "axes[1].scatter(y_tr[\"time\"], auprc_vals, c=np.where(y_tr[\"cens\"], \"orange\", \"blue\"))\n",
    "print(np.mean(ibs_vals), np.mean(auprc_vals))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448d62e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.widgets import Button, Slider\n",
    "%matplotlib notebook\n",
    "\n",
    "# The parametrized function to be plotted\n",
    "def metr_vals(y_tr, bins_HO, height, time, axis=1, mtr=\"ibs\"):\n",
    "    y_tr_ = cnt.get_y(cens=[True], time=[time])\n",
    "    sf = np.ones((y_tr_.shape[0], bins_HO.shape[0]))*height\n",
    "    sf[:, np.where(bins_HO > time)] *= 0.9\n",
    "    sf[:, np.where(bins_HO > time)] += 0.1\n",
    "    sf[:, np.where(bins_HO > time)] = 0\n",
    "    \n",
    "    if mtr == \"ibs\":\n",
    "        return metr.ibs_WW(y_tr_, y_tr_, sf, bins_HO, axis=axis)\n",
    "    return metr.auprc(y_tr_, y_tr_, sf, bins_HO, axis=axis)\n",
    "\n",
    "# Define initial parameters\n",
    "init_time = 500\n",
    "init_height = 0.5\n",
    "\n",
    "# Create the figure and the line that we will manipulate\n",
    "fig, ax = plt.subplots()\n",
    "line,  = ax.plot(bins_HO, \n",
    "                   metr_vals(y_tr, bins_HO, init_height, init_time),) \n",
    "#                    c=np.where(y_tr[\"cens\"], \"orange\", \"blue\"))\n",
    "ax.set_xlabel('Time [s]')\n",
    "\n",
    "# adjust the main plot to make room for the sliders\n",
    "fig.subplots_adjust(left=0.25, bottom=0.25)\n",
    "\n",
    "# Make a horizontal slider to control the frequency.\n",
    "axfreq = fig.add_axes([0.25, 0.1, 0.65, 0.03])\n",
    "time_slider = Slider(\n",
    "    ax=axfreq,\n",
    "    label='Time',\n",
    "    valmin=bins_HO[0],\n",
    "    valmax=bins_HO[-1],\n",
    "    valinit=init_time,\n",
    ")\n",
    "\n",
    "# Make a vertically oriented slider to control the amplitude\n",
    "axamp = fig.add_axes([0.1, 0.25, 0.0225, 0.63])\n",
    "height_slider = Slider(\n",
    "    ax=axamp,\n",
    "    label=\"Height\",\n",
    "    valmin=0,\n",
    "    valmax=1,\n",
    "    valinit=init_height,\n",
    "    orientation=\"vertical\"\n",
    ")\n",
    "\n",
    "\n",
    "# The function to be called anytime a slider's value changes\n",
    "def update(val):\n",
    "    by_time = metr_vals(y_tr, bins_HO, height_slider.val, time_slider.val)\n",
    "    mean_ibs = metr_vals(y_tr, bins_HO, height_slider.val, time_slider.val, axis=-1, mtr=\"ibs\")\n",
    "    mean_auprc = metr_vals(y_tr, bins_HO, height_slider.val, time_slider.val, axis=-1, mtr=\"auprc\")\n",
    "    line.set_ydata(by_time)\n",
    "    ax.set_title(f'IBS:{mean_ibs:.5f}, AUPRC:{mean_auprc:.5f}')\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "\n",
    "# register the update function with each slider\n",
    "time_slider.on_changed(update)\n",
    "height_slider.on_changed(update)\n",
    "\n",
    "# Create a `matplotlib.widgets.Button` to reset the sliders to initial values.\n",
    "resetax = fig.add_axes([0.8, 0.025, 0.1, 0.04])\n",
    "button = Button(resetax, 'Reset', hovercolor='0.975')\n",
    "\n",
    "def reset(event):\n",
    "    time_slider.reset()\n",
    "    height_slider.reset()\n",
    "button.on_clicked(reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280bb3e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7299a36a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
