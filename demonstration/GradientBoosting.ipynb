{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75ee3969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalene extension successfully loaded. Note: Scalene currently only\n",
      "supports CPU+GPU profiling inside Jupyter notebooks. For full Scalene\n",
      "profiling, use the command line version.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import pathlib, tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from graphviz import Digraph\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import stats\n",
    "\n",
    "from survivors import metrics as metr\n",
    "from survivors import constants as cnt\n",
    "from survivors import criteria as crit\n",
    "from numba import njit, jit, int32, float64\n",
    "from lifelines import KaplanMeierFitter, NelsonAalenFitter\n",
    "\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "%load_ext line_profiler\n",
    "%load_ext scalene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "125e0066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import survivors.datasets as ds\n",
    "from survivors.tree import CRAID\n",
    "\n",
    "param = {'criterion': 'peto', 'cut': True, 'depth': 10,\n",
    "         'max_features': 1.0, 'min_samples_leaf': 5, \n",
    "         'signif': 0.05}\n",
    "\n",
    "X, y, features, categ, sch_nan = ds.load_pbc_dataset()\n",
    "param[\"categ\"] = categ\n",
    "\n",
    "# cr = CRAID(**param)\n",
    "# cr.fit(X, y)\n",
    "\n",
    "# pred_cens = cr.predict(X, target=\"cens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a66393c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trt', 'sex', 'ascites', 'hepato', 'spiders']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "acf5384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def coxph_negative_gradient(cnp.npy_uint8[:] event,\n",
    "#                             cnp.npy_double[:] time,\n",
    "#                             cnp.npy_double[:] y_pred):\n",
    "#     cdef cnp.npy_double s\n",
    "#     cdef int i\n",
    "#     cdef int j\n",
    "#     cdef cnp.npy_intp n_samples = event.shape[0]\n",
    "\n",
    "#     cdef cnp.ndarray[cnp.npy_double, ndim=1] gradient = cnp.PyArray_EMPTY(1, &n_samples, cnp.NPY_DOUBLE, 0)\n",
    "#     cdef cnp.npy_double[:] exp_tsj = cnp.PyArray_ZEROS(1, &n_samples, cnp.NPY_DOUBLE, 0)\n",
    "\n",
    "#     cdef cnp.npy_double[:] exp_pred = np.exp(y_pred)\n",
    "#     with nogil:\n",
    "#         for i in range(n_samples):\n",
    "#             for j in range(n_samples):\n",
    "#                 if time[j] >= time[i]:\n",
    "#                     exp_tsj[i] += exp_pred[j]\n",
    "\n",
    "#         for i in range(n_samples):\n",
    "#             s = 0\n",
    "#             for j in range(n_samples):\n",
    "#                 if event[j] and time[i] >= time[j]:\n",
    "#                     s += exp_pred[i] / exp_tsj[j]\n",
    "#             gradient[i] = event[i] - s\n",
    "\n",
    "#     return gradient\n",
    "\n",
    "from numba import njit, jit\n",
    "\n",
    "@jit  # ('f8(i8[:], f8[:], f8[:])')\n",
    "def coxph_negative_gradient(event, time, y_pred):\n",
    "    n_samples = event.shape[0]\n",
    "\n",
    "    gradient = np.zeros(n_samples, dtype=float)\n",
    "    exp_tsj = np.zeros(n_samples, dtype=float)\n",
    "\n",
    "    exp_pred = np.exp(y_pred)\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_samples):\n",
    "            if time[j] >= time[i]:\n",
    "                exp_tsj[i] += exp_pred[j]\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        s = 0\n",
    "        for j in range(n_samples):\n",
    "            if event[j] and time[i] >= time[j]:\n",
    "                s += exp_pred[i] / exp_tsj[j]\n",
    "        gradient[i] = event[i] - s\n",
    "\n",
    "    return gradient\n",
    "\n",
    "@jit\n",
    "def fast_coxph_negative_gradient(event, time, y_pred):\n",
    "    n_samples = event.shape[0]\n",
    "    r = np.repeat(time[:, np.newaxis], n_samples, axis=1)\n",
    "    exp_pred = np.exp(y_pred)\n",
    "    exp_tsj = exp_pred.dot(r >= r.T)\n",
    "\n",
    "    shared_j = event/exp_tsj\n",
    "    gradient = event - exp_pred*shared_j.dot(r <= r.T)\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ac3352c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569 µs ± 4.47 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit coxph_negative_gradient(y[\"cens\"], y[\"time\"], pred_cens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f804c504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.98 ms ± 5.39 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit fast_coxph_negative_gradient(y[\"cens\"], y[\"time\"], pred_cens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f673be7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.        ,  0.85628939],\n",
       "       [ 0.        ,  0.16470588, -0.93795051],\n",
       "       [ 1.        ,  0.625     ,  0.73621406],\n",
       "       ...,\n",
       "       [ 0.        ,  0.16470588, -0.16909675],\n",
       "       [ 0.        ,  0.16470588, -0.09234032],\n",
       "       [ 0.        ,  0.01724138, -0.13255543]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([y[\"cens\"], pred_cens, grad]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55d42917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nan * 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af011f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.ensemble import RandomSurvivalForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "209a3598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from survivors.metrics import ibs, iauc, ipa, get_survival_func\n",
    "from survivors.constants import get_y\n",
    "\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines import NelsonAalenFitter\n",
    "\n",
    "bins = np.array([1, 10, 100, 1000])\n",
    "\n",
    "y_train = get_y(np.array([1, 0]), np.array([100, 100]))\n",
    "y_test_1 = get_y(np.array([1]), np.array([100]))\n",
    "y_test_2 = get_y(np.array([0]), np.array([100]))\n",
    "y_test_3 = get_y(np.array([1]), np.array([50]))\n",
    "\n",
    "kmf_train = KaplanMeierFitter()\n",
    "kmf_train.fit(y_train['time'], event_observed=y_train['cens'])\n",
    "sf_train = kmf_train.survival_function_at_times(bins).to_numpy()[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "2ace070a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23648648648648649"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ibs(y_train, y_test_3, sf_train, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "37b83bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaplanMeier:\n",
    "    def __init__(self):\n",
    "        self.timeline = None\n",
    "        self.survival_function = None\n",
    "        self.confidence_interval_ = None\n",
    "        self.alpha = 0.05\n",
    "\n",
    "    def fit(self, durations, right_censor, weights=None):\n",
    "        if weights is None:\n",
    "            weights = np.ones(right_censor.shape)\n",
    "        self.timeline = np.unique(durations)\n",
    "\n",
    "        dur_ = np.searchsorted(self.timeline, durations)\n",
    "        hist_dur = np.bincount(dur_, weights=weights)\n",
    "        self.hist_cens = np.bincount(dur_, weights=right_censor*weights)\n",
    "        self.cumul_hist_dur = np.cumsum(hist_dur[::-1])[::-1]\n",
    "        self.survival_function = np.hstack([1.0, np.cumprod((1.0 - self.hist_cens / (self.cumul_hist_dur)))])\n",
    "\n",
    "    def count_confidence_interval(self):\n",
    "        ''' exponential Greenwood: https://www.math.wustl.edu/~sawyer/handouts/greenwood.pdf '''\n",
    "        z = ss.norm.ppf(1 - self.alpha / 2)\n",
    "        cumulative_sq_ = np.sqrt(np.hstack([0.0, np.cumsum(self.hist_cens / (self.cumul_hist_dur * (self.cumul_hist_dur - self.hist_cens)))]))\n",
    "        np.nan_to_num(cumulative_sq_, copy=False, nan=0)\n",
    "        v = np.log(self.survival_function)\n",
    "        np.nan_to_num(v, copy=False, nan=0)\n",
    "        self.confidence_interval_ = np.vstack([np.exp(v * np.exp(- z * cumulative_sq_ / v)),\n",
    "                                               np.exp(v * np.exp(+ z * cumulative_sq_ / v))]).T\n",
    "        np.nan_to_num(self.confidence_interval_, copy=False, nan=1)\n",
    "\n",
    "    def get_confidence_interval_(self):\n",
    "        if self.confidence_interval_ is None:\n",
    "            self.count_confidence_interval()\n",
    "        return self.confidence_interval_\n",
    "\n",
    "    def survival_function_at_times(self, times):\n",
    "        place_bin = np.digitize(times, self.timeline)\n",
    "        return self.survival_function[np.clip(place_bin, 0, None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "9f8c9ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jit()\n",
    "# def _brier_score(ti, t, s, g_t, g_ti, d):\n",
    "#     if (ti <= t) and d == 1:\n",
    "#         return np.power(s, 2) * 1./g_ti\n",
    "#     if ti > t:\n",
    "#         return np.power(1 - s, 2) * 1./g_t\n",
    "#     return 0.\n",
    "\n",
    "@jit()\n",
    "def _brier_score(ti, t, s, g_t, g_ti, d):\n",
    "    return np.where(ti > t, \n",
    "                    np.power(1 - s, 2) * 1./g_t, \n",
    "                    np.where(d == 1, \n",
    "                             np.power(s, 2) * 1./g_ti, \n",
    "                             0.0))\n",
    "\n",
    "def _inverse_censoring_metric(func):\n",
    "    def metric(survival_train, survival_test, estimate, times):\n",
    "        ipcw = KaplanMeier()\n",
    "        ipcw.fit(survival_train[\"time\"], 1 - survival_train[\"cens\"])\n",
    "        g_t = ipcw.survival_function_at_times(times)\n",
    "        g_ti = ipcw.survival_function_at_times(survival_test[\"time\"])\n",
    "        arr = np.zeros_like(estimate, dtype=float)\n",
    "        \n",
    "        for i, t in enumerate(times):\n",
    "            arr[:,i] = func(survival_test[\"time\"], t, estimate[:,i], g_t[i], g_ti, survival_test[\"cens\"])\n",
    "        return arr\n",
    "    return metric\n",
    "\n",
    "def _integrated_metric(func):\n",
    "    def metric(survival_train, survival_test, estimate, times):\n",
    "        scores = func(survival_train, survival_test, estimate, times)\n",
    "        integral = np.trapz(scores, times)\n",
    "        return integral / (times[-1] - times[0])\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "f3299741",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibs__ = _inverse_censoring_metric(_brier_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "62c86078",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibs_ = _integrated_metric(ibs__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "e8835799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.   0.25 0.25]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.23648649])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ibs_(y_train, y_test_3, sf_train, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e03d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b370aaa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "f4e413a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from survivors.experiments.grid import generate_sample\n",
    "\n",
    "X, y, features, categ, sch_nan = ds.load_pbc_dataset()\n",
    "gen = generate_sample(X, y, 5)\n",
    "X_train, y_train, X_test, y_test, bins = next(gen)\n",
    "\n",
    "cr = CRAID(**param)\n",
    "cr.fit(X_train, y_train)\n",
    "\n",
    "pred_sf = cr.predict_at_times(X_test, bins, mode=\"surv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "ebbd5c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_hf = cr.predict_at_times(X_test, bins, mode=\"hazard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "9afd2001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.1 ms ± 361 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ibs_(y_train, y_test, pred_sf, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "76cdfbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 ms ± 623 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ibs(y_train, y_test, pred_sf, bins, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "32caa307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood(time, cens, sf, cumhf, bins):\n",
    "    index_times = np.digitize(time, bins, right=True) - 1\n",
    "    hf = np.hstack((cumhf[:, 0][np.newaxis].T, np.diff(cumhf)))\n",
    "    sf_by_times = np.take_along_axis(sf, index_times[:, np.newaxis], axis=1)[:, 0] + 1e-10\n",
    "    hf_by_times = (np.take_along_axis(hf, index_times[:, np.newaxis], axis=1)[:, 0] + 1e-10)**cens\n",
    "    likelihood = np.sum(np.log(sf_by_times) + np.log(hf_by_times))\n",
    "    return likelihood\n",
    "\n",
    "def kl(time, cens, sf, cumhf, bins):\n",
    "    index_times = np.digitize(time, bins, right=True) - 1\n",
    "    hf = np.hstack((cumhf[:, 0][np.newaxis].T, np.diff(cumhf)))\n",
    "    sf_by_times = np.take_along_axis(sf, index_times[:, np.newaxis], axis=1)[:, 0] + 1e-10\n",
    "    hf_by_times = (np.take_along_axis(hf, index_times[:, np.newaxis], axis=1)[:, 0] + 1e-10)**cens\n",
    "    likelihood = np.sum(np.log(sf_by_times) + np.log(hf_by_times))\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "55b35369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1099.563431516333"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loglikelihood(y_test[\"time\"], y_test[\"cens\"], pred_sf, pred_hf, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "0c63fbfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ...,  0,  1, 12], dtype=int64)"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[\"time\"], bins\n",
    "np.bincount(index_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "a58ec5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([103.07480247,  87.3916971 ,  36.01652794, 108.36656156,\n",
       "        86.50062991,   0.        ,   5.05844785,  87.3916971 ,\n",
       "        86.50062991,  86.50062991,  36.01652794, 103.07480247,\n",
       "         0.        , 111.14680412,  87.3916971 ,   0.        ,\n",
       "        86.50062991, 103.07480247,  87.3916971 , 103.07480247,\n",
       "        36.01652794,  86.50062991,  86.50062991, 108.36656156,\n",
       "         0.        ,  86.50062991, 103.07480247, 103.07480247,\n",
       "        72.20241692,  86.50062991,  86.50062991, 108.36656156,\n",
       "         0.        ,  87.3916971 ,  87.3916971 ,  72.20241692,\n",
       "       103.07480247,  86.50062991,  36.01652794,  36.01652794,\n",
       "        86.50062991, 108.36656156,  87.3916971 ,  86.50062991,\n",
       "        72.20241692, 103.07480247,   0.        , 108.36656156,\n",
       "        36.01652794,  36.01652794,  87.3916971 ,  87.3916971 ,\n",
       "         0.        ,   0.        ,   5.05844785,   0.        ,\n",
       "        72.20241692,  36.01652794,   0.        ,  68.7309792 ,\n",
       "        68.7309792 ,  36.01652794,   0.        ,   0.        ,\n",
       "        87.3916971 ,   5.05844785,  87.3916971 , 108.36656156,\n",
       "         0.        ,   0.        ,   5.05844785,  68.7309792 ,\n",
       "         0.        ,  86.50062991,   0.        ,  86.50062991,\n",
       "        86.50062991,   0.        ,  36.01652794,  36.01652794,\n",
       "        68.7309792 ,   0.        , 108.36656156,   0.        ])"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kullback–Leibler divergence\n",
    "\n",
    "index_times = np.digitize(y_test[\"time\"], bins, right=True) - 1\n",
    "arr = np.zeros_like(pred_hf, dtype=int)\n",
    "arr[np.arange(arr.shape[0]), index_times] = 1\n",
    "hf = np.hstack((pred_hf[:, 0][np.newaxis].T, np.diff(pred_hf)))\n",
    "\n",
    "np.sum(hf * np.log((hf + 1e-20)/(arr + 1e-20)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "0b7eaa2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00091614, 0.00281556, 0.01873142, 0.13838038, 0.83915649])"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def arc_x4(weights):\n",
    "    scaled = 1 + weights**4\n",
    "    return scaled/sum(scaled)\n",
    "\n",
    "wei = np.array([-0.1, -1.2, -2.1, -3.5, -5.5])\n",
    "arc_x4(wei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "61bc9b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "645.0"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0381894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from survivors.ensemble import BoostingCRAID\n",
    "\n",
    "def loglikelihood_i(time, cens, sf, cumhf, bins):\n",
    "    index_times = np.digitize(time, bins, right=True) - 1\n",
    "    hf = np.hstack((cumhf[:, 0][np.newaxis].T, np.diff(cumhf)))\n",
    "    sf_by_times = np.take_along_axis(sf, index_times[:, np.newaxis], axis=1)[:, 0] + 1e-10\n",
    "    hf_by_times = (np.take_along_axis(hf, index_times[:, np.newaxis], axis=1)[:, 0] + 1e-10)**cens\n",
    "    return np.log(sf_by_times) + np.log(hf_by_times)\n",
    "\n",
    "def values_to_hist(values):\n",
    "    unq, idx = np.unique(values, return_inverse=True)\n",
    "    # calculate the weighted frequencies of these indices\n",
    "    freqs_idx = np.bincount(idx)\n",
    "    # reconstruct the array of frequencies of the elements\n",
    "    return freqs_idx[idx]\n",
    "\n",
    "def arc_x4(weights):\n",
    "    scaled = 1 + weights**4\n",
    "    return scaled/sum(scaled)\n",
    "\n",
    "class ProbBoostingCRAID(BoostingCRAID):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.name = \"ProbBoostingCRAID\"\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.features = X.columns\n",
    "        X = X.reset_index(drop=True)\n",
    "        X[cnt.CENS_NAME] = y[cnt.CENS_NAME].astype(np.int32)\n",
    "        X[cnt.TIME_NAME] = y[cnt.TIME_NAME].astype(np.float32)\n",
    "        \n",
    "        self.X_train = X\n",
    "        self.X_train[\"ind_start\"] = self.X_train.index\n",
    "        self.y_train = y\n",
    "        \n",
    "        self.weights = np.ones(self.X_train.shape[0], dtype=float)\n",
    "        self.bettas = []\n",
    "        self.l_weights = []\n",
    "        self.update_params()\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            prob_weights = arc_x4(self.weights)\n",
    "            x_sub = self.X_train.sample(n=self.size_sample, weights=prob_weights,  # self.weights\n",
    "                                        replace=self.bootstrap, random_state=i)\n",
    "            x_oob = self.X_train.loc[self.X_train.index.difference(x_sub.index), :]\n",
    "\n",
    "            x_sub = x_sub.reset_index(drop=True)\n",
    "            X_sub_tr, y_sub_tr = cnt.pd_to_xy(x_sub)\n",
    "            if self.weighted_tree:\n",
    "                X_sub_tr[\"weights_obs\"] = prob_weights[x_sub['ind_start']]  # self.weights\n",
    "\n",
    "            model = CRAID(features=self.features, random_state=i, **self.tree_kwargs)\n",
    "            model.fit(X_sub_tr, y_sub_tr)\n",
    "            \n",
    "            wei_i, betta_i = self.count_model_weights(model, X_sub_tr, y_sub_tr)\n",
    "            self.add_model(model, x_oob, wei_i, betta_i)\n",
    "            self.update_weight(x_sub['ind_start'], wei_i)\n",
    "            \n",
    "            # print(betta_i)\n",
    "            self.ens_metr[i] = self.score_oob()\n",
    "            \n",
    "            if not (self.tolerance) and i > 0:\n",
    "                print(f\"METRIC: {self.ens_metr[i-1]} -> +1 model METRIC: {self.ens_metr[i]}\")\n",
    "                if self.descend_metr:\n",
    "                    stop = self.ens_metr[i-1] < self.ens_metr[i]\n",
    "                else:\n",
    "                    stop = self.ens_metr[i-1] > self.ens_metr[i]\n",
    "                if stop:\n",
    "                    self.select_model(0, len(self.models)-1)\n",
    "                    break\n",
    "        \n",
    "        if self.tolerance:\n",
    "            self.tolerance_find_best()\n",
    "        print('fitted:', len(self.models), 'models.')\n",
    "    \n",
    "    def count_model_weights(self, model, X_sub, y_sub):\n",
    "        if self.all_weight:\n",
    "            X_sub = self.X_train\n",
    "            y_sub = self.y_train\n",
    "        \n",
    "        pred_sf = model.predict_at_times(X_sub, bins=self.bins, mode=\"surv\")\n",
    "        pred_hf = model.predict_at_times(X_sub, bins=self.bins, mode=\"hazard\")\n",
    "        \n",
    "        time_hist = values_to_hist(y_sub[\"time\"])\n",
    "        lp_ti = np.log(time_hist / y_sub[\"time\"].shape)\n",
    "        lp_xi_ti = np.log(1 / time_hist)\n",
    "        likel = loglikelihood_i(y_sub[\"time\"], y_sub[\"cens\"], pred_sf, pred_hf, self.bins)\n",
    "        \n",
    "        lp_xi = lp_ti + lp_xi_ti - likel\n",
    "        wei = - np.exp(-lp_xi)\n",
    "        betta = np.sum(likel)\n",
    "        return wei, betta\n",
    "    \n",
    "    def update_weight(self, index, wei_i):\n",
    "        if self.all_weight:\n",
    "            self.weights = self.weights + wei_i\n",
    "        else:\n",
    "            self.weights[index] = (self.weights[index] - wei_i)\n",
    "        self.weights = (self.weights - self.weights.min()) / (self.weights.max() - self.weights.min())\n",
    "        \n",
    "    def get_aggreg(self, x):\n",
    "        if self.aggreg_func == 'median':\n",
    "            return np.median(x, axis=0)\n",
    "        elif self.aggreg_func == 'wei':\n",
    "            inv_wei = -1 / np.array(self.bettas)\n",
    "            wei = inv_wei/sum(inv_wei)\n",
    "            return np.sum((x.T*wei).T, axis=0)\n",
    "        return np.mean(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "916520e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6505 0.6904 0.7138 0.7394 0.7561 0.7691 0.7911 0.7886 0.7996 0.8016\n",
      " 0.8043 0.8018 0.805  0.8084 0.8075]\n",
      "fitted: 14 models.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-795.0394890758902"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from survivors.experiments.grid import generate_sample\n",
    "\n",
    "param = {\n",
    "    \"aggreg_func\": 'wei',\n",
    "    \"criterion\": \"weights\",\n",
    "    \"depth\": 5, \n",
    "    \"ens_metric_name\": \"roc\",\n",
    "    \"max_features\": \"sqrt\",\n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"n_estimators\": 15,\n",
    "    \"size_sample\": 0.5,\n",
    "    \"all_weight\": False,\n",
    "    \"leaf_model\": \"base_fast\"\n",
    "}\n",
    "\n",
    "X, y, features, categ, sch_nan = ds.load_pbc_dataset()\n",
    "gen = generate_sample(X, y, 5)\n",
    "X_train, y_train, X_test, y_test, bins = next(gen)\n",
    "\n",
    "prb = ProbBoostingCRAID(**param)\n",
    "prb.fit(X_train, y_train)\n",
    "\n",
    "pred_sf = prb.predict_at_times(X_test, bins=bins, mode=\"surv\")\n",
    "pred_hf = prb.predict_at_times(X_test, bins=bins, mode=\"hazard\")\n",
    "\n",
    "np.sum(loglikelihood_i(y_test[\"time\"], y_test[\"cens\"], pred_sf, pred_hf, bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "227b664e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[843.677177487109,\n",
       " 1128.4243320246874,\n",
       " 976.7412198817369,\n",
       " 1124.4976005335884,\n",
       " 773.0267519903804,\n",
       " 1062.5846902799788,\n",
       " 884.7021519288851,\n",
       " 726.4325516435139,\n",
       " 825.9051592548383,\n",
       " 799.8801451723468,\n",
       " 1014.1193201824256,\n",
       " 932.0453037013289,\n",
       " 1036.669474339849,\n",
       " 1095.0846413858062,\n",
       " 1080.4951369594746]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prb.bettas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "dec2bf55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0738521 , 0.05521622, 0.06379103, 0.05540904, 0.08060177,\n",
       "       0.05863752, 0.07042746, 0.08577166, 0.07544126, 0.07789583,\n",
       "       0.06143984, 0.06685011, 0.06010337, 0.05689727, 0.05766553])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_wei = -1/np.array(prb.bettas)\n",
    "wei = inv_wei/sum(inv_wei)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "429622e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       ...,\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47c7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "86abbdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "p = ParameterGrid({\"1\":[1, 2, 3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "3dc92f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "e2a8e0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 1}\n",
      "{'1': 2}\n",
      "{'1': 3}\n"
     ]
    }
   ],
   "source": [
    "for p_ in p:\n",
    "    print(p_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c281cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
