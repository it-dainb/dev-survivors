{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Experiments guide\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Iulii Vasilev <iuliivasilev@gmail.com>\n#\n# License: BSD 3 clause"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we will import modules\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Experiments instance\n\n1. The dataset is represented as a tuple:\n    - X (pd.DataFrame): Feature space\n    - y (structured np.ndarray): Target variables\n    - features (list): Covariates\n    - categ (list): Categorical covariates\n2. Metrics are defined according to the metrics module (keys of `METRIC_DICT` dictionary)\n3. Creating Experiments specify:\n    - folds (int): Quantity of cross-validate folds\n    - mode (str): Validation strategy.\n\nAvailable modes: \"CV\", \"CV+HOLD-OUT\", **\"CV+SAMPLE\"**, \"TIME-CV\".\nIn **this** case, five-fold cross-validation is performed followed by training the best models on 20 different samples of the original data.\nThe final quality is defined as the average quality over 20 samples.\n\nMethods for adding metrics (`set_metrics`), metrics for selecting the best models (`add_metric_best`) are used to control the experiment.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from survivors.experiments import grid as exp\nimport survivors.datasets as ds\n\nl_metrics = [\"CI\", \"CI_CENS\", \"IBS\", \"IBS_REMAIN\", \"IAUC\", \"IAUC_WW_TI\", \"AUPRC\"]\nX, y, features, categ, _ = ds.load_pbc_dataset()\nexperim = exp.Experiments(folds=5, mode=\"CV+SAMPLE\")\nexperim.set_metrics(l_metrics)\nexperim.add_metric_best(\"IBS_REMAIN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To add models, the `add_method` method is used with two parameters: model class and hyperparameter grid.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add models from external libraries\n\nExperiments support models from the external **scikit-survival** library. For each model a grid of hyperparameters is defined.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sksurv.linear_model import CoxPHSurvivalAnalysis\nfrom sksurv.tree import SurvivalTree\nfrom sksurv.ensemble import RandomSurvivalForest\nfrom sksurv.ensemble import GradientBoostingSurvivalAnalysis\nfrom sksurv.ensemble import ComponentwiseGradientBoostingSurvivalAnalysis\n\nCOX_param_grid = {\n    'alpha': [100, 10, 1, 0.1, 0.01, 0.001],\n    'ties': [\"breslow\"]\n}\n\nRSF_param_grid = {\n    'n_estimators': [50],\n    'max_depth': [None, 20],\n    'min_samples_leaf': [0.001, 0.01, 0.1, 0.25],\n    \"random_state\": [123]\n}\n\nST_param_grid = {\n    'max_depth': [None, 20, 30],\n    'min_samples_leaf': [1, 10, 20],\n    'max_features': [None, \"sqrt\"],\n    \"random_state\": [123]\n}\n\nGBSA_param_grid = {\n    'loss': [\"coxph\"],\n    'learning_rate': [0.01, 0.05, 0.1, 0.5],\n    'n_estimators': [50],\n    'min_samples_leaf': [1, 10, 50, 100],\n    'max_features': [\"sqrt\"],\n    \"random_state\": [123]\n}\n\nCWGBSA_param_grid = {\n    'loss': [\"coxph\"],\n    'learning_rate': [0.01, 0.05, 0.1, 0.5],\n    'n_estimators': [30, 50],\n    'subsample': [0.7, 1.0],\n    'dropout_rate': [0.0, 0.1, 0.5],\n    \"random_state\": [123]\n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "experim.add_method(CoxPHSurvivalAnalysis, COX_param_grid)\nexperim.add_method(SurvivalTree, ST_param_grid)\nexperim.add_method(RandomSurvivalForest, RSF_param_grid)\nexperim.add_method(ComponentwiseGradientBoostingSurvivalAnalysis, CWGBSA_param_grid)\nexperim.add_method(GradientBoostingSurvivalAnalysis, GBSA_param_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add embedded AFT models (optional)\n\nSome models of the external **lifelines** library (CoxPH, AFT, KaplanMeier, NelsonAalen) are also embedded in the library. \n\nNote that these models can be used in tree sheets to build stratified models.\n\nTo add your own model, you can use `LeafModel` wrapper from the external.leaf_model module.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from survivors.external import LogNormalAFT, AFT_param_grid\n\n# experim.add_method(LogNormalAFT, AFT_param_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add models from \"survivors\"\n\nOf course, the experiments support models from **survivors**:\n\n1. `CRAID`: a survival tree with weighted criteria, regularisation and complex non-parametric models.\n2. `BootstrapCRAID`: ensemble of independent trees on bootstrap samples.\n3. `ParallelBootstrapCRAID`: a parallel implementation of BootstrapCRAID.\n4. `BoostingCRAID`: adaptive bootstrapping with weighting of observations by probability of hitting the next subsample and correction based on base model error.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from survivors.tree import CRAID\nfrom survivors.ensemble import ParallelBootstrapCRAID, BoostingCRAID\n\nCRAID_param_grid = {\n    \"depth\": [10],\n    \"criterion\": [\"wilcoxon\", \"logrank\"],\n    \"l_reg\": [0, 0.01, 0.1, 0.5],\n    \"min_samples_leaf\": [0.05, 0.01, 0.001],\n    \"signif\": [0.1, 1.0],\n    \"categ\": [categ]\n}\n\nBSTR_param_grid = {\n    \"n_estimators\": [50],\n    \"depth\": [7],\n    \"size_sample\": [0.3, 0.7],\n    \"l_reg\": [0, 0.01, 0.1, 0.5],\n    \"criterion\": [\"tarone-ware\", \"wilcoxon\"],\n    \"min_samples_leaf\": [0.05, 0.01],\n    \"ens_metric_name\": [\"IBS_REMAIN\"],\n    \"max_features\": [\"sqrt\"],\n    \"categ\": [categ]\n}\n\nexperim.add_method(CRAID, CRAID_param_grid)\nexperim.add_method(ParallelBootstrapCRAID, BSTR_param_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run experiments\n\nTo run experiments, use the run_effective method with the source data and:\n    - verbose (int): log printing parameter.\n    - stratify_best (str/list): one or more hyperparameters on which to build independent best models (for each hyperparameter value).\n\n**Execution may take some time.**\n\nExperimental results can be obtained by calling methods:\n    - get_result: dataframe of results at the cross-validation stage.\n    - get_best_by_mode method: dataframe of model validation at 20 samples.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "experim.run_effective(X, y, verbose=0, stratify_best=[])\ndf_results = experim.get_result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_validation = experim.get_best_by_mode()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization\n\nFor example, here are the **result table** values and **boxplot**.\n\nFor each metric, four columns are defined:\n    - **\\<metric>**: list of metric indicators on each of the 20 samples.\n    - **\\<metric>_mean**: the average value of the metric at the 20 samples.\n    - **\\<metric>_CV**: list of metric indicators on cross-validation.\n    - **\\<metric>_CV_mean**: the average value of the metric on cross-validation.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_validation[[\"METHOD\", \"CI_CENS_mean\", \"IBS_REMAIN_mean\", \"IAUC_WW_TI_mean\", \"AUPRC_mean\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for m in l_metrics:\n    fig, axs = plt.subplots()\n    plt.title(m)\n    plt.boxplot(df_validation[m], labels=df_validation['METHOD'], showmeans=True, vert=False)\n    plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}